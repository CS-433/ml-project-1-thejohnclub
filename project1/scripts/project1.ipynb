{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # train data path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the features by the number of jets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing the rows of tX by the number of jets, dropping the column Pri_Jet_Num and adding an extra column of np.ones\n",
    "zero_indices = []\n",
    "one_indices = []\n",
    "two_three_indices = []\n",
    "zero_indices = np.where(tX[:,22]==0)[0]\n",
    "one_indices = np.where(tX[:,22]==1)[0]\n",
    "two_three_indices = np.where(np.logical_or(tX[:,22]==2, tX[:,22]==3))[0]\n",
    "tX_0 = tX[zero_indices, :]\n",
    "tX_0 = np.delete(tX_0, 22, axis=1)\n",
    "tX_1 = tX[one_indices, :]\n",
    "tX_1 = np.delete(tX_1, 22, axis=1)\n",
    "tX_2_3 = tX[two_three_indices, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing also the output by the type of particle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_0 = y[zero_indices]\n",
    "y_1 = y[one_indices]\n",
    "y_2_3 = y[two_three_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a column of zeros and ones to detect whether the mass has been measured or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the indices where the mass is not calculated, add the column which has 0 in those indices\n",
    "# and 1 everywhere else for all matrices 0,1,2_3\n",
    "zero_indices_0 = np.where(tX_0[:,1] == -999.)[0]\n",
    "column_to_add = np.array([0 if i in zero_indices_0 else 1 for i in range(tX_0.shape[0])])\n",
    "tX_0 = np.insert(tX_0, 0, column_to_add, axis=1)\n",
    "zero_indices_1 = np.where(tX_1[:,1] == -999.)[0]\n",
    "column_to_add = np.array([0 if i in zero_indices_1 else 1 for i in range(tX_1.shape[0])])\n",
    "tX_1 = np.insert(tX_1, 0, column_to_add, axis=1)\n",
    "zero_indices_2_3 = np.where(tX_2_3[:,1] == -999.)[0]\n",
    "column_to_add = np.array([0 if i in zero_indices_2_3 else 1 for i in range(tX_2_3.shape[0])])\n",
    "tX_2_3 = np.insert(tX_2_3, 0, column_to_add, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the labels to {0,1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y == 0 non detected Boson, y == 1 detected Boson\n",
    "y_ = np.array([0 if l == -1 else 1 for l in y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Throwing away the outliers from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, tX_2_3.shape[1]):\n",
    "    index_column_valid =np.where(tX_2_3[:,i] != -999.)[0]\n",
    "    column_25_quantile, column_75_quantile = np.quantile(tX_2_3[index_column_valid,i], \n",
    "                                                         np.array([0.25, 0.75]))\n",
    "    interquantile = column_75_quantile-column_25_quantile\n",
    "    column_15_quantile, column_85_quantile = np.quantile(tX_2_3[index_column_valid,i], \n",
    "                                                         np.array([0.15, 0.85]))\n",
    "    indices_outliers = np.where((column_15_quantile - 1.5 * interquantile >= tX_2_3[index_column_valid,i])\n",
    "                                             | (tX_2_3[index_column_valid,i] >= \n",
    "                                                column_85_quantile + 1.5 * interquantile))[0]\n",
    "    #indices_outliers = np.argwhere((tX_tilda_2_3[index_column_valid,i] >= column_85_quantile + 1.5 * interquantile) | \n",
    "                                  #(column_15_quantile - 1.5 * interquantile >= tX_tilda_2_3[index_column_valid,i]))\n",
    "    median = np.median(tX_2_3[index_column_valid, i], axis = 0)\n",
    "    #print(np.sort(tX_tilda_2_3[index_column_valid[indices_outliers],i]).T)\n",
    "    #print(np.where(tX_tilda_2_3[indices_outliers,i])==-999.)\n",
    "    #print(median)\n",
    "    tX_2_3[index_column_valid[indices_outliers],i] =  median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99913, 20)\n",
      "[5, 6, 7, 13, 23, 24, 25, 26, 27, 28]\n"
     ]
    }
   ],
   "source": [
    "col_to_delete = []\n",
    "for i in range(1, tX_0.shape[1]):\n",
    "    index_column_valid =np.where(tX_0[:,i] != -999.)[0]\n",
    "    if len(index_column_valid)==0:\n",
    "        #we drop the column (we will have to do the same for the test set as well)\n",
    "        col_to_delete.append(i)\n",
    "    else :\n",
    "        column_25_quantile, column_75_quantile = np.quantile(tX_0[index_column_valid,i], \n",
    "                                                         np.array([0.25, 0.75]))\n",
    "        interquantile = column_75_quantile-column_25_quantile\n",
    "        column_15_quantile, column_85_quantile = np.quantile(tX_0[index_column_valid,i], \n",
    "                                                         np.array([0.15, 0.85]))\n",
    "        indices_outliers = np.where((column_15_quantile - 1.5 * interquantile >= tX_0[index_column_valid,i])\n",
    "                                             | (tX_0[index_column_valid,i] >= \n",
    "                                                column_85_quantile + 1.5 * interquantile))[0]\n",
    "        #indices_outliers = np.argwhere((tX_tilda_2_3[index_column_valid,i] >= column_85_quantile + 1.5 * interquantile) | \n",
    "                                  #(column_15_quantile - 1.5 * interquantile >= tX_tilda_2_3[index_column_valid,i]))\n",
    "        median = np.median(tX_0[index_column_valid, i], axis = 0)\n",
    "        #print(np.sort(tX_tilda_2_3[index_column_valid[indices_outliers],i]).T)\n",
    "        #print(np.where(tX_tilda_2_3[indices_outliers,i])==-999.)\n",
    "        #print(median)\n",
    "        tX_0[index_column_valid[indices_outliers],i] =  median\n",
    "tX_0 = np.delete(tX_0, col_to_delete, axis=1)\n",
    "print(tX_0.shape)\n",
    "print(col_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 7, 13, 26, 27, 28]\n"
     ]
    }
   ],
   "source": [
    "col_to_delete = []\n",
    "for i in range(1, tX_1.shape[1]):\n",
    "    index_column_valid =np.where(tX_1[:,i] != -999.)[0]\n",
    "    if len(index_column_valid)==0:\n",
    "        #we drop the column (we will have to do the same for the test set as well)\n",
    "        col_to_delete.append(i)\n",
    "    else :\n",
    "        column_25_quantile, column_75_quantile = np.quantile(tX_1[index_column_valid,i], \n",
    "                                                         np.array([0.25, 0.75]))\n",
    "        interquantile = column_75_quantile-column_25_quantile\n",
    "        column_15_quantile, column_85_quantile = np.quantile(tX_1[index_column_valid,i], \n",
    "                                                         np.array([0.15, 0.85]))\n",
    "        indices_outliers = np.where((column_15_quantile - 1.5 * interquantile >= tX_1[index_column_valid,i])\n",
    "                                             | (tX_1[index_column_valid,i] >= \n",
    "                                                column_85_quantile + 1.5 * interquantile))[0]\n",
    "        #indices_outliers = np.argwhere((tX_tilda_2_3[index_column_valid,i] >= column_85_quantile + 1.5 * interquantile) | \n",
    "                                  #(column_15_quantile - 1.5 * interquantile >= tX_tilda_2_3[index_column_valid,i]))\n",
    "        median = np.median(tX_1[index_column_valid, i], axis = 0)\n",
    "        #print(np.sort(tX_tilda_2_3[index_column_valid[indices_outliers],i]).T)\n",
    "        #print(np.where(tX_tilda_2_3[indices_outliers,i])==-999.)\n",
    "        #print(median)\n",
    "        tX_1[index_column_valid[indices_outliers],i] =  median\n",
    "tX_1 = np.delete(tX_1, col_to_delete, axis=1)\n",
    "print(col_to_delete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we substitute the -999 values with the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, tX_2_3.shape[1]):\n",
    "    index_column_non_valid =np.where(tX_2_3[:,i] == -999.)[0]\n",
    "    index_column_valid =np.where(tX_2_3[:,i] != -999.)[0]\n",
    "    median = np.median(tX_2_3[index_column_valid, i], axis = 0)\n",
    "    tX_2_3[index_column_non_valid,i] =  median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, tX_1.shape[1]):\n",
    "    index_column_non_valid =np.where(tX_1[:,i] == -999.)[0]\n",
    "    index_column_valid =np.where(tX_1[:,i] != -999.)[0]\n",
    "    median = np.median(tX_1[index_column_valid, i], axis = 0)\n",
    "    tX_1[index_column_non_valid,i] =  median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, tX_0.shape[1]):\n",
    "    index_column_non_valid =np.where(tX_0[:,i] == -999.)[0]\n",
    "    index_column_valid =np.where(tX_0[:,i] != -999.)[0]\n",
    "    median = np.median(tX_0[index_column_valid, i], axis = 0)\n",
    "    tX_0[index_column_non_valid,i] =  median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tX_2_3[:,1:],_,_ = standardize(tX_2_3[:,1:]) #we standardize everything a part from the column added manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.97351249  0.46588281 ...  0.61614788 -1.36131161\n",
      "  -0.70374641]\n",
      " [ 1.         -0.82851663 -0.77157038 ...  0.11608109  1.71034105\n",
      "   0.2995537 ]\n",
      " [ 1.          1.3538447  -0.27431587 ...  0.07030726 -1.52202162\n",
      "   0.12704911]\n",
      " ...\n",
      " [ 1.          0.04006392  0.02513449 ...  0.25930888  0.22982758\n",
      "   0.42357227]\n",
      " [ 1.          0.66304099 -1.0843679  ...  0.29031696 -1.21821366\n",
      "  -0.20699627]\n",
      " [ 1.          0.04006392  0.31977857 ... -0.02271698 -0.62490751\n",
      "   0.05569682]]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(tX_2_3)\n",
    "print(np.count_nonzero(tX_2_3 == -999.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(tX_0[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_0[:,1:-1],_,_ = standardize(tX_0[:,1:-1]) \n",
    "# the last column is not standardized since it is the zero vector (to be dropped?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          1.05744907  0.7827665  ...  0.04662815 -0.7724943\n",
      "   0.        ]\n",
      " [ 1.          2.14505538 -1.37519852 ... -0.4674532  -1.44727052\n",
      "   0.        ]\n",
      " [ 1.         -0.24632406 -0.2496121  ...  0.0267496   0.12380588\n",
      "   0.        ]\n",
      " ...\n",
      " [ 1.         -0.0469687   0.00532098 ... -0.46524447 -0.8883482\n",
      "   0.        ]\n",
      " [ 1.         -0.60851918 -1.29333222 ...  0.46131675 -0.22629665\n",
      "   0.        ]\n",
      " [ 1.         -0.0469687   0.49300595 ... -0.86778508 -0.49908812\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(tX_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_1[:,1:],_,_ = standardize(tX_1[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000000e+00  1.55539992e+00  7.27047143e-01 ...  3.98445313e-01\n",
      "   6.45414781e-01 -4.14297220e-01]\n",
      " [ 1.00000000e+00  1.45598722e-03  3.58462301e+00 ...  1.12748232e+00\n",
      "  -1.10752634e+00 -4.89864560e-01]\n",
      " [ 1.00000000e+00  1.36261180e+00 -1.05809645e+00 ... -3.92076740e-01\n",
      "  -9.40265168e-01 -1.01072441e+00]\n",
      " ...\n",
      " [ 1.00000000e+00  1.45598722e-03  1.01732036e+00 ... -4.67286130e-01\n",
      "  -3.80160315e-01  8.39087556e-01]\n",
      " [ 1.00000000e+00  6.75509966e-01  9.95415259e-01 ... -6.76994064e-01\n",
      "   1.39533906e+00  5.32418071e-01]\n",
      " [ 1.00000000e+00 -2.21030015e-01  4.74893699e-01 ...  9.88591985e-01\n",
      "  -8.30516498e-02 -5.76298292e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(tX_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We insert the column for the bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_tilda_0 = np.insert(tX_0, 0, np.ones(tX_0.shape[0]), axis=1)\n",
    "tX_tilda_1 = np.insert(tX_1, 0, np.ones(tX_1.shape[0]), axis=1)\n",
    "tX_tilda_2_3 = np.insert(tX_2_3, 0, np.ones(tX_2_3.shape[0]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors = ['red', 'blue']\n",
    "# x_pos=[]\n",
    "# x_neg=[]\n",
    "\n",
    "# for j in range(len(y)):\n",
    "#  if(y[j]==1):\n",
    "#       x_pos.insert(0,tX[j])\n",
    "#    else:\n",
    "#        x_neg.insert(0,tX[j])\n",
    "# xpos = np.array(x_pos)\n",
    "# xneg = np.array(x_neg)\n",
    "# for i in range(tX.shape[1]):\n",
    "#  plt.hist(xpos[:,i], alpha = 0.5, color = 'r', bins = 100)\n",
    "#  plt.hist(xneg[:,i], alpha = 0.5, color = 'b', bins = 100)\n",
    "#  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    N = y.shape[0]\n",
    "    e = y - tx @ w \n",
    "    loss = 1/(2*N) * np.dot(e,e)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    N = y.shape[0]\n",
    "    e = y - tx @ w\n",
    "    gradient = -(1/N) * (tx.T) @ (e)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        gradient = compute_gradient(y,tx,w)\n",
    "        w = w - gamma * gradient\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        # print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "        # bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    N = y.shape[0]\n",
    "    random_number = random.randint(0,N)\n",
    "    #random_number =1\n",
    "    xn = tx[random_number,:]\n",
    "    random_gradient = - np.dot(xn, y[random_number] - np.dot(xn,w))\n",
    "    return random_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        stoch_gradient = compute_stoch_gradient(y,tx,w)\n",
    "        w = w - gamma * stoch_gradient\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        # print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "        #    bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "\n",
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    forcing_term = np.transpose(tx) @ y\n",
    "    coefficient_matrix = np.transpose(tx) @ tx\n",
    "    w = np.linalg.solve(coefficient_matrix, forcing_term)\n",
    "    return w\n",
    "\n",
    "def test_your_least_squares(y, tx):\n",
    "    \"\"\"compare the solution of the normal equations with the weights returned by gradient descent algorithm.\"\"\"\n",
    "    w_least_squares = least_squares(y, tx)\n",
    "    initial_w = np.zeros(tx.shape[1])\n",
    "    max_iters = 50\n",
    "    gamma = 0.7\n",
    "    losses_gradient_descent, w_gradient_descent = gradient_descent(y, tx, initial_w, max_iters, gamma)\n",
    "    w = w_gradient_descent[-1]\n",
    "    err = np.linalg.norm(w_least_squares-w)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    N = tx.shape\n",
    "    lambda_prime = 2 * N[0] * lambda_\n",
    "    coefficient_matrix = np.transpose(tx) @ tx + lambda_prime * np.eye(N[1])\n",
    "    forcing_term = np.transpose(tx) @ y\n",
    "    w = np.linalg.solve(coefficient_matrix, forcing_term)\n",
    "    return w\n",
    "\n",
    "def debug_ridge(y, tx):\n",
    "    \"\"\"debugging the ridge regression by setting lambda=0.\"\"\"\n",
    "    w_least_squares = least_squares(y, tx)\n",
    "    w_0 = ridge_regression(y, tx, 0)\n",
    "    err = np.linalg.norm(w_least_squares-w_0)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply the sigmoid function on t.\"\"\"\n",
    "    return np.exp(t) / (1+np.exp(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the loss: negative log likelihood.\"\"\"\n",
    "    e = - y * (tx @ w) + np.log(1+np.exp(1+ tx@w))\n",
    "    # e = - (y*np.log(sigmoid(tx @ w)) + (1-y)*np.log(1-sigmoid(tx @ w)))\n",
    "    return e.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    return np.transpose(tx) @ (sigmoid(tx @ w) - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent(y, tx, w_initial, gamma, max_iters, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    w1 = w_initial\n",
    "    for iter in range(max_iters):\n",
    "        grad = calculate_gradient(y, tx, w1)\n",
    "        w = w1 - gamma * grad\n",
    "        loss = calculate_loss(y, tx, w) + 2*lambda_*np.linalg.norm(w) ** 2\n",
    "        losses.append(loss)\n",
    "        w1 = w\n",
    "    return losses, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hessian(y, tx, w):\n",
    "    \"\"\"return the Hessian of the loss function.\"\"\"\n",
    "    diag = sigmoid(tx @ w) * (1 - sigmoid(tx @ w))\n",
    "    D = diag * np.eye(tx.shape[0])\n",
    "    return np.transpose(tx) @ D @ tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, w):\n",
    "    \"\"\"return the loss, gradient, and Hessian.\"\"\"\n",
    "    grad = calculate_gradient(y, tx, w)\n",
    "    hess = calculate_hessian(y, tx, w)\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    return loss, grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_newton_method(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step on Newton's method.\n",
    "    return the loss and updated w.\n",
    "    \"\"\"\n",
    "    loss, grad, hess = logistic_regression(y, tx, w)\n",
    "    sol = np.linalg.solve(hess, grad)\n",
    "    w = w - gamma * sol\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "    \"\"\"return the loss, gradient\"\"\"\n",
    "    loss = calculate_loss(y, tx, w) + lambda_*np.linalg.norm(w) ** 2\n",
    "    grad = calculate_gradient(y, tx, w) + 2*lambda_*w\n",
    "    hess = calculate_hessian(y, tx, w) + 2*lambda_*np.eye(w.shape[0])\n",
    "    return loss, grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_penalized_gradient(y, tx, w_initial, gamma, max_iters, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent, using the penalized logistic regression.\n",
    "    Return the loss and updated w.\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    w1 = w_initial\n",
    "    for iter in range(max_iters):\n",
    "        _, grad, hess = calculate_gradient(y, tx, w1, lambda_)\n",
    "        sol = np.linalg.solve(hess, grad)\n",
    "        w = w1 - gamma * sol\n",
    "        w_opt.append(w)\n",
    "        loss, _, _ = penalized_logistic_regression(y, tx, w, lambda_)\n",
    "        losses.append(loss)\n",
    "        w1 = w\n",
    "    return losses, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=1 up to j=degree.\"\"\"\n",
    "    powers = np.arange(1, degree + 1)\n",
    "    phi = np.column_stack([np.power(x[:,0], exponent) for exponent in powers])\n",
    "    for i in range(1, x.shape[1]):\n",
    "        phi_i = np.column_stack([np.power(x[:,i], exponent) for exponent in powers])\n",
    "        phi = np.column_stack([phi, phi_i])\n",
    "    return phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    N = y.shape[0]\n",
    "    np.random.seed(seed)\n",
    "    interval = int(np.floor(N / k_fold))\n",
    "    indices = np.random.permutation(N)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    N = y.shape[0]\n",
    "    k_fold = k_indices.shape[0]\n",
    "    list_ = []\n",
    "    interval = int(N/k_fold)\n",
    "    for i in range(k_fold):\n",
    "        if i != k:\n",
    "            list_.append(i)\n",
    "    x_training = np.zeros((int((k_fold-1)/k_fold*N), x.shape[1]))\n",
    "    y_training = np.zeros(int((k_fold-1)/k_fold*N))\n",
    "    for j in range(len(list_)):\n",
    "        x_training[interval*(j):interval*(j+1), :] = x[np.array([k_indices[list_[j]]]), :]\n",
    "    x_testing = x[k_indices[k], :]\n",
    "    for j in range(len(list_)):\n",
    "        y_training[interval*(j):interval*(j+1)] = y[np.array([k_indices[list_[j]]])]\n",
    "    y_testing = y[k_indices[k]]\n",
    "    x_training_augmented = build_poly(x_training, degree)\n",
    "    x_testing_augmented = build_poly(x_testing, degree)\n",
    "    w_opt_training = ridge_regression(y_training, x_training_augmented, lambda_)\n",
    "    loss_tr = compute_loss(y_training, x_training_augmented, w_opt_training)\n",
    "    loss_te = compute_loss(y_testing, x_testing_augmented, w_opt_training)\n",
    "    return loss_tr, loss_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.24916029 0.2421667  0.23991829 0.23876138 0.23803144]\n",
      " [0.24916027 0.24216667 0.23991825 0.23876127 0.23803111]\n",
      " [0.24916021 0.24216659 0.23991815 0.23876103 0.2380303 ]\n",
      " [0.24916008 0.24216643 0.23991797 0.23876056 0.23802858]\n",
      " [0.24915986 0.24216617 0.23991776 0.23875978 0.2380255 ]\n",
      " [0.24915968 0.24216611 0.23991818 0.23875953 0.23802221]\n",
      " [0.24916069 0.24216838 0.23992335 0.23876547 0.2380296 ]\n",
      " [0.24916862 0.24218348 0.23995241 0.23880362 0.23809433]\n",
      " [0.24920508 0.24225289 0.24007913 0.2389675  0.2383515 ]\n",
      " [0.24933831 0.24251095 0.24053507 0.23952445 0.23906248]\n",
      " [0.24976516 0.24330605 0.24185225 0.24102805 0.24055126]\n",
      " [0.25099515 0.2453916  0.24473633 0.24412544 0.24310165]\n",
      " [0.25427171 0.25017384 0.24948218 0.24890924 0.24722739]\n",
      " [0.26228721 0.25954922 0.256069   0.25515646 0.25401568]\n",
      " [0.27885334 0.27483722 0.26533625 0.26381616 0.26393929]]\n",
      "[0.00061054] [6]\n"
     ]
    }
   ],
   "source": [
    "degrees = np.arange(2, 7)\n",
    "lambdas = np.logspace(-5,0,15)\n",
    "k_fold = 5\n",
    "seed = 1\n",
    "training_loss = np.zeros((len(lambdas), len(degrees)))\n",
    "testing_loss = np.zeros((len(lambdas), len(degrees)))\n",
    "k_indices = build_k_indices(y_0, k_fold, seed)\n",
    "for index1 in range(len(lambdas)):\n",
    "    for index2 in range(len(degrees)):\n",
    "        train_loss = 0\n",
    "        test_loss = 0\n",
    "        for k in range(k_fold):\n",
    "            loss_tr, loss_te = cross_validation(y_0, tX_tilda_0, k_indices, k,\n",
    "                                                lambdas[index1], degrees[index2])\n",
    "            train_loss += loss_tr\n",
    "            test_loss += loss_te\n",
    "        training_loss[index1, index2] = train_loss / k_fold\n",
    "        testing_loss[index1, index2] = test_loss / k_fold\n",
    "best_result = np.where(testing_loss == np.amin(testing_loss))\n",
    "print(testing_loss)\n",
    "lambda_opt, degree_opt = lambdas[best_result[0]],degrees[best_result[1]]\n",
    "print(lambda_opt, degree_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.32091875 0.29890002 0.2953865  0.28987572 0.28931905]\n",
      " [0.32091872 0.29889996 0.29538639 0.28987558 0.28931891]\n",
      " [0.32091867 0.29889985 0.29538618 0.2898753  0.28931857]\n",
      " [0.32091855 0.29889963 0.29538582 0.28987509 0.28931899]\n",
      " [0.32091834 0.29889936 0.29538561 0.28987641 0.28932326]\n",
      " [0.3209181  0.29889991 0.29538802 0.2898856  0.28933933]\n",
      " [0.32091882 0.29890688 0.29540568 0.28992459 0.28939364]\n",
      " [0.3209265  0.298949   0.29549084 0.29007073 0.28959037]\n",
      " [0.32097027 0.29914957 0.29582926 0.29060091 0.29026797]\n",
      " [0.32116708 0.2999226  0.29696621 0.2922807  0.29217401]\n",
      " [0.32188658 0.30222004 0.2999793  0.29643882 0.29632654]\n",
      " [0.32400434 0.30737406 0.30598241 0.30419829 0.30340413]\n",
      " [0.32906837 0.31672222 0.31550933 0.31520061 0.31306897]\n",
      " [0.33934128 0.331582   0.32868471 0.32788647 0.3244386 ]\n",
      " [0.35718123 0.35173275 0.34464516 0.34165093 0.33748249]]\n",
      "[5.17947468e-05] [6]\n"
     ]
    }
   ],
   "source": [
    "degrees = np.arange(2, 7)\n",
    "lambdas = np.logspace(-5,0,15)\n",
    "k_fold = 5\n",
    "seed = 1\n",
    "training_loss = np.zeros((len(lambdas), len(degrees)))\n",
    "testing_loss = np.zeros((len(lambdas), len(degrees)))\n",
    "k_indices = build_k_indices(y_1, k_fold, seed)\n",
    "for index1 in range(len(lambdas)):\n",
    "    for index2 in range(len(degrees)):\n",
    "        train_loss = 0\n",
    "        test_loss = 0\n",
    "        for k in range(k_fold):\n",
    "            loss_tr, loss_te = cross_validation(y_1, tX_tilda_1, k_indices, k, \n",
    "                                                lambdas[index1], degrees[index2])\n",
    "            train_loss += loss_tr\n",
    "            test_loss += loss_te\n",
    "        training_loss[index1, index2] = train_loss / k_fold\n",
    "        testing_loss[index1, index2] = test_loss / k_fold\n",
    "best_result = np.where(testing_loss == np.amin(testing_loss))\n",
    "print(testing_loss)\n",
    "lambda_opt, degree_opt = lambdas[best_result[0]], degrees[best_result[1]]\n",
    "print(lambda_opt, degree_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = np.arange(2, 7)\n",
    "lambdas = np.logspace(-5,0,15)\n",
    "k_fold = 5\n",
    "seed = 1\n",
    "training_loss = np.zeros((len(lambdas), len(degrees)))\n",
    "testing_loss = np.zeros((len(lambdas), len(degrees)))\n",
    "k_indices = build_k_indices(y_2_3, k_fold, seed)\n",
    "for index1 in range(len(lambdas)):\n",
    "    for index2 in range(len(degrees)):\n",
    "        train_loss = 0\n",
    "        test_loss = 0\n",
    "        for k in range(k_fold):\n",
    "            loss_tr, loss_te = cross_validation(y_2_3, tX_tilda_2_3, k_indices, k,\n",
    "                                            lambdas[index1], degrees[index2])\n",
    "            train_loss += loss_tr\n",
    "            test_loss += loss_te\n",
    "        training_loss[index1, index2] = train_loss / k_fold\n",
    "        testing_loss[index1, index2] = test_loss / k_fold\n",
    "best_result = np.where(testing_loss == np.amin(testing_loss))\n",
    "lambda_opt, degree_opt = lambdas[best_result[0]], degrees[best_result[1]]\n",
    "print(testing_loss)\n",
    "print(lambda_opt, degree_opt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(weights, tX_test):\n",
    "    y = np.array(tX_test) @ np.array(weights)\n",
    "    labels = [1 if l > 0 else -1 for l in y]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indeces_lepton = [0, 1, 2, 3, 7, 8, 9, 10, 11, 12, 16, 17, 18, 21, 22]\n",
    "# indeces_hadronic_tau = [0, 3, 7, 8, 9, 10, 11, 13, 14, 15, 21, 22]\n",
    "# indeces_jet = [0, 4, 5, 6, 8, 9, 12, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
    "# indeces_MTE = [0, 1, 3, 8, 9, 11, 19, 20, 21, 22]\n",
    "\n",
    "# tX_lepton_test = tX_test[:, indeces_lepton]\n",
    "# tX_tilda_lepton_test = np.insert(tX_lepton_test, 0, np.ones(tX_test.shape[0]), axis=1)\n",
    "# labels_lepton = predict_labels(w_opt_lepton, tX_tilda_lepton_test)\n",
    "# print(1 / np.array(labels_lepton).shape[0] * np.count_nonzero(np.array(labels_lepton) == 1))\n",
    "\n",
    "# tX_hadronic_tau_test = tX_test[:, indeces_hadronic_tau]\n",
    "# tX_tilda_hadronic_tau_test = np.insert(tX_hadronic_tau_test, 0, np.ones(tX_test.shape[0]), axis=1)\n",
    "# labels_hadronic_tau = predict_labels(w_opt_hadronic_tau, tX_tilda_hadronic_tau_test)\n",
    "# print(1 / np.array(labels_hadronic_tau).shape[0] * np.count_nonzero(np.array(labels_hadronic_tau) == 1))\n",
    "\n",
    "# tX_jet_test = tX_test[:, indeces_jet]\n",
    "# tX_tilda_jet_test = np.insert(tX_jet_test, 0, np.ones(tX_test.shape[0]), axis=1)\n",
    "# labels_jet = predict_labels(w_opt_jet, tX_tilda_jet_test)\n",
    "# print(1 / np.array(labels_jet).shape[0] * np.count_nonzero(np.array(labels_jet) == 1))\n",
    "\n",
    "# tX_MTE_test = tX_test[:, indeces_MTE]\n",
    "# tX_tilda_MTE_test = np.insert(tX_MTE_test, 0, np.ones(tX_test.shape[0]), axis=1)\n",
    "# labels_MTE = predict_labels(w_opt_MTE, tX_tilda_MTE_test)\n",
    "# print(1 / np.array(labels_MTE).shape[0] * np.count_nonzero(np.array(labels_MTE) == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRIAL, UNDERESTIMATION OF THE TRUE PROBABILITY TO GET A BOSON\n",
    "# count = np.array(labels_MTE) + np.array(labels_lepton) + np.array(labels_hadronic_tau) + np.array(labels_jet)\n",
    "# TrueSignal = np.array([int(bool((counted == 4))) for counted in count])\n",
    "# print( 1 / np.array(count).shape[0] * np.count_nonzero(np.array(TrueSignal) == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
