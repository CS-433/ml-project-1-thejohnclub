{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # train data path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the features by the number of jets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing the rows of tX by the number of jets, dropping the column Pri_Jet_Num and adding an extra column of np.ones\n",
    "zero_indices = []\n",
    "one_indices = []\n",
    "two_three_indices = []\n",
    "zero_indices = np.where(tX[:,22]==0)[0]\n",
    "one_indices = np.where(tX[:,22]==1)[0]\n",
    "two_three_indices = np.where(np.logical_or(tX[:,22]==2, tX[:,22]==3))[0]\n",
    "tX_0 = tX[zero_indices, :]\n",
    "tX_0 = np.delete(tX_0, 22, axis=1)\n",
    "tX_1 = tX[one_indices, :]\n",
    "tX_1 = np.delete(tX_1, 22, axis=1)\n",
    "tX_2_3 = tX[two_three_indices, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing also the output by the type of particle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_0 = y[zero_indices]\n",
    "y_1 = y[one_indices]\n",
    "y_2_3 = y[two_three_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a column of zeros and ones to detect whether the mass has been measured or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the indices where the mass is not calculated, add the column which has 0 in those indices\n",
    "# and 1 everywhere else for all matrices 0,1,2_3\n",
    "zero_indices_0 = np.where(tX_0[:,1] == -999.)[0]\n",
    "column_to_add = np.array([0 if i in zero_indices_0 else 1 for i in range(tX_0.shape[0])])\n",
    "tX_0 = np.insert(tX_0, 0, column_to_add, axis=1)\n",
    "zero_indices_1 = np.where(tX_1[:,1] == -999.)[0]\n",
    "column_to_add = np.array([0 if i in zero_indices_1 else 1 for i in range(tX_1.shape[0])])\n",
    "tX_1 = np.insert(tX_1, 0, column_to_add, axis=1)\n",
    "zero_indices_2_3 = np.where(tX_2_3[:,1] == -999.)[0]\n",
    "column_to_add = np.array([0 if i in zero_indices_2_3 else 1 for i in range(tX_2_3.shape[0])])\n",
    "tX_2_3 = np.insert(tX_2_3, 0, column_to_add, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the labels to {0,1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y == 0 non detected Boson, y == 1 detected Boson\n",
    "y_ = np.array([0 if l == -1 else 1 for l in y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Throwing away the outliers from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, tX_2_3.shape[1]):\n",
    "    index_column_valid =np.where(tX_2_3[:,i] != -999.)[0]\n",
    "    column_25_quantile, column_75_quantile = np.quantile(tX_2_3[index_column_valid,i], \n",
    "                                                         np.array([0.25, 0.75]))\n",
    "    interquantile = column_75_quantile-column_25_quantile\n",
    "    column_15_quantile, column_85_quantile = np.quantile(tX_2_3[index_column_valid,i], \n",
    "                                                         np.array([0.15, 0.85]))\n",
    "    indices_outliers = np.where((column_15_quantile - 1.5 * interquantile >= tX_2_3[index_column_valid,i])\n",
    "                                             | (tX_2_3[index_column_valid,i] >= \n",
    "                                                column_85_quantile + 1.5 * interquantile))[0]\n",
    "    #indices_outliers = np.argwhere((tX_tilda_2_3[index_column_valid,i] >= column_85_quantile + 1.5 * interquantile) | \n",
    "                                  #(column_15_quantile - 1.5 * interquantile >= tX_tilda_2_3[index_column_valid,i]))\n",
    "    median = np.median(tX_2_3[index_column_valid, i], axis = 0)\n",
    "    #print(np.sort(tX_tilda_2_3[index_column_valid[indices_outliers],i]).T)\n",
    "    #print(np.where(tX_tilda_2_3[indices_outliers,i])==-999.)\n",
    "    #print(median)\n",
    "    tX_2_3[index_column_valid[indices_outliers],i] =  median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99913, 20)\n",
      "[5, 6, 7, 13, 23, 24, 25, 26, 27, 28]\n"
     ]
    }
   ],
   "source": [
    "col_to_delete = []\n",
    "for i in range(1, tX_0.shape[1]):\n",
    "    index_column_valid =np.where(tX_0[:,i] != -999.)[0]\n",
    "    if len(index_column_valid)==0:\n",
    "        #we drop the column (we will have to do the same for the test set as well)\n",
    "        col_to_delete.append(i)\n",
    "    else :\n",
    "        column_25_quantile, column_75_quantile = np.quantile(tX_0[index_column_valid,i], \n",
    "                                                         np.array([0.25, 0.75]))\n",
    "        interquantile = column_75_quantile-column_25_quantile\n",
    "        column_15_quantile, column_85_quantile = np.quantile(tX_0[index_column_valid,i], \n",
    "                                                         np.array([0.15, 0.85]))\n",
    "        indices_outliers = np.where((column_15_quantile - 1.5 * interquantile >= tX_0[index_column_valid,i])\n",
    "                                             | (tX_0[index_column_valid,i] >= \n",
    "                                                column_85_quantile + 1.5 * interquantile))[0]\n",
    "        #indices_outliers = np.argwhere((tX_tilda_2_3[index_column_valid,i] >= column_85_quantile + 1.5 * interquantile) | \n",
    "                                  #(column_15_quantile - 1.5 * interquantile >= tX_tilda_2_3[index_column_valid,i]))\n",
    "        median = np.median(tX_0[index_column_valid, i], axis = 0)\n",
    "        #print(np.sort(tX_tilda_2_3[index_column_valid[indices_outliers],i]).T)\n",
    "        #print(np.where(tX_tilda_2_3[indices_outliers,i])==-999.)\n",
    "        #print(median)\n",
    "        tX_0[index_column_valid[indices_outliers],i] =  median\n",
    "tX_0 = np.delete(tX_0, col_to_delete, axis=1)\n",
    "print(tX_0.shape)\n",
    "print(col_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 7, 13, 26, 27, 28]\n"
     ]
    }
   ],
   "source": [
    "col_to_delete = []\n",
    "for i in range(1, tX_1.shape[1]):\n",
    "    index_column_valid =np.where(tX_1[:,i] != -999.)[0]\n",
    "    if len(index_column_valid)==0:\n",
    "        #we drop the column (we will have to do the same for the test set as well)\n",
    "        col_to_delete.append(i)\n",
    "    else :\n",
    "        column_25_quantile, column_75_quantile = np.quantile(tX_1[index_column_valid,i], \n",
    "                                                         np.array([0.25, 0.75]))\n",
    "        interquantile = column_75_quantile-column_25_quantile\n",
    "        column_15_quantile, column_85_quantile = np.quantile(tX_1[index_column_valid,i], \n",
    "                                                         np.array([0.15, 0.85]))\n",
    "        indices_outliers = np.where((column_15_quantile - 1.5 * interquantile >= tX_1[index_column_valid,i])\n",
    "                                             | (tX_1[index_column_valid,i] >= \n",
    "                                                column_85_quantile + 1.5 * interquantile))[0]\n",
    "        #indices_outliers = np.argwhere((tX_tilda_2_3[index_column_valid,i] >= column_85_quantile + 1.5 * interquantile) | \n",
    "                                  #(column_15_quantile - 1.5 * interquantile >= tX_tilda_2_3[index_column_valid,i]))\n",
    "        median = np.median(tX_1[index_column_valid, i], axis = 0)\n",
    "        #print(np.sort(tX_tilda_2_3[index_column_valid[indices_outliers],i]).T)\n",
    "        #print(np.where(tX_tilda_2_3[indices_outliers,i])==-999.)\n",
    "        #print(median)\n",
    "        tX_1[index_column_valid[indices_outliers],i] =  median\n",
    "tX_1 = np.delete(tX_1, col_to_delete, axis=1)\n",
    "print(col_to_delete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we substitute the -999 values with the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, tX_2_3.shape[1]):\n",
    "    index_column_non_valid =np.where(tX_2_3[:,i] == -999.)[0]\n",
    "    index_column_valid =np.where(tX_2_3[:,i] != -999.)[0]\n",
    "    median = np.median(tX_2_3[index_column_valid, i], axis = 0)\n",
    "    tX_2_3[index_column_non_valid,i] =  median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, tX_1.shape[1]):\n",
    "    index_column_non_valid =np.where(tX_1[:,i] == -999.)[0]\n",
    "    index_column_valid =np.where(tX_1[:,i] != -999.)[0]\n",
    "    median = np.median(tX_1[index_column_valid, i], axis = 0)\n",
    "    tX_1[index_column_non_valid,i] =  median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, tX_0.shape[1]):\n",
    "    index_column_non_valid =np.where(tX_0[:,i] == -999.)[0]\n",
    "    index_column_valid =np.where(tX_0[:,i] != -999.)[0]\n",
    "    median = np.median(tX_0[index_column_valid, i], axis = 0)\n",
    "    tX_0[index_column_non_valid,i] =  median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tX_2_3[:,1:],_,_ = standardize(tX_2_3[:,1:]) #we standardize everything a part from the column added manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.97351249  0.46588281 ...  0.61614788 -1.36131161\n",
      "  -0.70374641]\n",
      " [ 1.         -0.82851663 -0.77157038 ...  0.11608109  1.71034105\n",
      "   0.2995537 ]\n",
      " [ 1.          1.3538447  -0.27431587 ...  0.07030726 -1.52202162\n",
      "   0.12704911]\n",
      " ...\n",
      " [ 1.          0.04006392  0.02513449 ...  0.25930888  0.22982758\n",
      "   0.42357227]\n",
      " [ 1.          0.66304099 -1.0843679  ...  0.29031696 -1.21821366\n",
      "  -0.20699627]\n",
      " [ 1.          0.04006392  0.31977857 ... -0.02271698 -0.62490751\n",
      "   0.05569682]]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(tX_2_3)\n",
    "print(np.count_nonzero(tX_2_3 == -999.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(tX_0[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_0[:,1:-1],_,_ = standardize(tX_0[:,1:-1]) \n",
    "# the last column is not standardized since it is the zero vector (to be dropped?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          1.05744907  0.7827665  ...  0.04662815 -0.7724943\n",
      "   0.        ]\n",
      " [ 1.          2.14505538 -1.37519852 ... -0.4674532  -1.44727052\n",
      "   0.        ]\n",
      " [ 1.         -0.24632406 -0.2496121  ...  0.0267496   0.12380588\n",
      "   0.        ]\n",
      " ...\n",
      " [ 1.         -0.0469687   0.00532098 ... -0.46524447 -0.8883482\n",
      "   0.        ]\n",
      " [ 1.         -0.60851918 -1.29333222 ...  0.46131675 -0.22629665\n",
      "   0.        ]\n",
      " [ 1.         -0.0469687   0.49300595 ... -0.86778508 -0.49908812\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(tX_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_1[:,1:],_,_ = standardize(tX_1[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000000e+00  1.55539992e+00  7.27047143e-01 ...  3.98445313e-01\n",
      "   6.45414781e-01 -4.14297220e-01]\n",
      " [ 1.00000000e+00  1.45598722e-03  3.58462301e+00 ...  1.12748232e+00\n",
      "  -1.10752634e+00 -4.89864560e-01]\n",
      " [ 1.00000000e+00  1.36261180e+00 -1.05809645e+00 ... -3.92076740e-01\n",
      "  -9.40265168e-01 -1.01072441e+00]\n",
      " ...\n",
      " [ 1.00000000e+00  1.45598722e-03  1.01732036e+00 ... -4.67286130e-01\n",
      "  -3.80160315e-01  8.39087556e-01]\n",
      " [ 1.00000000e+00  6.75509966e-01  9.95415259e-01 ... -6.76994064e-01\n",
      "   1.39533906e+00  5.32418071e-01]\n",
      " [ 1.00000000e+00 -2.21030015e-01  4.74893699e-01 ...  9.88591985e-01\n",
      "  -8.30516498e-02 -5.76298292e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(tX_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We insert the column for the bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_tilda_0 = np.insert(tX_0, 0, np.ones(tX_0.shape[0]), axis=1)\n",
    "tX_tilda_1 = np.insert(tX_1, 0, np.ones(tX_1.shape[0]), axis=1)\n",
    "tX_tilda_2_3 = np.insert(tX_2_3, 0, np.ones(tX_2_3.shape[0]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors = ['red', 'blue']\n",
    "# x_pos=[]\n",
    "# x_neg=[]\n",
    "\n",
    "# for j in range(len(y)):\n",
    "#  if(y[j]==1):\n",
    "#       x_pos.insert(0,tX[j])\n",
    "#    else:\n",
    "#        x_neg.insert(0,tX[j])\n",
    "# xpos = np.array(x_pos)\n",
    "# xneg = np.array(x_neg)\n",
    "# for i in range(tX.shape[1]):\n",
    "#  plt.hist(xpos[:,i], alpha = 0.5, color = 'r', bins = 100)\n",
    "#  plt.hist(xneg[:,i], alpha = 0.5, color = 'b', bins = 100)\n",
    "#  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    N = y.shape[0]\n",
    "    e = y - tx @ w \n",
    "    loss = 1/(2*N) * np.dot(e,e)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    N = y.shape[0]\n",
    "    e = y - tx @ w\n",
    "    gradient = -(1/N) * (tx.T) @ (e)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        gradient = compute_gradient(y,tx,w)\n",
    "        w = w - gamma * gradient\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        # print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "        # bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    N = y.shape[0]\n",
    "    random_number = random.randint(0,N)\n",
    "    #random_number =1\n",
    "    xn = tx[random_number,:]\n",
    "    random_gradient = - np.dot(xn, y[random_number] - np.dot(xn,w))\n",
    "    return random_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        stoch_gradient = compute_stoch_gradient(y,tx,w)\n",
    "        w = w - gamma * stoch_gradient\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        # print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "        #    bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "\n",
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    forcing_term = np.transpose(tx) @ y\n",
    "    coefficient_matrix = np.transpose(tx) @ tx\n",
    "    w = np.linalg.solve(coefficient_matrix, forcing_term)\n",
    "    return w\n",
    "\n",
    "def test_your_least_squares(y, tx):\n",
    "    \"\"\"compare the solution of the normal equations with the weights returned by gradient descent algorithm.\"\"\"\n",
    "    w_least_squares = least_squares(y, tx)\n",
    "    initial_w = np.zeros(tx.shape[1])\n",
    "    max_iters = 50\n",
    "    gamma = 0.7\n",
    "    losses_gradient_descent, w_gradient_descent = gradient_descent(y, tx, initial_w, max_iters, gamma)\n",
    "    w = w_gradient_descent[-1]\n",
    "    err = np.linalg.norm(w_least_squares-w)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    N = tx.shape\n",
    "    lambda_prime = 2 * N[0] * lambda_\n",
    "    coefficient_matrix = np.transpose(tx) @ tx + lambda_prime * np.eye(N[1])\n",
    "    forcing_term = np.transpose(tx) @ y\n",
    "    w = np.linalg.solve(coefficient_matrix, forcing_term)\n",
    "    return w\n",
    "\n",
    "def debug_ridge(y, tx):\n",
    "    \"\"\"debugging the ridge regression by setting lambda=0.\"\"\"\n",
    "    w_least_squares = least_squares(y, tx)\n",
    "    w_0 = ridge_regression(y, tx, 0)\n",
    "    err = np.linalg.norm(w_least_squares-w_0)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply the sigmoid function on t.\"\"\"\n",
    "    return np.exp(t) / (1+np.exp(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the loss: negative log likelihood.\"\"\"\n",
    "    N = y.shape[0]\n",
    "    e = - (y*np.log(sigmoid(tx @ w)) +\n",
    "                  (1-y)*np.log(1-sigmoid(tx @ w)))\n",
    "    return e.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    return np.transpose(tx) @ (sigmoid(tx @ w) - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    grad = calculate_gradient(y, tx, w)\n",
    "    w = w - gamma * grad\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hessian(y, tx, w):\n",
    "    \"\"\"return the Hessian of the loss function.\"\"\"\n",
    "    diag = sigmoid(tx @ w) * (1 - sigmoid(tx @ w))\n",
    "    D = diag * np.eye(tx.shape[0])\n",
    "    return np.transpose(tx) @ D @ tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, w):\n",
    "    \"\"\"return the loss, gradient, and Hessian.\"\"\"\n",
    "    grad = calculate_gradient(y, tx, w)\n",
    "    hess = calculate_hessian(y, tx, w)\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    return loss, grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_newton_method(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step on Newton's method.\n",
    "    return the loss and updated w.\n",
    "    \"\"\"\n",
    "    loss, grad, hess = logistic_regression(y, tx, w)\n",
    "    sol = np.linalg.solve(hess, grad)\n",
    "    w = w - gamma * sol\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "    \"\"\"return the loss, gradient\"\"\"\n",
    "    loss = calculate_loss(y, tx, w) + lambda_*np.linalg.norm(w) ** 2\n",
    "    grad = calculate_gradient(y, tx, w) + 2*lambda_*w\n",
    "    hess = calculate_hessian(y, tx, w) + 2*lambda_*np.eye(w.shape[0])\n",
    "    return loss, grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_penalized_gradient(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent, using the penalized logistic regression.\n",
    "    Return the loss and updated w.\n",
    "    \"\"\"\n",
    "    loss, grad, hess = penalized_logistic_regression(y, tx, w, lambda_)\n",
    "    sol = np.linalg.solve(hess, grad)\n",
    "    w = w - gamma * sol\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    N = y.shape[0]\n",
    "    np.random.seed(seed)\n",
    "    interval = int(np.floor(N / k_fold))\n",
    "    indices = np.random.permutation(N)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    N = y.shape[0]\n",
    "    k_fold = k_indices.shape[0]\n",
    "    list_ = []\n",
    "    interval = int(N/k_fold)\n",
    "    for i in range(k_fold):\n",
    "        if i != k:\n",
    "            list_.append(i)\n",
    "    x_training = np.zeros(int((k_fold-1)/k_fold*N))\n",
    "    y_training = np.zeros(int((k_fold-1)/k_fold*N))\n",
    "    for j in range(len(list_)):\n",
    "        x_training[interval*(j):interval*(j+1)] = x[np.array([k_indices[list_[j]]])]\n",
    "    x_testing = x[k_indices[k]]\n",
    "    for j in range(len(list_)):\n",
    "        y_training[interval*(j):interval*(j+1)] = y[np.array([k_indices[list_[j]]])]\n",
    "    y_testing = y[k_indices[k]]\n",
    "    x_training_augmented = build_poly(x_training, degree)\n",
    "    x_testing_augmented = build_poly(x_testing, degree)\n",
    "    w_opt_training = ridge_regression(y_training, x_training_augmented, lambda_)\n",
    "    loss_tr = compute_mse(y_training, x_training_augmented, w_opt_training)\n",
    "    loss_te = compute_mse(y_testing, x_testing_augmented, w_opt_training)\n",
    "    return loss_tr, loss_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial of different models on the three training matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(weights, tX_test):\n",
    "    y = np.array(tX_test) @ np.array(weights)\n",
    "    labels = [1 if l > 0 else -1 for l in y]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indeces_lepton = [0, 1, 2, 3, 7, 8, 9, 10, 11, 12, 16, 17, 18, 21, 22]\n",
    "# indeces_hadronic_tau = [0, 3, 7, 8, 9, 10, 11, 13, 14, 15, 21, 22]\n",
    "# indeces_jet = [0, 4, 5, 6, 8, 9, 12, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
    "# indeces_MTE = [0, 1, 3, 8, 9, 11, 19, 20, 21, 22]\n",
    "\n",
    "# tX_lepton_test = tX_test[:, indeces_lepton]\n",
    "# tX_tilda_lepton_test = np.insert(tX_lepton_test, 0, np.ones(tX_test.shape[0]), axis=1)\n",
    "# labels_lepton = predict_labels(w_opt_lepton, tX_tilda_lepton_test)\n",
    "# print(1 / np.array(labels_lepton).shape[0] * np.count_nonzero(np.array(labels_lepton) == 1))\n",
    "\n",
    "# tX_hadronic_tau_test = tX_test[:, indeces_hadronic_tau]\n",
    "# tX_tilda_hadronic_tau_test = np.insert(tX_hadronic_tau_test, 0, np.ones(tX_test.shape[0]), axis=1)\n",
    "# labels_hadronic_tau = predict_labels(w_opt_hadronic_tau, tX_tilda_hadronic_tau_test)\n",
    "# print(1 / np.array(labels_hadronic_tau).shape[0] * np.count_nonzero(np.array(labels_hadronic_tau) == 1))\n",
    "\n",
    "# tX_jet_test = tX_test[:, indeces_jet]\n",
    "# tX_tilda_jet_test = np.insert(tX_jet_test, 0, np.ones(tX_test.shape[0]), axis=1)\n",
    "# labels_jet = predict_labels(w_opt_jet, tX_tilda_jet_test)\n",
    "# print(1 / np.array(labels_jet).shape[0] * np.count_nonzero(np.array(labels_jet) == 1))\n",
    "\n",
    "# tX_MTE_test = tX_test[:, indeces_MTE]\n",
    "# tX_tilda_MTE_test = np.insert(tX_MTE_test, 0, np.ones(tX_test.shape[0]), axis=1)\n",
    "# labels_MTE = predict_labels(w_opt_MTE, tX_tilda_MTE_test)\n",
    "# print(1 / np.array(labels_MTE).shape[0] * np.count_nonzero(np.array(labels_MTE) == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRIAL, UNDERESTIMATION OF THE TRUE PROBABILITY TO GET A BOSON\n",
    "# count = np.array(labels_MTE) + np.array(labels_lepton) + np.array(labels_hadronic_tau) + np.array(labels_jet)\n",
    "# TrueSignal = np.array([int(bool((counted == 4))) for counted in count])\n",
    "# print( 1 / np.array(count).shape[0] * np.count_nonzero(np.array(TrueSignal) == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_503/2223051364.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mOUTPUT_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;31m# TODO: fill in desired name of output file for submission\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcreate_csv_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'weights' is not defined"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
