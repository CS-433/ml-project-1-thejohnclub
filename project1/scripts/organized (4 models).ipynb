{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our libraries\n",
    "from proj1_helpers import *\n",
    "from proj1_input_man import *\n",
    "from proj1_linear_model import *\n",
    "from proj1_ridge_regress import *\n",
    "from proj1_logistic import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv' # train data path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n"
     ]
    }
   ],
   "source": [
    "print(tX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the labels to {0,1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the array from -1 1 to 0 1\n",
    "# label simplification\n",
    "# y == 0 non detected Boson, y == 1 detected Boson\n",
    "y_ = np.array([0 if l == -1 else 1 for l in y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All of the following parts are the same for the test set, should be functions\n",
    "\n",
    "# less code = better code (in this case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the features by the number of jets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_0, tX_1, tX_2_3 = split_to_Jet_Num(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tX_0, tX_1, tX_2, tX_3 = alternative_split_to_Jet_Num(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288.1303027432567\n",
      "38.07417940661156\n",
      "48.78977837574543\n",
      "83.81996788992959\n",
      "1.4698926707450346\n",
      "317.2726868423743\n",
      "2.8672697710964288\n",
      "0.8272411967910458\n",
      "32.29848326337556\n",
      "150.13499581029197\n",
      "1.2471331404825152\n",
      "0.9433373230483033\n",
      "0.3876270487619129\n",
      "32.43010841851103\n",
      "1.2047095881208543\n",
      "1.8241134002031094\n",
      "32.96582686964663\n",
      "1.2176561938554824\n",
      "1.8180600465624461\n",
      "49.697288657467055\n",
      "1.8123415757149903\n",
      "158.8254499542401\n",
      "76.3120342898988\n",
      "1.5732280712204414\n",
      "1.8067332213365404\n",
      "40.12542350677477\n",
      "1.775313755049286\n",
      "1.8147406038684843\n",
      "128.45133643867\n"
     ]
    }
   ],
   "source": [
    "for i in range(tX_1.shape[1]):\n",
    "    print(np.std(tX_3[:,i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing also the output by the type of particle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_0,y_1,y_2_3 = split_labels_to_Jet_Num(y_,tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_0, y_1, y_2, y_3 = alternative_split_labels_to_Jet_Num(y_, tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a column of zeros and ones to detect whether the mass has been measured or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the indices where the mass is not calculated, add the column which has 0 in those indices\n",
    "# and 1 everywhere else for all matrices 0,1,2_3\n",
    "tX_0 = find_mass(tX_0)\n",
    "tX_1 = find_mass(tX_1)\n",
    "# tX_2_3 = find_mass(tX_2_3)\n",
    "tX_2 = find_mass(tX_2)\n",
    "tX_3 = find_mass(tX_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Throwing away the outliers from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 7, 13, 23, 24, 25, 26, 27, 28, 29]\n",
      "(99913, 19)\n",
      "[5, 6, 7, 13, 26, 27, 28]\n",
      "(77544, 23)\n",
      "[]\n",
      "[]\n",
      "[[ 1.00000e+00  1.38470e+02  5.16550e+01 ...  1.24000e+00 -2.47500e+00\n",
      "   1.13497e+02]\n",
      " [ 1.00000e+00  1.48754e+02  2.88620e+01 ...  1.31000e-01 -2.76700e+00\n",
      "   1.79877e+02]\n",
      " [ 1.00000e+00  1.41481e+02  7.36000e-01 ... -7.98000e-01 -2.78500e+00\n",
      "   2.78009e+02]\n",
      " ...\n",
      " [ 1.00000e+00  1.19934e+02  2.00780e+01 ... -1.72500e+00 -2.75600e+00\n",
      "   1.12938e+02]\n",
      " [ 1.00000e+00  1.26151e+02  2.90230e+01 ... -5.99000e-01 -2.52500e+00\n",
      "   1.93099e+02]\n",
      " [ 1.00000e+00  1.15254e+02  4.71560e+01 ... -5.80000e-02 -1.13700e+00\n",
      "   1.74176e+02]]\n",
      "[[ 1.00000e+00  8.97440e+01  1.35500e+01 ...  2.24000e-01  3.10600e+00\n",
      "   1.93660e+02]\n",
      " [ 1.00000e+00  1.14744e+02  1.02860e+01 ...  1.77300e+00 -2.07900e+00\n",
      "   1.65640e+02]\n",
      " [ 1.00000e+00  1.21681e+02  6.04100e+00 ... -1.25700e+00 -6.09000e-01\n",
      "   2.53461e+02]\n",
      " ...\n",
      " [ 1.00000e+00 -9.99000e+02  8.38710e+01 ...  3.07000e+00  1.61200e+00\n",
      "   2.71833e+02]\n",
      " [ 1.00000e+00 -9.99000e+02  3.80830e+01 ...  5.15000e-01  4.16000e-01\n",
      "   2.03569e+02]\n",
      " [ 1.00000e+00  1.30075e+02  3.91800e+00 ...  5.78000e-01 -2.21500e+00\n",
      "   5.46066e+02]]\n"
     ]
    }
   ],
   "source": [
    "tX_0, col_to_delete_0 = fix_array(tX_0, 0)\n",
    "print(tX_0.shape)\n",
    "tX_1, col_to_delete_1 = fix_array(tX_1, 1)\n",
    "print(tX_1.shape)\n",
    "tX_2, _ = fix_array(tX_2, 2)\n",
    "tX_3, _ = fix_array(tX_3, 3)\n",
    "print(tX_2)\n",
    "print(tX_3)\n",
    "# print(tX_2_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we substitute the -999 values with the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_0, column_median_0 = fix_median(tX_0)\n",
    "tX_1, column_median_1 = fix_median(tX_1)\n",
    "# tX_2_3 = fix_median(tX_2_3)\n",
    "tX_2, column_median_2 = fix_median(tX_2)\n",
    "tX_3, column_median_3 = fix_median(tX_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tX_2_3[:,1:], mean_2_3, std_2_3 = standardize(tX_2_3[:,1:])\n",
    "tX_3[:,1:], mean_3, std_3 = standardize(tX_3[:,1:])\n",
    "tX_2[:,1:], mean_2, std_2 = standardize(tX_2[:,1:]) #we standardize everything a part from the column added manually\n",
    "tX_0[:,1:], mean_0, std_0 = standardize(tX_0[:,1:])\n",
    "tX_1[:,1:], mean_1, std_1 = standardize(tX_1[:,1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We insert the column for the bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_tilda_0 = np.insert(tX_0, 0, np.ones(tX_0.shape[0]), axis=1)\n",
    "tX_tilda_1 = np.insert(tX_1, 0, np.ones(tX_1.shape[0]), axis=1)\n",
    "# tX_tilda_2_3 = np.insert(tX_2_3, 0, np.ones(tX_2_3.shape[0]), axis=1)\n",
    "tX_tilda_2 = np.insert(tX_2, 0, np.ones(tX_2.shape[0]), axis=1)\n",
    "tX_tilda_3 = np.insert(tX_3, 0, np.ones(tX_3.shape[0]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following cells of code aim at finetuning the hyperparameters of the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the optimal degree for gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-91f3dc9f6ecc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdegree_opt_0_GD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinetune_GD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX_tilda_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdegree_opt_1_GD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinetune_GD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX_tilda_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdegree_opt_2_3_GD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinetune_GD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX_tilda_2_3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_2_3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\MachineLearning\\Machine_Learning_Project_1\\Machine-Learning-P1\\project1\\scripts\\proj1_linear_model.py\u001b[0m in \u001b[0;36mfinetune_GD\u001b[1;34m(tX, y, k_fold, degrees)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mcurrent_sum_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m             \u001b[0mcurrent_test_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation_GD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegrees\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m10e-4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m             \u001b[0mcurrent_sum_test\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcurrent_test_acc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mtesting_acc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent_sum_test\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mk_fold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\MachineLearning\\Machine_Learning_Project_1\\Machine-Learning-P1\\project1\\scripts\\proj1_linear_model.py\u001b[0m in \u001b[0;36mcross_validation_GD\u001b[1;34m(y, x, k_indices, k, degree, gamma)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0mx_training_augmented\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_training\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mx_testing_augmented\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_testing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m     \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mws\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mleast_squares_GD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_training\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_training_augmented\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_training_augmented\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m     \u001b[0mw_opt_training\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mws\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[0mpredictions_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_testing_augmented\u001b[0m\u001b[1;33m@\u001b[0m\u001b[0mw_opt_training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\MachineLearning\\Machine_Learning_Project_1\\Machine-Learning-P1\\project1\\scripts\\proj1_linear_model.py\u001b[0m in \u001b[0;36mleast_squares_GD\u001b[1;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitial_w\u001b[0m \u001b[1;31m# Initialization of the weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# calculate the MSE loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# calculate the gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgradient\u001b[0m \u001b[1;31m# conduct a step of gradient descent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\MachineLearning\\Machine_Learning_Project_1\\Machine-Learning-P1\\project1\\scripts\\proj1_linear_model.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# N = Number of samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtx\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mw\u001b[0m \u001b[1;31m# e = error vector (truth - prediction)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# calculate the average loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "degree_opt_0_GD = finetune_GD(tX_tilda_0, y_0)\n",
    "degree_opt_1_GD = finetune_GD(tX_tilda_1, y_1)\n",
    "degree_opt_2_3_GD = finetune_GD(tX_tilda_2_3, y_2_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_opt_1_GD = finetune_GD(tX_tilda_1, y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell takes a long time to execute, the hyperparameters found are:\n",
    "# degree =2 for tX_tilda_0 with an accuracy of 0.82694771 on the validation set\n",
    "# degree =3 for tX_tilda_1 with an accuracy of 0.7937687 on the validation set\n",
    "# degree =3 for tX_tilda_2_3 with an accuracy of 0.81856907 on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_GD_0 = optimal_weights_GD(tX_tilda_0,y_0,degree_opt_0_GD,lambda_opt_0_GD)\n",
    "w_GD_1 = optimal_weights_GD(tX_tilda_1,y_1,degree_opt_1_GD,lambda_opt_1_GD)\n",
    "w_GD_2_3 = optimal_weights_GD(tX_tilda_2_3,y_2_3,degree_opt_2_3_GD,lambda_opt_2_3_GD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the optimal degree for stochastic gradient descent for feature augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_opt_0_SGD = finetune_SGD(tX_tilda_0, y_0)\n",
    "degree_opt_1_SGD = finetune_SGD(tX_tilda_1, y_1)\n",
    "degree_opt_2_SGD = finetune_SGD(tX_tilda_2, y_2)\n",
    "degree_opt_3_SGD = finetune_SGD(tX_tilda_3, y_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the optimal lambda for ridge regression and the optimal degree for feature augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.82453208 0.82971675 0.83003703 0.83069763 0.83225903]\n",
      " [0.82674407 0.83309979 0.83450105 0.83571214 0.83584226]\n",
      " [0.8267741  0.83317986 0.83463117 0.83598238 0.83606246]\n",
      " [0.82385147 0.82861575 0.82860575 0.82975678 0.831108  ]\n",
      " [0.82386148 0.8285557  0.82854569 0.82957662 0.83100791]\n",
      " [0.82619357 0.83258933 0.83333    0.83472125 0.83492143]\n",
      " [0.82753478 0.83435092 0.83613252 0.83751376 0.83768392]\n",
      " [0.82670403 0.83314983 0.83462116 0.83584226 0.83601241]\n",
      " [0.82399159 0.82879592 0.82897608 0.83000701 0.83127815]\n",
      " [0.82492243 0.8302272  0.83026724 0.83119808 0.83253929]]\n",
      "[0.00774608] [6]\n",
      "[[0.76926747 0.78622646 0.78794171 0.7898633  0.79083054]\n",
      " [0.77648955 0.79468661 0.79745938 0.79971628 0.80032241]\n",
      " [0.77695383 0.79518958 0.79802682 0.80030952 0.80060614]\n",
      " [0.76648182 0.78403405 0.78503998 0.78678102 0.78828991]\n",
      " [0.76632706 0.78407274 0.78492391 0.78672943 0.78823833]\n",
      " [0.7740521  0.79275213 0.79527986 0.797382   0.79789786]\n",
      " [0.7778179  0.79740779 0.79969048 0.80255352 0.80313387]\n",
      " [0.77673459 0.79485427 0.79794945 0.79996131 0.80042559]\n",
      " [0.76724271 0.78451122 0.78573639 0.78752902 0.78876709]\n",
      " [0.77018313 0.78687129 0.78875419 0.79152695 0.79219758]]\n",
      "[0.00774608] [6]\n",
      "[[0.81095782 0.82263027 0.82511166 0.82687841 0.82685856]\n",
      " [0.81151365 0.8222134  0.8248933  0.82729529 0.82679901]\n",
      " [0.81113648 0.82259057 0.82501241 0.82691811 0.82715633]\n",
      " [0.81165261 0.82227295 0.82467494 0.82713648 0.82654094]\n",
      " [0.8116129  0.82231266 0.82465509 0.82711663 0.82656079]\n",
      " [0.81107692 0.82251117 0.82499256 0.82717618 0.82717618]\n",
      " [0.81165261 0.82253102 0.82445658 0.82697767 0.82638213]\n",
      " [0.8116129  0.82231266 0.82437717 0.82679901 0.82638213]\n",
      " [0.81117618 0.8221737  0.82503226 0.82747395 0.82695782]\n",
      " [0.81155335 0.82257072 0.82437717 0.82699752 0.82628288]]\n",
      "[0.00546996] [5]\n",
      "[[0.81046931 0.82522563 0.83145307 0.83555957 0.83709386]\n",
      " [0.81019856 0.82554152 0.83172383 0.83601083 0.8375    ]\n",
      " [0.80965704 0.82509025 0.83109206 0.83542419 0.83700361]\n",
      " [0.80920578 0.82373646 0.83100181 0.8351083  0.83677798]\n",
      " [0.80934116 0.82382671 0.83055054 0.83501805 0.83700361]\n",
      " [0.80974729 0.82504513 0.83113718 0.83542419 0.83691336]\n",
      " [0.80970217 0.82477437 0.83095668 0.83524368 0.83677798]\n",
      " [0.8101083  0.82504513 0.83140794 0.83542419 0.83731949]\n",
      " [0.81028881 0.82558664 0.83190433 0.8359657  0.83759025]\n",
      " [0.80938628 0.82382671 0.83059567 0.8349278  0.83695848]]\n",
      "[0.00047263] [6]\n"
     ]
    }
   ],
   "source": [
    "lambda_opt_0_ridge, degree_opt_0_ridge = finetune_ridge(tX_tilda_0, y_0, lambdas = random_interval(0.001, 0.1, 10))\n",
    "lambda_opt_1_ridge, degree_opt_1_ridge = finetune_ridge(tX_tilda_1, y_1, lambdas = random_interval(0.001, 0.1, 10))\n",
    "lambda_opt_2_ridge, degree_opt_2_ridge = finetune_ridge(tX_tilda_2, y_2, lambdas = random_interval(10e-6, 10e-3, 10))\n",
    "lambda_opt_3_ridge, degree_opt_3_ridge = finetune_ridge(tX_tilda_3, y_3, lambdas = random_interval(10e-6, 10e-3, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_opt_1_ridge, degree_opt_1_ridge = finetune_ridge(tX_tilda_1, y_1, lambdas = random_interval(0.001, 0.1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check\n",
    "print(lambda_opt_0_ridge, degree_opt_0_ridge)\n",
    "print(lambda_opt_1_ridge, degree_opt_1_ridge)\n",
    "# print(lambda_opt_2_3_ridge, degree_opt_2_3_ridge)\n",
    "print(lambda_opt_2_ridge, degree_opt_2_ridge)\n",
    "print(lambda_opt_3_ridge, degree_opt_3_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarizing the best parameters found are\n",
    "# lambda = 0.00316228 and degree=6 for tX_tilda_0 with a validation accuracy of 0.83847463\n",
    "# [0.00558459] [6] gives accuracy = 0.83814433 for tX_tilda_0\n",
    "# lambda = 0.00316228 and degree= 6 for tX_tilda_1 with a validation accuracy of 0.80354656\n",
    "# lambda = 0.0047263 and degree = 6 for tX_tilda_3 gives 0.83759025 approximately\n",
    "# lambda = 0.00546996 and degree = 5 for tX_tilda_2 gives 0.82747395\n",
    "# lambda=1e-5 and degree=6 for tX_tilda_2_3 with a validation accuracy of 0.83021781"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the optimal weights with the calculated hyper parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_ridge_0 = optimal_weights_ridge(tX_tilda_0, y_0, degree_opt_0_ridge, lambda_opt_0_ridge)\n",
    "# w_ridge_1 = optimal_weights_ridge(tX_tilda_1, y_1, degree_opt_1_ridge, lambda_opt_1_ridge)\n",
    "w_ridge_1 = optimal_weights_ridge(tX_tilda_1, y_1, 6, 0.00774608)\n",
    "# w_ridge_2_3 = optimal_weights_ridge(tX_tilda_2_3, y_2_3, degree_opt_2_3_ridge, lambda_opt_2_3_ridge)\n",
    "# w_ridge_2 = optimal_weights_ridge(tX_tilda_2, y_2, degree_opt_2_ridge, lambda_opt_2_ridge)\n",
    "# w_ridge_2_3 = optimal_weights_ridge(tX_tilda_3, y_3, degree_opt_3_ridge, lambda_opt_3_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the optimal lambda for logistic regression and the optimal degree for feature augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.] [2]\n",
      "0.8266374409480343\n",
      "[1.] [2]\n",
      "0.79204064789023\n",
      "[10.] [2]\n",
      "0.8153088772431316\n",
      "[0.001] [2]\n",
      "0.8231366179390002\n"
     ]
    }
   ],
   "source": [
    "lambda_opt_0_logistic, degree_opt_0_logistic = finetune_logistic(tX_tilda_0, y_0, gamma = 1.7783e-04, degrees =np.arange(1, 4) , lambdas=np.logspace(-2, 1, 7) )\n",
    "lambda_opt_1_logistic, degree_opt_1_logistic = finetune_logistic(tX_tilda_1, y_1, gamma = 3.16228e-03, degrees = np.arange(1, 4), lambdas = np.logspace(-2, 1, 7) )\n",
    "# lambda_opt_2_3_logistic,degree_opt_2_3_logistic = finetune_logistic(tX_tilda_2_3, y_2_3, gamma =0.00017783, degrees = np.arange(1, 3), lambdas = np.logspace(-2, 1, 7) )\n",
    "lambda_opt_2_logistic, degree_opt_2_logistic = finetune_logistic(tX_tilda_2, y_2, gamma = 1.7783e-04, degrees = np.arange(1, 4), lambdas = np.logspace(-3, 1, 7))\n",
    "lambda_opt_3_logistic, degree_opt_3_logistic = finetune_logistic(tX_tilda_3, y_3, gamma = 1.7783e-04, degrees = np.arange(1, 4), lambdas = np.logspace(-3, 1, 7))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001] [2]\n",
      "0.8231366179390002\n"
     ]
    }
   ],
   "source": [
    "lambda_opt_3_logistic, degree_opt_3_logistic = finetune_logistic(tX_tilda_3, y_3, gamma = 1.7783e-04, degrees = np.arange(1, 4), lambdas = np.logspace(-3, 1, 7))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_opt_1_logistic,degree_opt_1_logistic = finetune_logistic(tX_tilda_1, y_1, gamma = 1.7783e-04, degrees = np.arange(1, 4), lambdas = random_interval(1.0e-07, 1.0e-02, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarizing the best parameters found are\n",
    "# lambda=1.95096 and degree=2 for tX_tilda_0 with a validation accuracy of 0.827668 #lambda =9.64683289, acc=0.82664\n",
    "# lambda=1.6672 and degree=2 for tX_tilda_1 with a validation accuracy of 0.79229 #lamda = 13.00571 acc = 0.792466 gamma = 0.0004\n",
    "# lambda=0.6835 and degree=2 for tX_tilda_2_3 with a validation accuracy of 0.8151\n",
    "# lambda = 10 and degree = 2 for tX_tilda_2 gives an accuracy 0.8153088 \n",
    "# lambda = 0.001 and degree = 2 for tX_tilda_3 gives an accuracy 0.8231366 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the optimal weights with the calculated hyper parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.00017783\n",
    "w_logistic_0 = optimal_weights_logistic(tX_tilda_0, y_0, gamma, degree_opt_0_logistic, lambda_opt_0_logistic)\n",
    "w_logistic_1 = optimal_weights_logistic(tX_tilda_1, y_1, gamma, degree_opt_1_logistic, lambda_opt_1_logistic)\n",
    "w_logistic_2 = optimal_weights_logistic(tX_tilda_2, y_2, gamma, degree_opt_2_logistic, lambda_opt_2_logistic)\n",
    "w_logistic_3 = optimal_weights_logistic(tX_tilda_3, y_3, gamma, degree_opt_3_logistic, lambda_opt_3_logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate the optimal lambda for logistic regression with batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_opt_0_logistic_batch,degree_opt_0_logistic_batch = finetune_batch_logistic(tX_tilda_0, y_0, gamma = 1.7783e-04, degrees =np.arange(1, 4) , lambdas=random_interval(0.8, 1.5, 5) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# open the test file\n",
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238, 30)\n"
     ]
    }
   ],
   "source": [
    "print(tX_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now format the tX_test as we did for tX_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we split the test into the three subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test_0, tX_test_1, tX_test_2, tX_test_3 = alternative_split_to_Jet_Num(tX_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a column of zeros and ones to detect whether the mass has been measured or not\n",
    "This should be done prior to splitting it is the same procedure and just wastes space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the indices where the mass is not calculated, add the column which has 0 in those indices\n",
    "# and 1 everywhere else for all matrices 0,1,2_3\n",
    "tX_test_0 = find_mass(tX_test_0)\n",
    "tX_test_1 = find_mass(tX_test_1)\n",
    "tX_test_2 = find_mass(tX_test_2)\n",
    "tX_test_3 = find_mass(tX_test_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We drop the same columns we have dropped for the X training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tX_test_0 = fix_array(tX_test_0, 0)\n",
    "# print(tX_test_0.shape)\n",
    "# tX_test_1 = fix_array(tX_test_1, 1)\n",
    "# print(tX_test_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test_0 = np.delete(tX_test_0, col_to_delete_0, axis=1)\n",
    "tX_test_1 = np.delete(tX_test_1, col_to_delete_1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we substitute the -999 values with the median\n",
    "This should also be done with a function it is the same thing repeated thrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test_0 = fix_median_test(tX_test_0, column_median_0)\n",
    "tX_test_1 = fix_median_test(tX_test_1, column_median_1)\n",
    "tX_test_2 = fix_median_test(tX_test_2, column_median_2)\n",
    "tX_test_3 = fix_median_test(tX_test_3, column_median_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227458, 19)\n",
      "(39,)\n"
     ]
    }
   ],
   "source": [
    "print(tX_test_0.shape)\n",
    "print(w_logistic_0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We standardize the test set using the mean and the standard deviation of the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tX_test_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tX_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the data in the test set\n",
    "# should have used the same function both here and on the training part same process this is reduntant\n",
    "def standardize_test(x, mean, std):\n",
    "    \"\"\"Standardize the test set.\"\"\"\n",
    "    x = x - mean \n",
    "    x = x / std\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test_0[:,1:] = standardize_test(tX_test_0[:,1:], mean_0, std_0)  #we standardize everything a part from the column added manually\n",
    "tX_test_1[:,1:] = standardize_test(tX_test_1[:,1:], mean_1, std_1)  #we standardize everything a part from the column added manually\n",
    "tX_test_2[:,1:] = standardize_test(tX_test_2[:,1:], mean_2, std_2) #we standardize everything a part from the column added manually\n",
    "tX_test_3[:,1:] = standardize_test(tX_test_3[:,1:], mean_3, std_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We insert the column for the bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_tilda_test_0 = np.insert(tX_test_0, 0, np.ones(tX_test_0.shape[0]), axis=1) #the first column now is all ones and is used for bias\n",
    "tX_tilda_test_1 = np.insert(tX_test_1, 0, np.ones(tX_test_1.shape[0]), axis=1) #the first column now is all ones and is used for bias\n",
    "tX_tilda_test_2 = np.insert(tX_test_2, 0, np.ones(tX_test_2.shape[0]), axis=1) #the first column now is all ones and is used for bias\n",
    "tX_tilda_test_3 = np.insert(tX_test_3, 0, np.ones(tX_test_3.shape[0]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We make the predictions with GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_GD_0 = predict_GD(tX_tilda_test_0,w_GD_0,degree_opt_0_GD)\n",
    "predictions_GD_1 = predict_GD(tX_tilda_test_1,w_GD_1,degree_opt_1_GD)\n",
    "predictions_GD_2_3 = predict_GD(tX_tilda_test_2_3,w_GD_2_3,degree_opt_2_3_GD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We make the predictions with ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_ridge_0 = predict_ridge(tX_tilda_test_0, w_ridge_0,degree_opt_0_ridge)\n",
    "# predictions_ridge_1 = predict_ridge(tX_tilda_test_1, w_ridge_1,degree_opt_1_ridge)\n",
    "predictions_ridge_1 = predict_ridge(tX_tilda_test_1, w_ridge_1, 6)\n",
    "# predictions_ridge_2_3 = predict_ridge(tX_tilda_test_2_3, w_ridge_2_3, degree_opt_2_3_ridge)\n",
    "# predictions_ridge_2 = predict_ridge(tX_tilda_test_2, w_ridge_2, degree_opt_2_ridge)\n",
    "# predictions_ridge_2 = predict_ridge(tX_tilda_test_3, w_ridge_3, degree_opt_3_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_logistic_0 = predict_logistic(tX_tilda_test_0, w_logistic_0, degree_opt_0_logistic)\n",
    "predictions_logistic_1 = predict_logistic(tX_tilda_test_1, w_logistic_1, degree_opt_1_logistic)\n",
    "predictions_logistic_2 = predict_logistic(tX_tilda_test_2, w_logistic_2, degree_opt_2_logistic)\n",
    "predictions_logistic_3 = predict_logistic(tX_tilda_test_3, w_logistic_3, degree_opt_3_logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to reconstruct a single vector of predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate final prediction list and print it in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_predictions_GD = create_output(tX_test,predictions_GD_0,predictions_GD_1,predictions_GD_2_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_predictions_ridge = create_output(tX_test,predictions_ridge_0,predictions_ridge_1,predictions_ridge_2_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_predictions_logistic = create_output(tX_test,predictions_logistic_0,predictions_logistic_1,predictions_logistic_2_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mixed_predictions = create_output(tX_test, predictions_logistic_0, predictions_logistic_1, predictions_logistic_2, predictions_logistic_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1 -1 ... -1 -1 -1]\n",
      "5.279477965218799e-06\n"
     ]
    }
   ],
   "source": [
    "print(final_mixed_predictions)\n",
    "print(1 / len(final_mixed_predictions) * np.count_nonzero(final_mixed_predictions == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this does not seem to be used and should be removed in such a case\n",
    "#def predict_labels(weights, tX_test):\n",
    "#    y = np.array(tX_test) @ np.array(weights)\n",
    "#    labels = [1 if l > 0 else -1 for l in y]\n",
    "#    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH_GD = '../data/submission_GD.csv' # name towards GD output \n",
    "OUTPUT_PATH_RIDGE = '../data/submission_ridge.csv' # name towards ridge output \n",
    "OUTPUT_PATH_LOGISTIC = '../data/submission_logistic.csv' # name towards logistic output \n",
    "#create_csv_submission(ids_test, final_predictions_GD, OUTPUT_PATH_GD) # print csv file according to results\n",
    "# create_csv_submission(ids_test, final_predictions_ridge, OUTPUT_PATH_RIDGE) # print csv file according to results\n",
    "#create_csv_submission(ids_test, final_predictions_logistic, OUTPUT_PATH_LOGISTIC)\n",
    "create_csv_submission(ids_test, final_mixed_predictions, OUTPUT_PATH_LOGISTIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
