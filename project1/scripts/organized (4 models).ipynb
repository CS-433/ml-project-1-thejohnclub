{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our libraries\n",
    "from proj1_helpers import *\n",
    "from proj1_input_man import *\n",
    "from proj1_linear_model import *\n",
    "from proj1_ridge_regress import *\n",
    "from proj1_logistic import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv' # train data path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n"
     ]
    }
   ],
   "source": [
    "print(tX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the labels to {0,1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the array from -1 1 to 0 1\n",
    "# label simplification\n",
    "# y == 0 non detected Boson, y == 1 detected Boson\n",
    "y_ = np.array([0 if l == -1 else 1 for l in y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All of the following parts are the same for the test set, should be functions\n",
    "\n",
    "# less code = better code (in this case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the features by the number of jets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_0, tX_1, tX_2_3 = split_to_Jet_Num(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tX_0, tX_1, tX_2, tX_3 = alternative_split_to_Jet_Num(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288.1303027432567\n",
      "38.07417940661156\n",
      "48.78977837574543\n",
      "83.81996788992959\n",
      "1.4698926707450346\n",
      "317.2726868423743\n",
      "2.8672697710964288\n",
      "0.8272411967910458\n",
      "32.29848326337556\n",
      "150.13499581029197\n",
      "1.2471331404825152\n",
      "0.9433373230483033\n",
      "0.3876270487619129\n",
      "32.43010841851103\n",
      "1.2047095881208543\n",
      "1.8241134002031094\n",
      "32.96582686964663\n",
      "1.2176561938554824\n",
      "1.8180600465624461\n",
      "49.697288657467055\n",
      "1.8123415757149903\n",
      "158.8254499542401\n",
      "76.3120342898988\n",
      "1.5732280712204414\n",
      "1.8067332213365404\n",
      "40.12542350677477\n",
      "1.775313755049286\n",
      "1.8147406038684843\n",
      "128.45133643867\n"
     ]
    }
   ],
   "source": [
    "for i in range(tX_1.shape[1]):\n",
    "    print(np.std(tX_3[:,i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing also the output by the type of particle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_0,y_1,y_2_3 = split_labels_to_Jet_Num(y_,tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_0, y_1, y_2, y_3 = alternative_split_labels_to_Jet_Num(y_, tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a column of zeros and ones to detect whether the mass has been measured or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the indices where the mass is not calculated, add the column which has 0 in those indices\n",
    "# and 1 everywhere else for all matrices 0,1,2_3\n",
    "tX_0 = find_mass(tX_0)\n",
    "tX_1 = find_mass(tX_1)\n",
    "# tX_2_3 = find_mass(tX_2_3)\n",
    "tX_2 = find_mass(tX_2)\n",
    "tX_3 = find_mass(tX_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Throwing away the outliers from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 7, 13, 23, 24, 25, 26, 27, 28, 29]\n",
      "(99913, 19)\n",
      "[5, 6, 7, 13, 26, 27, 28]\n",
      "(77544, 23)\n",
      "[]\n",
      "[]\n",
      "[[ 1.00000e+00  1.38470e+02  5.16550e+01 ...  1.24000e+00 -2.47500e+00\n",
      "   1.13497e+02]\n",
      " [ 1.00000e+00  1.48754e+02  2.88620e+01 ...  1.31000e-01 -2.76700e+00\n",
      "   1.79877e+02]\n",
      " [ 1.00000e+00  1.41481e+02  7.36000e-01 ... -7.98000e-01 -2.78500e+00\n",
      "   2.78009e+02]\n",
      " ...\n",
      " [ 1.00000e+00  1.19934e+02  2.00780e+01 ... -1.72500e+00 -2.75600e+00\n",
      "   1.12938e+02]\n",
      " [ 1.00000e+00  1.26151e+02  2.90230e+01 ... -5.99000e-01 -2.52500e+00\n",
      "   1.93099e+02]\n",
      " [ 1.00000e+00  1.15254e+02  4.71560e+01 ... -5.80000e-02 -1.13700e+00\n",
      "   1.74176e+02]]\n",
      "[[ 1.00000e+00  8.97440e+01  1.35500e+01 ...  2.24000e-01  3.10600e+00\n",
      "   1.93660e+02]\n",
      " [ 1.00000e+00  1.14744e+02  1.02860e+01 ...  1.77300e+00 -2.07900e+00\n",
      "   1.65640e+02]\n",
      " [ 1.00000e+00  1.21681e+02  6.04100e+00 ... -1.25700e+00 -6.09000e-01\n",
      "   2.53461e+02]\n",
      " ...\n",
      " [ 1.00000e+00 -9.99000e+02  8.38710e+01 ...  3.07000e+00  1.61200e+00\n",
      "   2.71833e+02]\n",
      " [ 1.00000e+00 -9.99000e+02  3.80830e+01 ...  5.15000e-01  4.16000e-01\n",
      "   2.03569e+02]\n",
      " [ 1.00000e+00  1.30075e+02  3.91800e+00 ...  5.78000e-01 -2.21500e+00\n",
      "   5.46066e+02]]\n"
     ]
    }
   ],
   "source": [
    "tX_0, col_to_delete_0 = fix_array(tX_0, 0)\n",
    "print(tX_0.shape)\n",
    "tX_1, col_to_delete_1 = fix_array(tX_1, 1)\n",
    "print(tX_1.shape)\n",
    "tX_2, _ = fix_array(tX_2, 2)\n",
    "tX_3, _ = fix_array(tX_3, 3)\n",
    "print(tX_2)\n",
    "print(tX_3)\n",
    "# print(tX_2_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we substitute the -999 values with the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_0, column_median_0 = fix_median(tX_0)\n",
    "tX_1, column_median_1 = fix_median(tX_1)\n",
    "# tX_2_3 = fix_median(tX_2_3)\n",
    "tX_2, column_median_2 = fix_median(tX_2)\n",
    "tX_3, column_median_3 = fix_median(tX_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tX_2_3[:,1:], mean_2_3, std_2_3 = standardize(tX_2_3[:,1:])\n",
    "tX_3[:,1:], mean_3, std_3 = standardize(tX_3[:,1:])\n",
    "tX_2[:,1:], mean_2, std_2 = standardize(tX_2[:,1:]) #we standardize everything a part from the column added manually\n",
    "tX_0[:,1:], mean_0, std_0 = standardize(tX_0[:,1:])\n",
    "tX_1[:,1:], mean_1, std_1 = standardize(tX_1[:,1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We insert the column for the bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_tilda_0 = np.insert(tX_0, 0, np.ones(tX_0.shape[0]), axis=1)\n",
    "tX_tilda_1 = np.insert(tX_1, 0, np.ones(tX_1.shape[0]), axis=1)\n",
    "# tX_tilda_2_3 = np.insert(tX_2_3, 0, np.ones(tX_2_3.shape[0]), axis=1)\n",
    "tX_tilda_2 = np.insert(tX_2, 0, np.ones(tX_2.shape[0]), axis=1)\n",
    "tX_tilda_3 = np.insert(tX_3, 0, np.ones(tX_3.shape[0]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following cells of code aim at finetuning the hyperparameters of the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the optimal degree for gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/Documents/MachineLearning/Machine-Learning-P1/project1/scripts/proj1_linear_model.py:19: RuntimeWarning: invalid value encountered in matmul\n",
      "  gradient = -(1/N) * (tx.T) @ (e) # calculate the gradient\n",
      "/home/john/Documents/MachineLearning/Machine-Learning-P1/project1/scripts/proj1_linear_model.py:38: RuntimeWarning: invalid value encountered in subtract\n",
      "  w = w - gamma * gradient # conduct a step of gradient descent\n"
     ]
    }
   ],
   "source": [
    "degree_opt_0_GD = finetune_GD(tX_tilda_0, y_0)\n",
    "degree_opt_1_GD = finetune_GD(tX_tilda_1, y_1)\n",
    "degree_opt_2_3_GD = finetune_GD(tX_tilda_2_3, y_2_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_opt_1_GD = finetune_GD(tX_tilda_1, y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell takes a long time to execute, the hyperparameters found are:\n",
    "# degree =2 for tX_tilda_0 with an accuracy of 0.82694771 on the validation set\n",
    "# degree =3 for tX_tilda_1 with an accuracy of 0.7937687 on the validation set\n",
    "# degree =3 for tX_tilda_2_3 with an accuracy of 0.81856907 on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_GD_0 = optimal_weights_GD(tX_tilda_0,y_0,degree_opt_0_GD,lambda_opt_0_GD)\n",
    "w_GD_1 = optimal_weights_GD(tX_tilda_1,y_1,degree_opt_1_GD,lambda_opt_1_GD)\n",
    "w_GD_2_3 = optimal_weights_GD(tX_tilda_2_3,y_2_3,degree_opt_2_3_GD,lambda_opt_2_3_GD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the optimal degree for stochastic gradient descent for feature augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_opt_0_SGD = finetune_SGD(tX_tilda_0, y_0)\n",
    "degree_opt_1_SGD = finetune_SGD(tX_tilda_1, y_1)\n",
    "degree_opt_2_SGD = finetune_SGD(tX_tilda_2, y_2)\n",
    "degree_opt_3_SGD = finetune_SGD(tX_tilda_3, y_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the optimal lambda for ridge regression and the optimal degree for feature augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_opt_0_ridge, degree_opt_0_ridge = finetune_ridge(tX_tilda_0, y_0, degrees=np.array([2]), lambdas = np.logspace(-5, -1, 10), crossing=True)\n",
    "lambda_opt_1_ridge, degree_opt_1_ridge = finetune_ridge(tX_tilda_1, y_1, degrees= np.array([2]), lambdas = np.logspace(-5, -1, 10), crossing=True)\n",
    "lambda_opt_2_ridge, degree_opt_2_ridge = finetune_ridge(tX_tilda_2, y_2, degrees=np.array([2]), lambdas = np.logspace(-5, -1, 10), crossing=True)\n",
    "lambda_opt_3_ridge, degree_opt_3_ridge = finetune_ridge(tX_tilda_3, y_3, degrees=np.array([2]), lambdas = np.logspace(-5, -1, 10), crossing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check\n",
    "print(lambda_opt_0_ridge, degree_opt_0_ridge)\n",
    "print(lambda_opt_1_ridge, degree_opt_1_ridge)\n",
    "# print(lambda_opt_2_3_ridge, degree_opt_2_3_ridge)\n",
    "print(lambda_opt_2_ridge, degree_opt_2_ridge)\n",
    "print(lambda_opt_3_ridge, degree_opt_3_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarizing the best parameters found are\n",
    "# lambda = 0.00316228 and degree=6 for tX_tilda_0 with a validation accuracy of 0.83847463\n",
    "# [0.00558459] [6] gives accuracy = 0.83814433 for tX_tilda_0\n",
    "# lambda = 0.00316228 and degree= 6 for tX_tilda_1 with a validation accuracy of 0.80354656\n",
    "# lambda = 0.0047263 and degree = 6 for tX_tilda_3 gives 0.83759025 approximately\n",
    "# lambda = 0.00546996 and degree = 5 for tX_tilda_2 gives 0.82747395\n",
    "# lambda=1e-5 and degree=6 for tX_tilda_2_3 with a validation accuracy of 0.83021781"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the optimal weights with the calculated hyper parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_ridge_0 = optimal_weights_ridge(tX_tilda_0, y_0, degree_opt_0_ridge, lambda_opt_0_ridge)\n",
    "# w_ridge_1 = optimal_weights_ridge(tX_tilda_1, y_1, degree_opt_1_ridge, lambda_opt_1_ridge)\n",
    "w_ridge_1 = optimal_weights_ridge(tX_tilda_1, y_1, 6, 0.00774608)\n",
    "# w_ridge_2_3 = optimal_weights_ridge(tX_tilda_2_3, y_2_3, degree_opt_2_3_ridge, lambda_opt_2_3_ridge)\n",
    "# w_ridge_2 = optimal_weights_ridge(tX_tilda_2, y_2, degree_opt_2_ridge, lambda_opt_2_ridge)\n",
    "# w_ridge_2_3 = optimal_weights_ridge(tX_tilda_3, y_3, degree_opt_3_ridge, lambda_opt_3_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the optimal lambda for logistic regression and the optimal degree for feature augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_opt_0_logistic, degree_opt_0_logistic = finetune_logistic(tX_tilda_0, y_0, 1.7783e-04, np.array([2]) , np.logspace(-2, 1, 7), 4, True)\n",
    "lambda_opt_1_logistic, degree_opt_1_logistic = finetune_logistic(tX_tilda_1, y_1, 3.16228e-03, np.array([2]), np.logspace(-2, 1, 7), 4, True)\n",
    "lambda_opt_2_logistic, degree_opt_2_logistic = finetune_logistic(tX_tilda_2, y_2, 1.7783e-04, np.array([2]), np.logspace(-3, 1, 7), 4, True)\n",
    "lambda_opt_3_logistic, degree_opt_3_logistic = finetune_logistic(tX_tilda_3, y_3, 1.7783e-04, np.array([2]), np.logspace(-3, 1, 7), 4, True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Optimal values for logistic regression with feature crossing expansion of order 2 expanding also the bias term and the mass \n",
    "### for tX_tilda_0 lambda = 0.03162278 and degree = 2 with accuracy 0.8195411962527024\n",
    "### for tX_tilda_1 lambda = 2.11944285 and degree = 2 with accuracy 0.7975343031053337\n",
    "### for tX_tilda_2 lambda = 0.02154435 and degree = 2 with accuracy 0.8323606479275846\n",
    "### for tX_tilda_3 lambda = 0.46415888 and degree = 2 with accuracy 0.8326565601876917\n",
    "\n",
    "### Optimal values for logistic regression with feature crossing expansion of order 2 not expanding bias and mass\n",
    "### for tX_tilda_0 lambda = 3.16227766 and degree = 2 with accuracy 0.824855873163882\n",
    "### for tX_tilda_1 lambda = 1 and degree = 2 with accuracy 0.797585886722377\n",
    "### for tX_tilda_2 lambda = 10 and degree = 2 with accuracy 0.830757393997142\n",
    "### for tX_tilda_3 lambda = 10 and degree = 2 with accuracy 0.831573723154665\n",
    "\n",
    "### Optimal values for logistic regression with feature crossing expansion of order 2 not expanding only the bias\n",
    "### for tX_tilda_0 lambda = 0.1 and degree = 2 with accuracy 0.8224937945391945\n",
    "### for tX_tilda_1 lambda = 1 and degree = 2 with accuracy 0.7975600949138554\n",
    "### for tX_tilda_2 lambda = 10 and degree = 2 with accuracy 0.8321025885342226\n",
    "### for tX_tilda_3 lambda = 10 and degree = 2 with accuracy 0.8334686879624617\n",
    "\n",
    "\n",
    "### Best logistic regression models for each one of the four matrices\n",
    "### for tX_tilda_0 without crossing feature expansion, normal expansion of degree = 2 and lambda = 1.95096 accuracy = 0.827668\n",
    "### for tX_tilda_1 with crossing feature expansion expanding the mass, degree = 2 and lambda = 1 accuracy = 0.797585886722377\n",
    "### for tX_tilda_2 with crossing feature expansion expanding the mass, degree = 2 and lambda = 10 accuracy = 0.832102588\n",
    "### for tX_tilda_3 with crossing feature expansion expanding the mass, degree = 2 and lambda = 10 accuracy = 0.83346868\n",
    "\n",
    "### best results on the best run on Aicrowd\n",
    "### lambda = 1 and degree = 2 for tX_tilda_0\n",
    "### lambda = 1.5 and degree = 3 for tX_tilda_1\n",
    "### lambda = 0 and degree = 2 for tX_tilda_2_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_opt_1_logistic,degree_opt_1_logistic = finetune_logistic(tX_tilda_1, y_1, gamma = 1.7783e-04, degrees = np.arange(1, 4), lambdas = random_interval(1.0e-07, 1.0e-02, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarizing the best parameters found are\n",
    "# lambda=1.95096 and degree=2 for tX_tilda_0 with a validation accuracy of 0.827668 #lambda =9.64683289, acc=0.82664\n",
    "# lambda=1.6672 and degree=2 for tX_tilda_1 with a validation accuracy of 0.79229 #lamda = 13.00571 acc = 0.792466 gamma = 0.0004\n",
    "# lambda=0.6835 and degree=2 for tX_tilda_2_3 with a validation accuracy of 0.8151\n",
    "# lambda = 10 and degree = 2 for tX_tilda_2 gives an accuracy 0.8153088 \n",
    "# lambda = 0.001 and degree = 2 for tX_tilda_3 gives an accuracy 0.8231366 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will store in a dictionary for each predictor the parameters of the predictor, the first element of the list will be the lambda and the second\n",
    "# will be the degree.\n",
    "\n",
    "params_0 = {'model_1':[2,9.58856], 'model_2':[2, 7.04376], 'model_3': [2, 3.76039]}\n",
    "params_1 = {'model_1':[2, 2.2889], 'model_2':[2, 13.89056], 'model_3': [2, 3.54514]}\n",
    "params_2 = {'model_1':[2, 7.33430], 'model_2':[2, 12.49542], 'model_3': [2, 5.17695]}\n",
    "params_3 = {'model_1':[2, 3.91067], 'model_2':[2, 13.78283], 'model_3': [2, 18.61038]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the optimal weights with the calculated hyper parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we now compute the weights for each model and we store in a dictionary\n",
    "gamma = 0.00017783\n",
    "print(params_0['model_1'][0],params_0['model_1'][1])\n",
    "w_logistic_0_model_1 = optimal_weights_logistic(tX_tilda_0, y_0, gamma, params_0['model_1'][0], params_0['model_1'][1])\n",
    "w_logistic_1_model_1 = optimal_weights_logistic(tX_tilda_1, y_1, gamma, params_1['model_1'][0], params_1['model_1'][1])\n",
    "w_logistic_2_model_1 = optimal_weights_logistic(tX_tilda_2, y_2, gamma, params_2['model_1'][0], params_2['model_1'][1])\n",
    "w_logistic_3_model_1 = optimal_weights_logistic(tX_tilda_3, y_3, gamma, params_3['model_1'][0], params_3['model_1'][1])\n",
    "\n",
    "w_logistic_0_model_2 = optimal_weights_logistic(tX_tilda_0, y_0, gamma, params_0['model_2'][0], params_0['model_2'][1])\n",
    "w_logistic_1_model_2 = optimal_weights_logistic(tX_tilda_1, y_1, gamma, params_1['model_2'][0], params_1['model_2'][1])\n",
    "w_logistic_2_model_2 = optimal_weights_logistic(tX_tilda_2, y_2, gamma, params_2['model_2'][0], params_2['model_2'][1])\n",
    "w_logistic_3_model_2 = optimal_weights_logistic(tX_tilda_3, y_3, gamma, params_3['model_2'][0], params_3['model_2'][1])\n",
    "\n",
    "w_logistic_0_model_3 = optimal_weights_logistic(tX_tilda_0, y_0, gamma, params_0['model_3'][0], params_0['model_3'][1])\n",
    "w_logistic_1_model_3 = optimal_weights_logistic(tX_tilda_1, y_1, gamma, params_1['model_3'][0], params_1['model_3'][1])\n",
    "w_logistic_2_model_3 = optimal_weights_logistic(tX_tilda_2, y_2, gamma, params_2['model_3'][0], params_2['model_3'][1])\n",
    "w_logistic_3_model_3 = optimal_weights_logistic(tX_tilda_3, y_3, gamma, params_3['model_3'][0], params_3['model_3'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate the optimal lambda for logistic regression with batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_opt_0_logistic_batch,degree_opt_0_logistic_batch = finetune_batch_logistic(tX_tilda_0, y_0, gamma = 1.7783e-04, degrees =np.arange(1, 4) , lambdas=random_interval(0.8, 1.5, 5) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# open the test file\n",
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tX_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now format the tX_test as we did for tX_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we split the test into the three subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test_0, tX_test_1, tX_test_2, tX_test_3 = alternative_split_to_Jet_Num(tX_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a column of zeros and ones to detect whether the mass has been measured or not\n",
    "This should be done prior to splitting it is the same procedure and just wastes space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the indices where the mass is not calculated, add the column which has 0 in those indices\n",
    "# and 1 everywhere else for all matrices 0,1,2_3\n",
    "tX_test_0 = find_mass(tX_test_0)\n",
    "tX_test_1 = find_mass(tX_test_1)\n",
    "tX_test_2 = find_mass(tX_test_2)\n",
    "tX_test_3 = find_mass(tX_test_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We drop the same columns we have dropped for the X training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tX_test_0 = fix_array(tX_test_0, 0)\n",
    "# print(tX_test_0.shape)\n",
    "# tX_test_1 = fix_array(tX_test_1, 1)\n",
    "# print(tX_test_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test_0 = np.delete(tX_test_0, col_to_delete_0, axis=1)\n",
    "tX_test_1 = np.delete(tX_test_1, col_to_delete_1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we substitute the -999 values with the median\n",
    "This should also be done with a function it is the same thing repeated thrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test_0 = fix_median_test(tX_test_0, column_median_0)\n",
    "tX_test_1 = fix_median_test(tX_test_1, column_median_1)\n",
    "tX_test_2 = fix_median_test(tX_test_2, column_median_2)\n",
    "tX_test_3 = fix_median_test(tX_test_3, column_median_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tX_test_0.shape)\n",
    "print(w_logistic_0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We standardize the test set using the mean and the standard deviation of the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tX_test_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tX_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the data in the test set\n",
    "# should have used the same function both here and on the training part same process this is reduntant\n",
    "def standardize_test(x, mean, std):\n",
    "    \"\"\"Standardize the test set.\"\"\"\n",
    "    x = x - mean \n",
    "    x = x / std\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test_0[:,1:] = standardize_test(tX_test_0[:,1:], mean_0, std_0)  #we standardize everything a part from the column added manually\n",
    "tX_test_1[:,1:] = standardize_test(tX_test_1[:,1:], mean_1, std_1)  #we standardize everything a part from the column added manually\n",
    "tX_test_2[:,1:] = standardize_test(tX_test_2[:,1:], mean_2, std_2) #we standardize everything a part from the column added manually\n",
    "tX_test_3[:,1:] = standardize_test(tX_test_3[:,1:], mean_3, std_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We insert the column for the bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_tilda_test_0 = np.insert(tX_test_0, 0, np.ones(tX_test_0.shape[0]), axis=1) #the first column now is all ones and is used for bias\n",
    "tX_tilda_test_1 = np.insert(tX_test_1, 0, np.ones(tX_test_1.shape[0]), axis=1) #the first column now is all ones and is used for bias\n",
    "tX_tilda_test_2 = np.insert(tX_test_2, 0, np.ones(tX_test_2.shape[0]), axis=1) #the first column now is all ones and is used for bias\n",
    "tX_tilda_test_3 = np.insert(tX_test_3, 0, np.ones(tX_test_3.shape[0]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We make the predictions with GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_GD_0 = predict_GD(tX_tilda_test_0,w_GD_0,degree_opt_0_GD)\n",
    "predictions_GD_1 = predict_GD(tX_tilda_test_1,w_GD_1,degree_opt_1_GD)\n",
    "predictions_GD_2_3 = predict_GD(tX_tilda_test_2_3,w_GD_2_3,degree_opt_2_3_GD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We make the predictions with ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ridge_0 = predict_ridge(tX_tilda_test_0, w_ridge_0,degree_opt_0_ridge)\n",
    "predictions_ridge_1 = predict_ridge(tX_tilda_test_1, w_ridge_1,degree_opt_1_ridge)\n",
    "predictions_ridge_2 = predict_ridge(tX_tilda_test_2, w_ridge_2, degree_opt_2_ridge)\n",
    "predictions_ridge_3 = predict_ridge(tX_tilda_test_3, w_ridge_3, degree_opt_3_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_logistic_0 = predict_logistic(tX_tilda_test_0, w_logistic_0, degree_opt_0_logistic, False)\n",
    "predictions_logistic_1 = predict_logistic(tX_tilda_test_1, w_logistic_1, degree_opt_1_logistic, True)\n",
    "predictions_logistic_2 = predict_logistic(tX_tilda_test_2, w_logistic_2, degree_opt_2_logistic, True)\n",
    "predictions_logistic_3 = predict_logistic(tX_tilda_test_3, w_logistic_3, degree_opt_3_logistic, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(params_0['model_1'][1])\n",
    "print(w_logistic_0_model_1.shape)\n",
    "print(tX_tilda_test_0.shape)\n",
    "predictions_logistic_0_model_1 = predict_logistic(tX_tilda_test_0, w_logistic_0_model_1, params_0['model_1'][1])\n",
    "predictions_logistic_1_model_1 = predict_logistic(tX_tilda_test_1, w_logistic_1_model_1, params_1['model_1'][1])\n",
    "predictions_logistic_2_model_1 = predict_logistic(tX_tilda_test_2, w_logistic_2_model_1, params_2['model_1'][1])\n",
    "predictions_logistic_3_model_1 = predict_logistic(tX_tilda_test_3, w_logistic_3_model_1, params_3['model_1'][1])\n",
    "\n",
    "predictions_logistic_0_model_2 = predict_logistic(tX_tilda_test_0, w_logistic_0_model_2, params_0['model_2'][1])\n",
    "predictions_logistic_1_model_2 = predict_logistic(tX_tilda_test_1, w_logistic_1_model_2, params_1['model_2'][1])\n",
    "predictions_logistic_2_model_2 = predict_logistic(tX_tilda_test_2, w_logistic_2_model_2, params_2['model_2'][1])\n",
    "predictions_logistic_3_model_2 = predict_logistic(tX_tilda_test_3, w_logistic_3_model_2, params_3['model_2'][1])\n",
    "\n",
    "predictions_logistic_0_model_3 = predict_logistic(tX_tilda_test_0, w_logistic_0_model_3, params_0['model_3'][1])\n",
    "predictions_logistic_1_model_3 = predict_logistic(tX_tilda_test_1, w_logistic_1_model_3, params_1['model_3'][1])\n",
    "predictions_logistic_2_model_3 = predict_logistic(tX_tilda_test_2, w_logistic_2_model_3, params_2['model_3'][1])\n",
    "predictions_logistic_3_model_3 = predict_logistic(tX_tilda_test_3, w_logistic_3_model_3, params_3['model_3'][1])\n",
    "\n",
    "average_predictions_0 = ensemble_logistic(predictions_logistic_0_model_1, predictions_logistic_0_model_2, predictions_logistic_0_model_3)\n",
    "average_predictions_1 = ensemble_logistic(predictions_logistic_0_model_1, predictions_logistic_0_model_2, predictions_logistic_0_model_3)\n",
    "average_predictions_2 = ensemble_logistic(predictions_logistic_0_model_1, predictions_logistic_0_model_2, predictions_logistic_0_model_3)\n",
    "average_predictions_3 = ensemble_logistic(predictions_logistic_0_model_1, predictions_logistic_0_model_2, predictions_logistic_0_model_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to reconstruct a single vector of predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate final prediction list and print it in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_predictions_GD = create_output(tX_test,predictions_GD_0,predictions_GD_1,predictions_GD_2_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_predictions_ridge = create_output(tX_test,predictions_ridge_0,predictions_ridge_1,predictions_ridge_2_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_predictions_logistic = create_output(tX_test,predictions_logistic_0,predictions_logistic_1,predictions_logistic_2_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mixed_predictions = create_output(tX_test, predictions_logistic_0, predictions_logistic_1, predictions_logistic_2, predictions_logistic_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_mixed_predictions)\n",
    "print(1 / len(final_mixed_predictions) * np.count_nonzero(final_mixed_predictions == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH_GD = '../data/submission_GD.csv' # name towards GD output \n",
    "OUTPUT_PATH_RIDGE = '../data/submission_ridge.csv' # name towards ridge output \n",
    "OUTPUT_PATH_LOGISTIC = '../data/submission_logistic.csv' # name towards logistic output \n",
    "#create_csv_submission(ids_test, final_predictions_GD, OUTPUT_PATH_GD) # print csv file according to results\n",
    "# create_csv_submission(ids_test, final_predictions_ridge, OUTPUT_PATH_RIDGE) # print csv file according to results\n",
    "#create_csv_submission(ids_test, final_predictions_logistic, OUTPUT_PATH_LOGISTIC)\n",
    "create_csv_submission(ids_test, final_mixed_predictions, OUTPUT_PATH_LOGISTIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
