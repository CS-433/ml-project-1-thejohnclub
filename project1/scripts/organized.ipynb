{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our libraries\n",
    "from proj1_helpers import *\n",
    "from proj1_input_man import *\n",
    "from proj1_linear_model import *\n",
    "from proj1_ridge_regress import *\n",
    "from proj1_logistic import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv' # train data path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n"
     ]
    }
   ],
   "source": [
    "print(tX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the labels to {0,1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the array from -1 1 to 0 1\n",
    "# label simplification\n",
    "# y == 0 non detected Boson, y == 1 detected Boson\n",
    "y_ = np.array([0 if l == -1 else 1 for l in y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All of the following parts are the same for the test set, should be functions\n",
    "\n",
    "# less code = better code (in this case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the features by the number of jets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_0,tX_1,tX_2_3 = split_to_Jet_Num(tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing also the output by the type of particle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_0,y_1,y_2_3 = split_labels_to_Jet_Num(y_,tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a column of zeros and ones to detect whether the mass has been measured or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the indices where the mass is not calculated, add the column which has 0 in those indices\n",
    "# and 1 everywhere else for all matrices 0,1,2_3\n",
    "tX_0 = find_mass(tX_0)\n",
    "tX_1 = find_mass(tX_1)\n",
    "tX_2_3 = find_mass(tX_2_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Throwing away the outliers from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 7, 13, 23, 24, 25, 26, 27, 28, 29]\n",
      "(99913, 19)\n",
      "(99913, 19)\n",
      "[5, 6, 7, 13, 26, 27, 28]\n",
      "(77544, 23)\n",
      "(77544, 23)\n",
      "(72543, 31)\n",
      "(72543, 31)\n"
     ]
    }
   ],
   "source": [
    "tX_0 = fix_array(tX_0,0)\n",
    "print(tX_0.shape)\n",
    "tX_1 = fix_array(tX_1,1)\n",
    "print(tX_1.shape)\n",
    "tX_2_3 = fix_array(tX_2_3)\n",
    "print(tX_2_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we substitute the -999 values with the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_0 = fix_mean(tX_0)\n",
    "tX_1 = fix_mean(tX_1)\n",
    "tX_2_3 = fix_mean(tX_2_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tX_2_3[:,1:], mean_2_3,std_2_3 = standardize(tX_2_3[:,1:]) #we standardize everything a part from the column added manually\n",
    "tX_0[:,1:],mean_0,std_0 = standardize(tX_0[:,1:])\n",
    "tX_1[:,1:],mean_1,std_1 = standardize(tX_1[:,1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We insert the column for the bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_tilda_0 = np.insert(tX_0, 0, np.ones(tX_0.shape[0]), axis=1)\n",
    "tX_tilda_1 = np.insert(tX_1, 0, np.ones(tX_1.shape[0]), axis=1)\n",
    "tX_tilda_2_3 = np.insert(tX_2_3, 0, np.ones(tX_2_3.shape[0]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following cells of code aim at finetuning the hyperparameters of the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the optimal degree for gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.61847606  0.06020022  0.006644   ...  0.5805158   0.28062325\n",
      " -0.01899064]\n",
      "[1 0 0 ... 1 0 0]\n",
      "[0 0 0 ... 1 0 0]\n",
      "[ 0.15864768 -0.03560335  0.20053426 ...  0.608775   -0.01836511\n",
      "  0.03172052]\n",
      "[0 0 0 ... 1 0 0]\n",
      "[1 0 0 ... 1 0 0]\n",
      "[ 0.26937014 -0.00129739  0.03183365 ...  0.67090778  0.36748447\n",
      "  0.81767221]\n",
      "[0 0 0 ... 1 0 1]\n",
      "[0 0 0 ... 1 0 1]\n",
      "[0.11828094 0.27527221 0.22687442 ... 0.43692667 0.2788262  0.20996465]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[ 0.82239773 -0.01319973  0.0410209  ...  0.55932944  0.23118963\n",
      " -0.00646542]\n",
      "[1 0 0 ... 1 0 0]\n",
      "[0 0 0 ... 1 0 0]\n",
      "[ 0.47410793 -0.00442422  0.19485567 ...  0.63209243  0.13035331\n",
      "  0.03630025]\n",
      "[0 0 0 ... 1 0 0]\n",
      "[1 0 0 ... 1 0 0]\n",
      "[-0.16364379  0.00518823  0.01600962 ...  0.83116894  0.39815189\n",
      "  0.8920483 ]\n",
      "[0 0 0 ... 1 0 1]\n",
      "[0 0 0 ... 1 0 1]\n",
      "[0.05848122 0.17219089 0.09329759 ... 0.54380012 0.54811636 0.08759498]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[0 0 0 ... 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luca\\Documents\\Luca\\Machine Learning\\Machine-Learning-P1\\project1\\scripts\\proj1_linear_model.py:19: RuntimeWarning: invalid value encountered in matmul\n",
      "  gradient = -(1/N) * (tx.T) @ (e) # calculate the gradient\n",
      "C:\\Users\\Luca\\Documents\\Luca\\Machine Learning\\Machine-Learning-P1\\project1\\scripts\\proj1_linear_model.py:37: RuntimeWarning: invalid value encountered in subtract\n",
      "  w = w - gamma * gradient # conduct a step of gradient descent\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan ... nan nan nan]\n",
      "[1 1 1 ... 1 1 1]\n",
      "[0 0 0 ... 1 0 0]\n",
      "[nan nan nan ... nan nan nan]\n",
      "[1 1 1 ... 1 1 1]\n",
      "[1 0 0 ... 1 0 0]\n",
      "[nan nan nan ... nan nan nan]\n",
      "[1 1 1 ... 1 1 1]\n",
      "[0 0 0 ... 1 0 1]\n",
      "[nan nan nan ... nan nan nan]\n",
      "[1 1 1 ... 1 1 1]\n",
      "[0 0 0 ... 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luca\\Documents\\Luca\\Machine Learning\\Machine-Learning-P1\\project1\\scripts\\proj1_linear_model.py:11: RuntimeWarning: overflow encountered in matmul\n",
      "  e = y - tx @ w # e = error vector (truth - prediction)\n",
      "C:\\Users\\Luca\\Documents\\Luca\\Machine Learning\\Machine-Learning-P1\\project1\\scripts\\proj1_linear_model.py:18: RuntimeWarning: overflow encountered in matmul\n",
      "  e = y - tx @ w # e = error vector (truth - prediction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan ... nan nan nan]\n",
      "[1 1 1 ... 1 1 1]\n",
      "[0 0 0 ... 1 0 0]\n",
      "[nan nan nan ... nan nan nan]\n",
      "[1 1 1 ... 1 1 1]\n",
      "[1 0 0 ... 1 0 0]\n",
      "[nan nan nan ... nan nan nan]\n",
      "[1 1 1 ... 1 1 1]\n",
      "[0 0 0 ... 1 0 1]\n",
      "[nan nan nan ... nan nan nan]\n",
      "[1 1 1 ... 1 1 1]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[0.8183001  0.82694771 0.25513452 0.25513452]\n",
      "[2]\n",
      "[0.51504206 0.09484613 0.11386846 ... 0.59970959 0.27147103 0.33439649]\n",
      "[1 0 0 ... 1 0 0]\n",
      "[1 0 0 ... 1 0 0]\n",
      "[0.33210021 0.12997659 0.2331945  ... 0.09187338 0.13889746 0.55722934]\n",
      "[0 0 0 ... 0 0 1]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[ 0.26950688  0.50120399  0.70961342 ...  0.48160547  0.48359656\n",
      " -0.00478934]\n",
      "[0 1 1 ... 0 0 0]\n",
      "[0 0 1 ... 0 0 0]\n",
      "[0.07835575 0.02938548 0.47226914 ... 0.33415758 0.25938869 0.416166  ]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[0.56803713 0.18758206 0.00183958 ... 0.61996469 0.14517563 0.25332729]\n",
      "[1 0 0 ... 1 0 0]\n",
      "[1 0 0 ... 1 0 0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-91f3dc9f6ecc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdegree_opt_0_GD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinetune_GD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX_tilda_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdegree_opt_1_GD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinetune_GD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX_tilda_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdegree_opt_2_3_GD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinetune_GD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX_tilda_2_3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_2_3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Luca\\Machine Learning\\Machine-Learning-P1\\project1\\scripts\\proj1_linear_model.py\u001b[0m in \u001b[0;36mfinetune_GD\u001b[1;34m(tX, y, k_fold, degrees)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdegrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mcurrent_sum_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m             \u001b[0mcurrent_test_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation_GD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegrees\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m10e-4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[0mcurrent_sum_test\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcurrent_test_acc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Luca\\Machine Learning\\Machine-Learning-P1\\project1\\scripts\\proj1_linear_model.py\u001b[0m in \u001b[0;36mcross_validation_GD\u001b[1;34m(y, x, k_indices, k, degree, gamma)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[0my_testing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0mx_training_augmented\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_training\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m     \u001b[0mx_testing_augmented\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_testing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mws\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mleast_squares_GD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_training\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_training_augmented\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_training_augmented\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mw_opt_training\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mws\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Luca\\Machine Learning\\Machine-Learning-P1\\project1\\scripts\\proj1_linear_model.py\u001b[0m in \u001b[0;36mleast_squares_GD\u001b[1;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitial_w\u001b[0m \u001b[1;31m# Initialization of the weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# calculate the MSE loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# calculate the gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgradient\u001b[0m \u001b[1;31m# conduct a step of gradient descent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Luca\\Machine Learning\\Machine-Learning-P1\\project1\\scripts\\proj1_linear_model.py\u001b[0m in \u001b[0;36mcompute_gradient\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcompute_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# N = Number of samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtx\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mw\u001b[0m \u001b[1;31m# e = error vector (truth - prediction)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m@\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# calculate the gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "degree_opt_0_GD = finetune_GD(tX_tilda_0, y_0)\n",
    "degree_opt_1_GD = finetune_GD(tX_tilda_1, y_1)\n",
    "degree_opt_2_3_GD = finetune_GD(tX_tilda_2_3, y_2_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell takes a long time to execute, the hyperparameters found are:\n",
    "# degree =2 for tX_tilda_0 with an accuracy of 0.82694771 on the validation set\n",
    "# degree =3 for tX_tilda_1 with an accuracy of 0.7937687 on the validation set\n",
    "# degree =3 for tX_tilda_2_3 with an accuracy of 0.81856907 on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_GD_0 = optimal_weights_GD(tX_tilda_0,y_0,degree_opt_0_GD,lambda_opt_0_GD)\n",
    "w_GD_1 = optimal_weights_GD(tX_tilda_1,y_1,degree_opt_1_GD,lambda_opt_1_GD)\n",
    "w_GD_2_3 = optimal_weights_GD(tX_tilda_2_3,y_2_3,degree_opt_2_3_GD,lambda_opt_2_3_GD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the optimal lambda for ridge regression and the optimal degree for feature augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.82664398 0.83305975 0.83441097 0.83549194 0.83553198]\n",
      " [0.82517266 0.83081774 0.83079772 0.83164848 0.83303974]\n",
      " [0.82615354 0.83251927 0.83331999 0.83471124 0.8348914 ]\n",
      " [0.82482234 0.83007707 0.83023721 0.83111801 0.83251927]\n",
      " [0.82470223 0.83001702 0.83015714 0.83093784 0.83242919]\n",
      " [0.82599339 0.83203883 0.83267941 0.83404064 0.83441097]\n",
      " [0.82437193 0.82918627 0.82941647 0.83048744 0.83175858]\n",
      " [0.8237814  0.82832549 0.82836553 0.82937644 0.83083775]\n",
      " [0.82563307 0.83134821 0.8315584  0.83263938 0.83366029]\n",
      " [0.82414173 0.82891602 0.82916625 0.83013712 0.83149835]]\n",
      "[0.02464968] [6]\n",
      "[[0.76926747 0.78622646 0.78794171 0.7898633  0.79083054]\n",
      " [0.77648955 0.79468661 0.79745938 0.79971628 0.80032241]\n",
      " [0.77695383 0.79518958 0.79802682 0.80030952 0.80060614]\n",
      " [0.76648182 0.78403405 0.78503998 0.78678102 0.78828991]\n",
      " [0.76632706 0.78407274 0.78492391 0.78672943 0.78823833]\n",
      " [0.7740521  0.79275213 0.79527986 0.797382   0.79789786]\n",
      " [0.7778179  0.79740779 0.79969048 0.80255352 0.80313387]\n",
      " [0.77673459 0.79485427 0.79794945 0.79996131 0.80042559]\n",
      " [0.76724271 0.78451122 0.78573639 0.78752902 0.78876709]\n",
      " [0.77018313 0.78687129 0.78875419 0.79152695 0.79219758]]\n",
      "[0.00774608] [6]\n",
      "[[0.80977392 0.82325613 0.82550317 0.82932175 0.82961125]\n",
      " [0.80952578 0.82241522 0.82504825 0.82888062 0.82890819]\n",
      " [0.80976013 0.82307692 0.82524125 0.82890819 0.82926661]\n",
      " [0.80952578 0.82227736 0.82499311 0.82863248 0.82901847]\n",
      " [0.80949821 0.82227736 0.82506203 0.82866005 0.8289909 ]\n",
      " [0.80964985 0.82285636 0.82526882 0.8287979  0.82922526]\n",
      " [0.80918114 0.82201544 0.82502068 0.82860491 0.82849462]\n",
      " [0.80889165 0.8217673  0.82486904 0.8281362  0.82860491]\n",
      " [0.80959471 0.82253929 0.82519989 0.82883926 0.82905983]\n",
      " [0.80913978 0.8219603  0.82500689 0.82853598 0.82849462]]\n",
      "[0.00239647] [6]\n"
     ]
    }
   ],
   "source": [
    "lambda_opt_0_ridge, degree_opt_0_ridge = finetune_ridge(tX_tilda_0,y_0, lambdas = random_interval(0.001, 0.1, 10))\n",
    "lambda_opt_1_ridge, degree_opt_1_ridge = finetune_ridge(tX_tilda_1,y_1, lambdas = random_interval(0.001, 0.1, 10))\n",
    "lambda_opt_2_3_ridge, degree_opt_2_3_ridge = finetune_ridge(tX_tilda_2_3,y_2_3, lambdas = random_interval(10e-6, 10e-3, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024649684045609337 6\n",
      "0.007746077914420765 6\n",
      "0.0023964681173296695 6\n"
     ]
    }
   ],
   "source": [
    "# to check\n",
    "print(lambda_opt_0_ridge, degree_opt_0_ridge)\n",
    "print(lambda_opt_1_ridge, degree_opt_1_ridge)\n",
    "print(lambda_opt_2_3_ridge, degree_opt_2_3_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarizing the best parameters found are\n",
    "# lambda=0.00316228 and degree=6 for tX_tilda_0 with a validation accuracy of 0.83847463\n",
    "# lambda=0.00316228 and degree=6 for tX_tilda_1 with a validation accuracy of 0.80354656\n",
    "# lambda=1e-5 and degree=6 for tX_tilda_2_3 with a validation accuracy of 0.83021781"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the optimal weights with the calculated hyper parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ridge_0 = optimal_weights_ridge(tX_tilda_0, y_0, degree_opt_0_ridge, lambda_opt_0_ridge)\n",
    "w_ridge_1 = optimal_weights_ridge(tX_tilda_1, y_1, degree_opt_1_ridge, lambda_opt_1_ridge)\n",
    "w_ridge_2_3 = optimal_weights_ridge(tX_tilda_2_3, y_2_3, degree_opt_2_3_ridge, lambda_opt_2_3_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the optimal lambda for logistic regression and the optimal degree for feature augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.] [2]\n",
      "0.8266374409480343\n",
      "[1.] [2]\n",
      "0.79204064789023\n",
      "[10.] [2]\n",
      "0.814543700027571\n"
     ]
    }
   ],
   "source": [
    "lambda_opt_0_logistic,degree_opt_0_logistic = finetune_logistic(tX_tilda_0, y_0, gamma = 1.7783e-04, degrees =np.arange(1, 3) , lambdas=np.logspace(-2, 1, 7) )\n",
    "lambda_opt_1_logistic,degree_opt_1_logistic = finetune_logistic(tX_tilda_1, y_1, gamma = 3.16228e-03, degrees = np.arange(1, 3), lambdas = np.logspace(-2, 1, 7) )\n",
    "lambda_opt_2_3_logistic,degree_opt_2_3_logistic = finetune_logistic(tX_tilda_2_3, y_2_3, gamma =0.00017783, degrees = np.arange(1, 3), lambdas = np.logspace(-2, 1, 7) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.05714106] [2]\n",
      "0.7924662127308367\n"
     ]
    }
   ],
   "source": [
    "lambda_opt_1_logistic,degree_opt_1_logistic = finetune_logistic(tX_tilda_1, y_1, gamma = 0.0004, degrees = np.arange(1, 3), lambdas = random_interval(1,20,7) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarizing the best parameters found are\n",
    "# lambda=1.95096 and degree=2 for tX_tilda_0 with a validation accuracy of 0.827668 #lambda =9.64683289, acc=0.82664\n",
    "# lambda=1.6672 and degree=2 for tX_tilda_1 with a validation accuracy of 0.79229 #lamda = 13.00571 acc = 0.792466 gamma = 0.0004\n",
    "# lambda=0.6835 and degree=2 for tX_tilda_2_3 with a validation accuracy of 0.8151"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the optimal weights with the calculated hyper parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_logistic_0 = optimal_weights_logistic(tX_tilda_0,y_0, gamma=0.00017783, degree_opt_0,lambda_opt_0)\n",
    "w_logistic_1 = optimal_weights_logistic(tX_tilda_1,y_1, gamma =0.00316228 , degree_opt_1,lambda_opt_1)\n",
    "w_logistic_2_3 = optimal_weights_logistic(tX_tilda_2_3,y_2_3, gamma =0.00017783 ,degree_opt_2_3,lambda_opt_2_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate the optimal lambda for logistic regression with batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_opt_0_logistic_batch,degree_opt_0_logistic_batch = finetune_batch_logistic(tX_tilda_0, y_0, gamma = 1.7783e-04, degrees =np.arange(1, 4) , lambdas=random_interval(0.8, 1.5, 5) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# open the test file\n",
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238, 30)\n"
     ]
    }
   ],
   "source": [
    "print(tX_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now format the tX_test as we did for tX_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we split the test into the three subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test_0,tX_test_1,tX_test_2_3 = split_to_Jet_Num(tX_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a column of zeros and ones to detect whether the mass has been measured or not\n",
    "This should be done prior to splitting it is the same procedure and just wastes space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the indices where the mass is not calculated, add the column which has 0 in those indices\n",
    "# and 1 everywhere else for all matrices 0,1,2_3\n",
    "tX_test_0 = find_mass(tX_test_0)\n",
    "tX_test_1 = find_mass(tX_test_1)\n",
    "tX_test_2_3 = find_mass(tX_test_2_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We drop the same columns we have dropped for the X training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 7, 13, 23, 24, 25, 26, 27, 28, 29]\n",
      "(227458, 19)\n",
      "(227458, 19)\n",
      "[5, 6, 7, 13, 26, 27, 28]\n",
      "(175338, 23)\n",
      "(175338, 23)\n"
     ]
    }
   ],
   "source": [
    "tX_test_0 = fix_array(tX_test_0,0)\n",
    "print(tX_test_0.shape)\n",
    "tX_test_1 = fix_array(tX_test_1,1)\n",
    "print(tX_test_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we substitute the -999 values with the median\n",
    "This should also be done with a function it is the same thing repeated thrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_0 = fix_mean(tX_0)\n",
    "tX_1 = fix_mean(tX_1)\n",
    "tX_2_3 = fix_mean(tX_2_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We standardize the test set using the mean and the standard deviation of the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227458, 19)\n"
     ]
    }
   ],
   "source": [
    "print(tX_test_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99913, 19)\n"
     ]
    }
   ],
   "source": [
    "print(tX_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the data in the test set\n",
    "# should have used the same function both here and on the training part same process this is reduntant\n",
    "def standardize_test(x, mean, std):\n",
    "    \"\"\"Standardize the test set.\"\"\"\n",
    "    x = x - mean \n",
    "    x = x / std\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test_0[:,1:] = standardize_test(tX_test_0[:,1:], mean_0, std_0)  #we standardize everything a part from the column added manually\n",
    "tX_test_1[:,1:] = standardize_test(tX_test_1[:,1:], mean_1, std_1)  #we standardize everything a part from the column added manually\n",
    "tX_test_2_3[:,1:]= standardize_test(tX_test_2_3[:,1:], mean_2_3, std_2_3) #we standardize everything a part from the column added manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We insert the column for the bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_tilda_test_0 = np.insert(tX_test_0, 0, np.ones(tX_test_0.shape[0]), axis=1) #the first column now is all ones and is used for bias\n",
    "tX_tilda_test_1 = np.insert(tX_test_1, 0, np.ones(tX_test_1.shape[0]), axis=1) #the first column now is all ones and is used for bias\n",
    "tX_tilda_test_2_3 = np.insert(tX_test_2_3, 0, np.ones(tX_test_2_3.shape[0]), axis=1) #the first column now is all ones and is used for bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We make the predictions with GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_GD_0 = predict_GD(tX_tilda_test_0,w_GD_0,degree_opt_0_GD)\n",
    "predictions_GD_1 = predict_GD(tX_tilda_test_1,w_GD_1,degree_opt_1_GD)\n",
    "predictions_GD_2_3 = predict_GD(tX_tilda_test_2_3,w_GD_2_3,degree_opt_2_3_GD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We make the predictions with ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ridge_0 = predict_ridge(tX_tilda_test_0, w_ridge_0,degree_opt_0_ridge)\n",
    "predictions_ridge_1 = predict_ridge(tX_tilda_test_1, w_ridge_1,degree_opt_1_ridge)\n",
    "predictions_ridge_2_3 = predict_ridge(tX_tilda_test_2_3, w_ridge_2_3, degree_opt_2_3_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_logistic_0 = predict_logistic(tX_tilda_test_0,w_logistic_0,degree_opt_0_logistic)\n",
    "predictions_logistic_1 = predict_logistic(tX_tilda_test_1,w_logistic_1,degree_opt_1_logistic)\n",
    "predictions_logistic_2_3 = predict_logistic(tX_tilda_test_2_3,w_logistic_2_3,degree_opt_2_3_logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to reconstruct a single vector of predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate final prediction list and print it in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions_GD = create_output(tX_test,predictions_GD_0,predictions_GD_1,predictions_GD_2_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions_ridge = create_output(tX_test,predictions_ridge_0,predictions_ridge_1,predictions_ridge_2_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions_logistic = create_output(tX_test,predictions_logistic_0,predictions_logistic_1,predictions_logistic_2_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1 -1 ...  1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "print(final_predictions_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this does not seem to be used and should be removed in such a case\n",
    "#def predict_labels(weights, tX_test):\n",
    "#    y = np.array(tX_test) @ np.array(weights)\n",
    "#    labels = [1 if l > 0 else -1 for l in y]\n",
    "#    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH_GD = '../data/submission_GD.csv' # name towards GD output \n",
    "OUTPUT_PATH_RIDGE = '../data/submission_ridge.csv' # name towards ridge output \n",
    "OUTPUT_PATH_LOGISTIC = '../data/submission_logistic.csv' # name towards logistic output \n",
    "#create_csv_submission(ids_test, final_predictions_GD, OUTPUT_PATH_GD) # print csv file according to results\n",
    "create_csv_submission(ids_test, final_predictions_ridge, OUTPUT_PATH_RIDGE) # print csv file according to results\n",
    "#create_csv_submission(ids_test, final_predictions_logistic, OUTPUT_PATH_LOGISTIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
