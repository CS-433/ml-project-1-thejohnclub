{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our libraries\n",
    "from proj1_helpers import *\n",
    "from proj1_input_man import *\n",
    "from proj1_linear_model import *\n",
    "from proj1_ridge_regress import *\n",
    "from proj1_logistic import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv' # train data path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n"
     ]
    }
   ],
   "source": [
    "print(tX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the labels to {0,1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the array from -1 1 to 0 1\n",
    "# label simplification\n",
    "# y == 0 non detected Boson, y == 1 detected Boson\n",
    "y_ = np.array([0 if l == -1 else 1 for l in y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All of the following parts are the same for the test set, should be functions\n",
    "\n",
    "# less code = better code (in this case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the features by the number of jets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_0,tX_1,tX_2_3 = split_to_Jet_Num(tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing also the output by the type of particle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_0,y_1,y_2_3 = split_labels_to_Jet_Num(y_,tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a column of zeros and ones to detect whether the mass has been measured or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the indices where the mass is not calculated, add the column which has 0 in those indices\n",
    "# and 1 everywhere else for all matrices 0,1,2_3\n",
    "tX_0 = find_mass(tX_0)\n",
    "tX_1 = find_mass(tX_1)\n",
    "tX_2_3 = find_mass(tX_2_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Throwing away the outliers from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 7, 13, 23, 24, 25, 26, 27, 28, 29]\n",
      "(99913, 19)\n",
      "(99913, 19)\n",
      "[5, 6, 7, 13, 26, 27, 28]\n",
      "(77544, 23)\n",
      "(77544, 23)\n",
      "(72543, 31)\n",
      "(72543, 31)\n"
     ]
    }
   ],
   "source": [
    "tX_0 = fix_array(tX_0,0)\n",
    "print(tX_0.shape)\n",
    "tX_1 = fix_array(tX_1,1)\n",
    "print(tX_1.shape)\n",
    "tX_2_3 = fix_array(tX_2_3)\n",
    "print(tX_2_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we substitute the -999 values with the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_0 = fix_mean(tX_0)\n",
    "tX_1 = fix_mean(tX_1)\n",
    "tX_2_3 = fix_mean(tX_2_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tX_2_3[:,1:], mean_2_3,std_2_3 = standardize(tX_2_3[:,1:]) #we standardize everything a part from the column added manually\n",
    "tX_0[:,1:],mean_0,std_0 = standardize(tX_0[:,1:])\n",
    "tX_1[:,1:],mean_1,std_1 = standardize(tX_1[:,1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We insert the column for the bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_tilda_0 = np.insert(tX_0, 0, np.ones(tX_0.shape[0]), axis=1)\n",
    "tX_tilda_1 = np.insert(tX_1, 0, np.ones(tX_1.shape[0]), axis=1)\n",
    "tX_tilda_2_3 = np.insert(tX_2_3, 0, np.ones(tX_2_3.shape[0]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following cells of code aim at finetuning the hyperparameters of the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the optimal degree for gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.61847606  0.06020022  0.006644   ...  0.5805158   0.28062325\n",
      " -0.01899064]\n",
      "[1 0 0 ... 1 0 0]\n",
      "[0 0 0 ... 1 0 0]\n",
      "[ 0.15864768 -0.03560335  0.20053426 ...  0.608775   -0.01836511\n",
      "  0.03172052]\n",
      "[0 0 0 ... 1 0 0]\n",
      "[1 0 0 ... 1 0 0]\n",
      "[ 0.26937014 -0.00129739  0.03183365 ...  0.67090778  0.36748447\n",
      "  0.81767221]\n",
      "[0 0 0 ... 1 0 1]\n",
      "[0 0 0 ... 1 0 1]\n",
      "[0.11828094 0.27527221 0.22687442 ... 0.43692667 0.2788262  0.20996465]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[ 0.82239773 -0.01319973  0.0410209  ...  0.55932944  0.23118963\n",
      " -0.00646542]\n",
      "[1 0 0 ... 1 0 0]\n",
      "[0 0 0 ... 1 0 0]\n",
      "[ 0.47410793 -0.00442422  0.19485567 ...  0.63209243  0.13035331\n",
      "  0.03630025]\n",
      "[0 0 0 ... 1 0 0]\n",
      "[1 0 0 ... 1 0 0]\n",
      "[-0.16364379  0.00518823  0.01600962 ...  0.83116894  0.39815189\n",
      "  0.8920483 ]\n",
      "[0 0 0 ... 1 0 1]\n",
      "[0 0 0 ... 1 0 1]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-8514c25c5a0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdegree_opt_0_GD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinetune_GD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX_tilda_0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdegree_opt_1_GD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinetune_GD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX_tilda_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdegree_opt_2_3_GD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinetune_GD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX_tilda_2_3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_2_3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Luca\\Machine Learning\\Machine-Learning-P1\\project1\\scripts\\proj1_linear_model.py\u001b[0m in \u001b[0;36mfinetune_GD\u001b[1;34m(tX, y, k_fold, degrees)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mcurrent_sum_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m             \u001b[0mcurrent_test_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation_GD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegrees\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m10e-4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m             \u001b[0mcurrent_sum_test\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcurrent_test_acc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[0mtesting_acc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent_sum_test\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mk_fold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Luca\\Machine Learning\\Machine-Learning-P1\\project1\\scripts\\proj1_linear_model.py\u001b[0m in \u001b[0;36mcross_validation_GD\u001b[1;34m(y, x, k_indices, k, degree, gamma)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0my_training\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minterval\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0minterval\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlist_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0my_testing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m     \u001b[0mx_training_augmented\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_training\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m     \u001b[0mx_testing_augmented\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_testing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mws\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mleast_squares_GD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_training\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_training_augmented\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_training_augmented\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Luca\\Machine Learning\\Machine-Learning-P1\\project1\\scripts\\proj1_helpers.py\u001b[0m in \u001b[0;36mbuild_poly\u001b[1;34m(x, degree)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[0mphi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[0mphi_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexponent\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mexponent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpowers\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m         \u001b[0mphi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mphi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphi_i\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mphi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Luca\\Machine Learning\\Machine-Learning-P1\\project1\\scripts\\proj1_helpers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[0mphi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[0mphi_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexponent\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mexponent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpowers\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m         \u001b[0mphi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mphi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphi_i\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mphi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "degree_opt_0_GD = finetune_GD(tX_tilda_0,y_0)\n",
    "degree_opt_1_GD = finetune_GD(tX_tilda_1,y_1)\n",
    "degree_opt_2_3_GD = finetune_GD(tX_tilda_2_3,y_2_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell takes a long time to execute, the hyperparameters found are:\n",
    "# degree =2 for tX_tilda_0 with an accuracy of 0.82694771 on the validation set\n",
    "# degree =3 for tX_tilda_1 with an accuracy of 0.7937687 on the validation set\n",
    "# degree =3 for tX_tilda_2_3 with an accuracy of 0.81856907 on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the optimal lambda for ridge regression and the optimal degree for feature augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_opt_0,degree_opt_0 = finetune_ridge(tX_tilda_0,y_0)\n",
    "lambda_opt_1,degree_opt_1 = finetune_ridge(tX_tilda_1,y_1)\n",
    "lambda_opt_2_3,degree_opt_2_3 = finetune_ridge(tX_tilda_2_3,y_2_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the optimal weights with the calculated hyper parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ridge_0 = optimal_weights_ridge(tX_tilda_0,y_0,degree_opt_0,lambda_opt_0)\n",
    "w_ridge_1 = optimal_weights_ridge(tX_tilda_1,y_1,degree_opt_1,lambda_opt_1)\n",
    "w_ridge_2_3 = optimal_weights_ridge(tX_tilda_2_3,y_2_3,degree_opt_2_3,lambda_opt_2_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the optimal lambda for ridge regression and the optimal degree for feature augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_opt_0,degree_opt_0 = finetune_logistic(tX_tilda_0,y_0)\n",
    "lambda_opt_1,degree_opt_1 = finetune_logistic(tX_tilda_1,y_1)\n",
    "lambda_opt_2_3,degree_opt_2_3 = finetune_logistic(tX_tilda_2_3,y_2_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the optimal weights with the calculated hyper parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_logistic_0 = optimal_weights_logistic(tX_tilda_0,y_0,degree_opt_0,lambda_opt_0)\n",
    "w_logistic_1 = optimal_weights_logistic(tX_tilda_1,y_1,degree_opt_1,lambda_opt_1)\n",
    "w_logistic_2_3 = optimal_weights_logistic(tX_tilda_2_3,y_2_3,degree_opt_2_3,lambda_opt_2_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# open the test file\n",
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tX_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now format the tX_test as we did for tX_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we split the test into the three subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test_0,tX_test_1,tX_test_2_3 = split_to_Jet_Num(tX_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a column of zeros and ones to detect whether the mass has been measured or not\n",
    "This should be done prior to splitting it is the same procedure and just wastes space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the indices where the mass is not calculated, add the column which has 0 in those indices\n",
    "# and 1 everywhere else for all matrices 0,1,2_3\n",
    "tX_test_0 = find_mass(tX_test_0)\n",
    "tX_test_1 = find_mass(tX_test_1)\n",
    "tX_test_2_3 = find_mass(tX_test_2_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We drop the same columns we have dropped for the X training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test_0 = fix_array(tX_test_0,0)\n",
    "print(tX_test_0.shape)\n",
    "tX_test_1 = fix_array(tX_test_1,1)\n",
    "print(tX_test_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we substitute the -999 values with the median\n",
    "This should also be done with a function it is the same thing repeated thrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_0 = fix_mean(tX_0)\n",
    "tX_1 = fix_mean(tX_1)\n",
    "tX_2_3 = fix_mean(tX_2_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We standardize the test set using the mean and the standard deviation of the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tX_test_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tX_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the data in the test set\n",
    "# should have used the same function both here and on the training part same process this is reduntant\n",
    "def standardize_test(x, mean, std):\n",
    "    \"\"\"Standardize the test set.\"\"\"\n",
    "    x = x - mean \n",
    "    x = x / std\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test_0[:,1:] = standardize_test(tX_test_0[:,1:], mean_0, std_0)  #we standardize everything a part from the column added manually\n",
    "tX_test_1[:,1:] = standardize_test(tX_test_1[:,1:], mean_1, std_1)  #we standardize everything a part from the column added manually\n",
    "tX_test_2_3[:,1:]= standardize_test(tX_test_2_3[:,1:], mean_2_3, std_2_3) #we standardize everything a part from the column added manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We insert the column for the bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_tilda_test_0 = np.insert(tX_test_0, 0, np.ones(tX_test_0.shape[0]), axis=1) #the first column now is all ones and is used for bias\n",
    "tX_tilda_test_1 = np.insert(tX_test_1, 0, np.ones(tX_test_1.shape[0]), axis=1) #the first column now is all ones and is used for bias\n",
    "tX_tilda_test_2_3 = np.insert(tX_test_2_3, 0, np.ones(tX_test_2_3.shape[0]), axis=1) #the first column now is all ones and is used for bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We make the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ridge_0 = predict_ridge(tX_tilda_test_0,w_ridge_0,6)\n",
    "predictions_ridge_1 = predict_ridge(tX_tilda_test_1,w_ridge_1,6)\n",
    "predictions_ridge_2_3 = predict_ridge(tX_tilda_test_2_3,w_ridge_2_3,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_logistic_0 = predict_logistic(tX_tilda_test_0,w_logistic_0,2)\n",
    "predictions_logistic_1 = predict_logistic(tX_tilda_test_1,w_logistic_1,2)\n",
    "predictions_logistic_2_3 = predict_logistic(tX_tilda_test_2_3,w_logistic_2_3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to reconstruct a single vector of predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate final prediction list and print it in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions_ridge = create_output(tX_test,predictions_ridge_0,predictions_ridge_1,predictions_ridge_2_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions_logistic = create_output(tX_test,predictions_logistic_0,predictions_logistic_1,predictions_logistic_2_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_predictions_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this does not seem to be used and should be removed in such a case\n",
    "#def predict_labels(weights, tX_test):\n",
    "#    y = np.array(tX_test) @ np.array(weights)\n",
    "#    labels = [1 if l > 0 else -1 for l in y]\n",
    "#    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH_RIDGE = '../data/submission_ridge.csv' # name towards ridge output \n",
    "OUTPUT_PATH_LOGISTIC = '../data/submission_logistic.csv' # name towards logistic output\n",
    "#y_pred = predict_labels(weights, tX_test)\n",
    "#y_pred = final_predictions_ridge # seems reduntant \n",
    "create_csv_submission(ids_test, final_predictions_ridge, OUTPUT_PATH_RIDGE) # print csv file according to results\n",
    "create_csv_submission(ids_test, final_predictions_logistic, OUTPUT_PATH_LOGISTIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
