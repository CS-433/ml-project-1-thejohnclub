{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # train data path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n"
     ]
    }
   ],
   "source": [
    "print(tX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the labels to {0,1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y == 0 non detected Boson, y == 1 detected Boson\n",
    "y_ = np.array([0 if l == -1 else 1 for l in y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the features by the number of jets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing the rows of tX by the number of jets, dropping the column Pri_Jet_Num and adding an extra column of np.ones\n",
    "zero_indices = []\n",
    "one_indices = []\n",
    "two_three_indices = []\n",
    "zero_indices = np.where(tX[:,22]==0)[0]\n",
    "one_indices = np.where(tX[:,22]==1)[0]\n",
    "two_three_indices = np.where(np.logical_or(tX[:,22]==2, tX[:,22]==3))[0]\n",
    "tX_0 = tX[zero_indices, :]\n",
    "tX_0 = np.delete(tX_0, 22, axis=1)\n",
    "tX_1 = tX[one_indices, :]\n",
    "tX_1 = np.delete(tX_1, 22, axis=1)\n",
    "tX_2_3 = tX[two_three_indices, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing also the output by the type of particle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_0 = y_[zero_indices]\n",
    "y_1 = y_[one_indices]\n",
    "y_2_3 = y_[two_three_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a column of zeros and ones to detect whether the mass has been measured or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the indices where the mass is not calculated, add the column which has 0 in those indices\n",
    "# and 1 everywhere else for all matrices 0,1,2_3\n",
    "zero_indices_0 = np.where(tX_0[:,1] == -999.)[0]\n",
    "column_to_add = np.array([0 if i in zero_indices_0 else 1 for i in range(tX_0.shape[0])])\n",
    "tX_0 = np.insert(tX_0, 0, column_to_add, axis=1)\n",
    "zero_indices_1 = np.where(tX_1[:,1] == -999.)[0]\n",
    "column_to_add = np.array([0 if i in zero_indices_1 else 1 for i in range(tX_1.shape[0])])\n",
    "tX_1 = np.insert(tX_1, 0, column_to_add, axis=1)\n",
    "zero_indices_2_3 = np.where(tX_2_3[:,1] == -999.)[0]\n",
    "column_to_add = np.array([0 if i in zero_indices_2_3 else 1 for i in range(tX_2_3.shape[0])])\n",
    "tX_2_3 = np.insert(tX_2_3, 0, column_to_add, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Throwing away the outliers from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, tX_2_3.shape[1]):\n",
    "    index_column_valid =np.where(tX_2_3[:,i] != -999.)[0]\n",
    "    column_25_quantile, column_75_quantile = np.quantile(tX_2_3[index_column_valid,i], \n",
    "                                                         np.array([0.25, 0.75]))\n",
    "    interquantile = column_75_quantile-column_25_quantile\n",
    "    column_15_quantile, column_85_quantile = np.quantile(tX_2_3[index_column_valid,i], \n",
    "                                                         np.array([0.15, 0.85]))\n",
    "    indices_outliers = np.where((column_15_quantile - 1.5 * interquantile >= tX_2_3[index_column_valid,i])\n",
    "                                             | (tX_2_3[index_column_valid,i] >= \n",
    "                                                column_85_quantile + 1.5 * interquantile))[0]\n",
    "    #indices_outliers = np.argwhere((tX_tilda_2_3[index_column_valid,i] >= column_85_quantile + 1.5 * interquantile) | \n",
    "                                  #(column_15_quantile - 1.5 * interquantile >= tX_tilda_2_3[index_column_valid,i]))\n",
    "    median = np.median(tX_2_3[index_column_valid, i], axis = 0)\n",
    "    #print(np.sort(tX_tilda_2_3[index_column_valid[indices_outliers],i]).T)\n",
    "    #print(np.where(tX_tilda_2_3[indices_outliers,i])==-999.)\n",
    "    #print(median)\n",
    "    tX_2_3[index_column_valid[indices_outliers],i] =  median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99913, 19)\n",
      "[5, 6, 7, 13, 23, 24, 25, 26, 27, 28, 29]\n"
     ]
    }
   ],
   "source": [
    "col_to_delete_0 = []\n",
    "for i in range(1, tX_0.shape[1]):\n",
    "    index_column_valid =np.where(tX_0[:,i] != -999.)[0]\n",
    "    if len(index_column_valid)==0:\n",
    "        #we drop the column (we will have to do the same for the test set as well)\n",
    "        col_to_delete_0.append(i)\n",
    "    else :\n",
    "        column_25_quantile, column_75_quantile = np.quantile(tX_0[index_column_valid,i], \n",
    "                                                         np.array([0.25, 0.75]))\n",
    "        interquantile = column_75_quantile-column_25_quantile\n",
    "        column_15_quantile, column_85_quantile = np.quantile(tX_0[index_column_valid,i], \n",
    "                                                         np.array([0.15, 0.85]))\n",
    "        indices_outliers = np.where((column_15_quantile - 1.5 * interquantile >= tX_0[index_column_valid,i])\n",
    "                                             | (tX_0[index_column_valid,i] >= \n",
    "                                                column_85_quantile + 1.5 * interquantile))[0]\n",
    "        #indices_outliers = np.argwhere((tX_tilda_2_3[index_column_valid,i] >= column_85_quantile + 1.5 * interquantile) | \n",
    "                                  #(column_15_quantile - 1.5 * interquantile >= tX_tilda_2_3[index_column_valid,i]))\n",
    "        median = np.median(tX_0[index_column_valid, i], axis = 0)\n",
    "        #print(np.sort(tX_tilda_2_3[index_column_valid[indices_outliers],i]).T)\n",
    "        #print(np.where(tX_tilda_2_3[indices_outliers,i])==-999.)\n",
    "        #print(median)\n",
    "        tX_0[index_column_valid[indices_outliers],i] =  median\n",
    "col_to_delete_0.append(tX_0.shape[1]-1)\n",
    "tX_0 = np.delete(tX_0, col_to_delete_0, axis=1)\n",
    "print(tX_0.shape)\n",
    "print(col_to_delete_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 7, 13, 26, 27, 28]\n"
     ]
    }
   ],
   "source": [
    "col_to_delete_1 = []\n",
    "for i in range(1, tX_1.shape[1]):\n",
    "    index_column_valid =np.where(tX_1[:,i] != -999.)[0]\n",
    "    if len(index_column_valid)==0:\n",
    "        #we drop the column (we will have to do the same for the test set as well)\n",
    "        col_to_delete_1.append(i)\n",
    "    else :\n",
    "        column_25_quantile, column_75_quantile = np.quantile(tX_1[index_column_valid,i], \n",
    "                                                         np.array([0.25, 0.75]))\n",
    "        interquantile = column_75_quantile-column_25_quantile\n",
    "        column_15_quantile, column_85_quantile = np.quantile(tX_1[index_column_valid,i], \n",
    "                                                         np.array([0.15, 0.85]))\n",
    "        indices_outliers = np.where((column_15_quantile - 1.5 * interquantile >= tX_1[index_column_valid,i])\n",
    "                                             | (tX_1[index_column_valid,i] >= \n",
    "                                                column_85_quantile + 1.5 * interquantile))[0]\n",
    "        #indices_outliers = np.argwhere((tX_tilda_2_3[index_column_valid,i] >= column_85_quantile + 1.5 * interquantile) | \n",
    "                                  #(column_15_quantile - 1.5 * interquantile >= tX_tilda_2_3[index_column_valid,i]))\n",
    "        median = np.median(tX_1[index_column_valid, i], axis = 0)\n",
    "        #print(np.sort(tX_tilda_2_3[index_column_valid[indices_outliers],i]).T)\n",
    "        #print(np.where(tX_tilda_2_3[indices_outliers,i])==-999.)\n",
    "        #print(median)\n",
    "        tX_1[index_column_valid[indices_outliers],i] =  median\n",
    "tX_1 = np.delete(tX_1, col_to_delete_1, axis=1)\n",
    "print(col_to_delete_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we substitute the -999 values with the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, tX_2_3.shape[1]):\n",
    "    index_column_non_valid =np.where(tX_2_3[:,i] == -999.)[0]\n",
    "    index_column_valid =np.where(tX_2_3[:,i] != -999.)[0]\n",
    "    median = np.median(tX_2_3[index_column_valid, i], axis = 0)\n",
    "    tX_2_3[index_column_non_valid,i] =  median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, tX_1.shape[1]):\n",
    "    index_column_non_valid =np.where(tX_1[:,i] == -999.)[0]\n",
    "    index_column_valid =np.where(tX_1[:,i] != -999.)[0]\n",
    "    median = np.median(tX_1[index_column_valid, i], axis = 0)\n",
    "    tX_1[index_column_non_valid,i] =  median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, tX_0.shape[1]):\n",
    "    index_column_non_valid =np.where(tX_0[:,i] == -999.)[0]\n",
    "    index_column_valid =np.where(tX_0[:,i] != -999.)[0]\n",
    "    median = np.median(tX_0[index_column_valid, i], axis = 0)\n",
    "    tX_0[index_column_non_valid,i] =  median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tX_2_3[:,1:], mean_2_3,std_2_3 = standardize(tX_2_3[:,1:]) #we standardize everything a part from the column added manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.97351249  0.46588281 ...  0.61614788 -1.36131161\n",
      "  -0.70374641]\n",
      " [ 1.         -0.82851663 -0.77157038 ...  0.11608109  1.71034105\n",
      "   0.2995537 ]\n",
      " [ 1.          1.3538447  -0.27431587 ...  0.07030726 -1.52202162\n",
      "   0.12704911]\n",
      " ...\n",
      " [ 1.          0.04006392  0.02513449 ...  0.25930888  0.22982758\n",
      "   0.42357227]\n",
      " [ 1.          0.66304099 -1.0843679  ...  0.29031696 -1.21821366\n",
      "  -0.20699627]\n",
      " [ 1.          0.04006392  0.31977857 ... -0.02271698 -0.62490751\n",
      "   0.05569682]]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(tX_2_3)\n",
    "print(np.count_nonzero(tX_2_3 == -999.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000e+00  1.43905e+02  8.14170e+01 ...  3.10820e+01  6.00000e-02\n",
      "   8.60620e+01]\n",
      " [ 1.00000e+00  1.75864e+02  1.69150e+01 ...  2.72300e+00 -8.71000e-01\n",
      "   5.31310e+01]\n",
      " [ 1.00000e+00  1.05594e+02  5.05590e+01 ...  3.77910e+01  2.40000e-02\n",
      "   1.29804e+02]\n",
      " ...\n",
      " [ 1.00000e+00  1.11452e+02  5.81790e+01 ...  4.67370e+01 -8.67000e-01\n",
      "   8.04080e+01]\n",
      " [ 1.00000e+00  9.49510e+01  1.93620e+01 ...  1.21500e+01  8.11000e-01\n",
      "   1.12718e+02]\n",
      " [ 1.00000e+00  1.11452e+02  7.27560e+01 ...  4.07290e+01 -1.59600e+00\n",
      "   9.94050e+01]]\n"
     ]
    }
   ],
   "source": [
    "print(tX_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_0[:,1:],mean_0,std_0 = standardize(tX_0[:,1:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99913, 19)\n"
     ]
    }
   ],
   "source": [
    "print(tX_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_1[:,1:],mean_1,std_1 = standardize(tX_1[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000000e+00  1.55539992e+00  7.27047143e-01 ...  3.98445313e-01\n",
      "   6.45414781e-01 -4.14297220e-01]\n",
      " [ 1.00000000e+00  1.45598722e-03  3.58462301e+00 ...  1.12748232e+00\n",
      "  -1.10752634e+00 -4.89864560e-01]\n",
      " [ 1.00000000e+00  1.36261180e+00 -1.05809645e+00 ... -3.92076740e-01\n",
      "  -9.40265168e-01 -1.01072441e+00]\n",
      " ...\n",
      " [ 1.00000000e+00  1.45598722e-03  1.01732036e+00 ... -4.67286130e-01\n",
      "  -3.80160315e-01  8.39087556e-01]\n",
      " [ 1.00000000e+00  6.75509966e-01  9.95415259e-01 ... -6.76994064e-01\n",
      "   1.39533906e+00  5.32418071e-01]\n",
      " [ 1.00000000e+00 -2.21030015e-01  4.74893699e-01 ...  9.88591985e-01\n",
      "  -8.30516498e-02 -5.76298292e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(tX_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We insert the column for the bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_tilda_0 = np.insert(tX_0, 0, np.ones(tX_0.shape[0]), axis=1)\n",
    "tX_tilda_1 = np.insert(tX_1, 0, np.ones(tX_1.shape[0]), axis=1)\n",
    "tX_tilda_2_3 = np.insert(tX_2_3, 0, np.ones(tX_2_3.shape[0]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors = ['red', 'blue']\n",
    "# x_pos=[]\n",
    "# x_neg=[]\n",
    "\n",
    "# for j in range(len(y)):\n",
    "#  if(y[j]==1):\n",
    "#       x_pos.insert(0,tX[j])\n",
    "#    else:\n",
    "#        x_neg.insert(0,tX[j])\n",
    "# xpos = np.array(x_pos)\n",
    "# xneg = np.array(x_neg)\n",
    "# for i in range(tX.shape[1]):\n",
    "#  plt.hist(xpos[:,i], alpha = 0.5, color = 'r', bins = 100)\n",
    "#  plt.hist(xneg[:,i], alpha = 0.5, color = 'b', bins = 100)\n",
    "#  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    N = y.shape[0]\n",
    "    e = y - tx @ w \n",
    "    loss = 1/(2*N) * np.dot(e,e)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    N = y.shape[0]\n",
    "    e = y - tx @ w\n",
    "    gradient = -(1/N) * (tx.T) @ (e)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        gradient = compute_gradient(y,tx,w)\n",
    "        w = w - gamma * gradient\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        if n_iter %100==0:\n",
    "            print('gradient descent loss', loss)\n",
    "        # print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "        # bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    N = y.shape[0]\n",
    "    random_number = random.randint(0,N)\n",
    "    #random_number =1\n",
    "    xn = tx[random_number,:]\n",
    "    random_gradient = - np.dot(xn, y[random_number] - np.dot(xn,w))\n",
    "    return random_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        stoch_gradient = compute_stoch_gradient(y,tx,w)\n",
    "        w = w - gamma * stoch_gradient\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        if n_iter %100==0:\n",
    "            print('gradient descent loss', loss)\n",
    "        # print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "        #    bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "\n",
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    forcing_term = np.transpose(tx) @ y\n",
    "    coefficient_matrix = np.transpose(tx) @ tx\n",
    "    w = np.linalg.solve(coefficient_matrix, forcing_term)\n",
    "    return w\n",
    "\n",
    "def test_your_least_squares(y, tx):\n",
    "    \"\"\"compare the solution of the normal equations with the weights returned by gradient descent algorithm.\"\"\"\n",
    "    w_least_squares = least_squares(y, tx)\n",
    "    initial_w = np.zeros(tx.shape[1])\n",
    "    max_iters = 50\n",
    "    gamma = 0.7\n",
    "    losses_gradient_descent, w_gradient_descent = gradient_descent(y, tx, initial_w, max_iters, gamma)\n",
    "    w = w_gradient_descent[-1]\n",
    "    err = np.linalg.norm(w_least_squares-w)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_grad_desc(y, x, k_indices, k, degree, gamma):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    N = y.shape[0]\n",
    "    k_fold = k_indices.shape[0]\n",
    "    list_ = []\n",
    "    interval = int(N/k_fold)\n",
    "    for i in range(k_fold):\n",
    "        if i != k:\n",
    "            list_.append(i)\n",
    "    x_training = np.zeros((int((k_fold-1)/k_fold*N), x.shape[1]))\n",
    "    y_training = np.zeros(int((k_fold-1)/k_fold*N))\n",
    "    for j in range(len(list_)):\n",
    "        x_training[interval*(j):interval*(j+1), :] = x[np.array([k_indices[list_[j]]]), :]\n",
    "    x_testing = x[k_indices[k], :]\n",
    "    for j in range(len(list_)):\n",
    "        y_training[interval*(j):interval*(j+1)] = y[np.array([k_indices[list_[j]]])]\n",
    "    y_testing = y[k_indices[k]]\n",
    "    x_training_augmented = build_poly(x_training, degree)\n",
    "    x_testing_augmented = build_poly(x_testing, degree)\n",
    "    losses, ws = least_squares_GD(y_training, x_training_augmented, np.zeros(x_training_augmented.shape[1]) , 2000, gamma)\n",
    "    w_opt_training = ws[-1]\n",
    "    #loss_tr = compute_loss(y_training, x_training_augmented, w_opt_training)\n",
    "    #loss_te = compute_loss(y_testing, x_testing_augmented, w_opt_training)\n",
    "    predictions_test = x_testing_augmented@w_opt_training\n",
    "    print(predictions_test)\n",
    "    predictions_test = np.array([0 if el <0.5 else 1 for el in predictions_test])\n",
    "    print(predictions_test)\n",
    "    print(y_testing)\n",
    "    acc_test = compute_accuracy(y_testing, predictions_test)\n",
    "    return acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_test, pred):\n",
    "    N = y_test.shape[0]\n",
    "    accuracy = (y_test == pred).sum() / N\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    N = tx.shape\n",
    "    lambda_prime = 2 * N[0] * lambda_\n",
    "    coefficient_matrix = np.transpose(tx) @ tx + lambda_prime * np.eye(N[1])\n",
    "    forcing_term = np.transpose(tx) @ y\n",
    "    w = np.linalg.solve(coefficient_matrix, forcing_term)\n",
    "    return w\n",
    "\n",
    "def debug_ridge(y, tx):\n",
    "    \"\"\"debugging the ridge regression by setting lambda=0.\"\"\"\n",
    "    w_least_squares = least_squares(y, tx)\n",
    "    w_0 = ridge_regression(y, tx, 0)\n",
    "    err = np.linalg.norm(w_least_squares-w_0)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply the sigmoid function on t.\"\"\"\n",
    "    if t >= 0:\n",
    "        z = exp(-x)\n",
    "        return 1 / (1 + z)\n",
    "    else:\n",
    "        z = exp(x)\n",
    "        return z / (1 + z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the loss: negative log likelihood.\"\"\"\n",
    "    term1 = sigmoid(tx @ w)\n",
    "    term1[y == 0] = 1\n",
    "    term2 = 1 - sigmoid(tx @ w)\n",
    "    term2[y == 1] = 1\n",
    "    summands = np.multiply(y, np.log(term1)) + np.multiply(1 - y, np.log(term2))\n",
    "    # e = - y[i] * (tx[:,i] @ w) + np.log(1 + np.exp(1 + tx @ w))\n",
    "    # return e.sum()\n",
    "    # loss = - np.sum(y*np.log(sigmoid(tx@w))+((1-y) *np.log(1-sigmoid(tx@w))))\n",
    "    # return e.sum()\n",
    "    return - np.sum(summands) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    return np.transpose(tx) @ (sigmoid(tx @ w) - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent(y, tx, w_initial, gamma, max_iters, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    w1 = w_initial\n",
    "    for iter in range(max_iters):\n",
    "        grad = calculate_gradient(y, tx, w1)\n",
    "        w = w1 - gamma * grad\n",
    "        loss = calculate_loss(y, tx, w) + 2*lambda_*np.linalg.norm(w) ** 2\n",
    "        losses.append(loss)\n",
    "        w1 = w\n",
    "    return losses, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hessian(y, tx, w):\n",
    "    \"\"\"return the Hessian of the loss function.\"\"\"\n",
    "    diag = sigmoid(tx @ w) * (1 - sigmoid(tx @ w))\n",
    "    D = diag * np.eye(tx.shape[0])\n",
    "    return np.transpose(tx) @ D @ tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, w):\n",
    "    \"\"\"return the loss, gradient, and Hessian.\"\"\"\n",
    "    grad = calculate_gradient(y, tx, w)\n",
    "    hess = calculate_hessian(y, tx, w)\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    return loss, grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_newton_method(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step on Newton's method.\n",
    "    return the loss and updated w.\n",
    "    \"\"\"\n",
    "    loss, grad, hess = logistic_regression(y, tx, w)\n",
    "    sol = np.linalg.solve(hess, grad)\n",
    "    w = w - gamma * sol\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "    \"\"\"return the loss, gradient\"\"\"\n",
    "    loss = calculate_loss(y, tx, w) + lambda_*np.linalg.norm(w) ** 2\n",
    "    grad = calculate_gradient(y, tx, w) + 2*lambda_*w\n",
    "    hess = calculate_hessian(y, tx, w) + 2*lambda_*np.eye(w.shape[0])\n",
    "    return loss, grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_penalized_gradient(y, tx, w_initial, gamma, max_iters, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent, using the penalized logistic regression.\n",
    "    Return the loss and updated w.\n",
    "    \"\"\"\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "    w1 = w_initial\n",
    "    for iter in range(max_iters):\n",
    "        grad = calculate_gradient(y, tx, w1) + 2*lambda_*w1\n",
    "        #sol = np.linalg.solve(hess, grad)\n",
    "        w = w1 - gamma * grad\n",
    "        loss = calculate_loss(y, tx, w) + 2*lambda_*np.linalg.norm(w) ** 2\n",
    "        losses.append(loss)\n",
    "        w1 = w\n",
    "        if iter % 25 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    return losses, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=1 up to j=degree.\"\"\"\n",
    "    powers = np.arange(1, degree + 1)\n",
    "    #phi = np.column_stack([np.power(x[:,0], exponent) for exponent in powers])\n",
    "    phi = x[:,0]\n",
    "    for i in range(1, x.shape[1]):\n",
    "        phi_i = np.column_stack([np.power(x[:,i], exponent) for exponent in powers])\n",
    "        phi = np.column_stack([phi, phi_i])\n",
    "    return phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    N = y.shape[0]\n",
    "    np.random.seed(seed)\n",
    "    interval = int(np.floor(N / k_fold))\n",
    "    indices = np.random.permutation(N)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_ridge(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    N = y.shape[0]\n",
    "    k_fold = k_indices.shape[0]\n",
    "    list_ = []\n",
    "    interval = int(N/k_fold)\n",
    "    for i in range(k_fold):\n",
    "        if i != k:\n",
    "            list_.append(i)\n",
    "    x_training = np.zeros((int((k_fold-1)/k_fold*N), x.shape[1]))\n",
    "    y_training = np.zeros(int((k_fold-1)/k_fold*N))\n",
    "    for j in range(len(list_)):\n",
    "        x_training[interval*(j):interval*(j+1), :] = x[np.array([k_indices[list_[j]]]), :]\n",
    "    x_testing = x[k_indices[k], :]\n",
    "    for j in range(len(list_)):\n",
    "        y_training[interval*(j):interval*(j+1)] = y[np.array([k_indices[list_[j]]])]\n",
    "    y_testing = y[k_indices[k]]\n",
    "    x_training_augmented = build_poly(x_training, degree)\n",
    "    x_testing_augmented = build_poly(x_testing, degree)\n",
    "    w_opt_training = ridge_regression(y_training, x_training_augmented, lambda_)\n",
    "    loss_tr = compute_loss(y_training, x_training_augmented, w_opt_training)\n",
    "    loss_te = compute_loss(y_testing, x_testing_augmented, w_opt_training)\n",
    "    return loss_tr, loss_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_interval(low, high, size):\n",
    "    sample = np.random.uniform(low, high, size)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will tune hyperparameters for simple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters tuning for GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient descent loss 0.12732537966744067\n",
      "gradient descent loss 0.07514296948276802\n",
      "gradient descent loss 0.06936189627881702\n",
      "gradient descent loss 0.06841761462572284\n",
      "gradient descent loss 0.06810393485501035\n",
      "gradient descent loss 0.06791644525713052\n",
      "gradient descent loss 0.06777792666186155\n",
      "gradient descent loss 0.0676692519628826\n",
      "gradient descent loss 0.06758201240116901\n",
      "gradient descent loss 0.06751108696409762\n",
      "gradient descent loss 0.06745293018640557\n",
      "gradient descent loss 0.06740493987089534\n",
      "gradient descent loss 0.06736513926657837\n",
      "gradient descent loss 0.06733199194963059\n",
      "gradient descent loss 0.06730428464704838\n",
      "gradient descent loss 0.06728104840752037\n",
      "gradient descent loss 0.06726150295256433\n",
      "gradient descent loss 0.0672450159057109\n",
      "gradient descent loss 0.06723107209065948\n",
      "gradient descent loss 0.06721924995054254\n",
      "[ 0.61847606  0.06020022  0.006644   ...  0.5805158   0.28062325\n",
      " -0.01899064]\n",
      "[1 0 0 ... 1 0 0]\n",
      "[0 0 0 ... 1 0 0]\n",
      "0.8145968452237969\n",
      "gradient descent loss 0.12689166466490512\n",
      "gradient descent loss 0.07503179089828232\n",
      "gradient descent loss 0.06927775277522912\n",
      "gradient descent loss 0.06833405713422884\n",
      "gradient descent loss 0.06801967698940967\n",
      "gradient descent loss 0.0678323535361318\n",
      "gradient descent loss 0.06769457450509309\n",
      "gradient descent loss 0.06758686627358379\n",
      "gradient descent loss 0.06750061955136945\n",
      "gradient descent loss 0.06743061469475915\n",
      "gradient descent loss 0.06737326466818923\n",
      "gradient descent loss 0.06732595578979578\n",
      "gradient descent loss 0.06728671501242099\n",
      "gradient descent loss 0.06725401672733053\n",
      "gradient descent loss 0.06722666124283444\n",
      "gradient descent loss 0.06720369365468229\n",
      "gradient descent loss 0.06718434702496708\n",
      "gradient descent loss 0.06716800106459847\n",
      "gradient descent loss 0.06715415123042591\n",
      "gradient descent loss 0.06714238513119791\n",
      "[ 0.15864768 -0.03560335  0.20053426 ...  0.608775   -0.01836511\n",
      "  0.03172052]\n",
      "[0 0 0 ... 1 0 0]\n",
      "[1 0 0 ... 1 0 0]\n",
      "0.8164785010809512\n",
      "gradient descent loss 0.12839298582752823\n",
      "gradient descent loss 0.0755915243550824\n",
      "gradient descent loss 0.06978024942492031\n",
      "gradient descent loss 0.06883701638162723\n",
      "gradient descent loss 0.0685236665153681\n",
      "gradient descent loss 0.06833543456488098\n",
      "gradient descent loss 0.06819579949813578\n",
      "gradient descent loss 0.0680859671518859\n",
      "gradient descent loss 0.06799765666960227\n",
      "gradient descent loss 0.06792578570306286\n",
      "gradient descent loss 0.06786681079438023\n",
      "gradient descent loss 0.06781811807276061\n",
      "gradient descent loss 0.0677777153009359\n",
      "gradient descent loss 0.06774405079309453\n",
      "gradient descent loss 0.06771589769664331\n",
      "gradient descent loss 0.06769227551283889\n",
      "gradient descent loss 0.06767239435231846\n",
      "gradient descent loss 0.06765561391672463\n",
      "gradient descent loss 0.06764141252197757\n",
      "gradient descent loss 0.06762936326386518\n",
      "[ 0.26937014 -0.00129739  0.03183365 ...  0.67090778  0.36748447\n",
      "  0.81767221]\n",
      "[0 0 0 ... 1 0 1]\n",
      "[0 0 0 ... 1 0 1]\n",
      "0.8220033629594042\n",
      "gradient descent loss 0.12765900659246804\n",
      "gradient descent loss 0.07549116316252738\n",
      "gradient descent loss 0.06970254184433862\n",
      "gradient descent loss 0.0687502717847882\n",
      "gradient descent loss 0.06842995454718721\n",
      "gradient descent loss 0.06823731592279042\n",
      "gradient descent loss 0.06809477327517674\n",
      "gradient descent loss 0.0679828859321437\n",
      "gradient descent loss 0.06789303191600558\n",
      "gradient descent loss 0.06781994736061997\n",
      "gradient descent loss 0.06775998787635523\n",
      "gradient descent loss 0.06771047990070189\n",
      "gradient descent loss 0.06766939326273616\n",
      "gradient descent loss 0.06763515034840714\n",
      "gradient descent loss 0.06760650540424432\n",
      "gradient descent loss 0.06758246344108508\n",
      "gradient descent loss 0.06756222305434195\n",
      "gradient descent loss 0.06754513457260018\n",
      "gradient descent loss 0.06753066856344168\n",
      "gradient descent loss 0.0675183916537879\n",
      "[0.11828094 0.27527221 0.22687442 ... 0.43692667 0.2788262  0.20996465]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[0 0 0 ... 1 1 1]\n",
      "0.82012170710225\n",
      "gradient descent loss 0.12732537966744067\n",
      "gradient descent loss 0.06675122313949883\n",
      "gradient descent loss 0.06401641878136477\n",
      "gradient descent loss 0.0632640950002836\n",
      "gradient descent loss 0.06294298360461567\n",
      "gradient descent loss 0.06276679149120899\n",
      "gradient descent loss 0.06265097266394963\n",
      "gradient descent loss 0.0625653377553975\n",
      "gradient descent loss 0.06249735805272001\n",
      "gradient descent loss 0.06244106038170594\n",
      "gradient descent loss 0.062393219405942495\n",
      "gradient descent loss 0.0623518964630776\n",
      "gradient descent loss 0.06231581734004484\n",
      "gradient descent loss 0.06228408181527446\n",
      "gradient descent loss 0.06225601696413023\n",
      "gradient descent loss 0.0622310978124695\n",
      "gradient descent loss 0.06220890165339876\n",
      "gradient descent loss 0.06218908014872615\n",
      "gradient descent loss 0.062171341312889754\n",
      "gradient descent loss 0.0621554372635977\n",
      "[ 0.82239773 -0.01319973  0.0410209  ...  0.55932944  0.23118963\n",
      " -0.00646542]\n",
      "[1 0 0 ... 1 0 0]\n",
      "[0 0 0 ... 1 0 0]\n",
      "0.8235647369685323\n",
      "gradient descent loss 0.12689166466490512\n",
      "gradient descent loss 0.06655657886542703\n",
      "gradient descent loss 0.06384901980287978\n",
      "gradient descent loss 0.06310591026700249\n",
      "gradient descent loss 0.06278982123459602\n",
      "gradient descent loss 0.06261770913530315\n",
      "gradient descent loss 0.06250576682036525\n",
      "gradient descent loss 0.0624238772322633\n",
      "gradient descent loss 0.06235945988725243\n",
      "gradient descent loss 0.0623064953838606\n",
      "gradient descent loss 0.062261735923240434\n",
      "gradient descent loss 0.06222323872718662\n",
      "gradient descent loss 0.06218973657551584\n",
      "gradient descent loss 0.06216034223095559\n",
      "gradient descent loss 0.06213439861810401\n",
      "gradient descent loss 0.0621113976581627\n",
      "gradient descent loss 0.062090933547756015\n",
      "gradient descent loss 0.062072674278469964\n",
      "gradient descent loss 0.06205634330675867\n",
      "gradient descent loss 0.06204170715028034\n",
      "[ 0.47410793 -0.00442422  0.19485567 ...  0.63209243  0.13035331\n",
      "  0.03630025]\n",
      "[0 0 0 ... 1 0 0]\n",
      "[1 0 0 ... 1 0 0]\n",
      "0.8223236448074306\n",
      "gradient descent loss 0.12839298582752823\n",
      "gradient descent loss 0.06728162291005789\n",
      "gradient descent loss 0.06457556408033471\n",
      "gradient descent loss 0.06382388160038521\n",
      "gradient descent loss 0.06350235123250428\n",
      "gradient descent loss 0.06332644584930862\n",
      "gradient descent loss 0.06321134774669034\n",
      "gradient descent loss 0.06312655766001614\n",
      "gradient descent loss 0.06305937910157007\n",
      "gradient descent loss 0.0630037709096015\n",
      "gradient descent loss 0.0629564915819075\n",
      "gradient descent loss 0.06291560934720067\n",
      "gradient descent loss 0.06287986533926687\n",
      "gradient descent loss 0.06284837638966924\n",
      "gradient descent loss 0.0628204855107072\n",
      "gradient descent loss 0.06279568157608817\n",
      "gradient descent loss 0.06277355348582742\n",
      "gradient descent loss 0.062753762447432\n",
      "gradient descent loss 0.0627360242467368\n",
      "gradient descent loss 0.06272009729511849\n",
      "[-0.16364379  0.00518823  0.01600962 ...  0.83116894  0.39815189\n",
      "  0.8920483 ]\n",
      "[0 0 0 ... 1 0 1]\n",
      "[0 0 0 ... 1 0 1]\n",
      "0.830450796701097\n",
      "gradient descent loss 0.12765900659246804\n",
      "gradient descent loss 0.06713485122452681\n",
      "gradient descent loss 0.06448990940985835\n",
      "gradient descent loss 0.06375999413217298\n",
      "gradient descent loss 0.06344533766998707\n",
      "gradient descent loss 0.0632713404764991\n",
      "gradient descent loss 0.06315650354940941\n",
      "gradient descent loss 0.06307146777140307\n",
      "gradient descent loss 0.06300393678685029\n",
      "gradient descent loss 0.06294800642994507\n",
      "gradient descent loss 0.062900475767987\n",
      "gradient descent loss 0.06285941824003374\n",
      "gradient descent loss 0.06282356798434088\n",
      "gradient descent loss 0.06279203153114372\n",
      "gradient descent loss 0.06276414162843502\n",
      "gradient descent loss 0.06273937805304125\n",
      "gradient descent loss 0.06271732202869025\n",
      "gradient descent loss 0.06269762842948537\n",
      "gradient descent loss 0.06268000786725239\n",
      "gradient descent loss 0.0626642145401662\n",
      "[0.05848122 0.17219089 0.09329759 ... 0.54380012 0.54811636 0.08759498]\n",
      "[0 0 0 ... 1 1 0]\n",
      "[0 0 0 ... 1 1 1]\n",
      "0.831451677476179\n",
      "gradient descent loss 0.12732537966744067\n",
      "gradient descent loss 6.540558023217673e+35\n",
      "gradient descent loss 3.579420822090342e+74\n",
      "gradient descent loss 1.9588930143478322e+113\n",
      "gradient descent loss 1.0720342849822707e+152\n",
      "gradient descent loss 5.866872257748444e+190\n",
      "gradient descent loss 3.210735941090313e+229\n",
      "gradient descent loss 1.7571245512963572e+268\n",
      "gradient descent loss inf\n",
      "gradient descent loss inf\n",
      "gradient descent loss inf\n",
      "gradient descent loss inf\n",
      "gradient descent loss inf\n",
      "gradient descent loss inf\n",
      "gradient descent loss inf\n",
      "gradient descent loss inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-fa4e2ee648d3>:4: RuntimeWarning: invalid value encountered in matmul\n",
      "  gradient = -(1/N) * (tx.T) @ (e)\n",
      "<ipython-input-25-fb8f159f0002>:8: RuntimeWarning: invalid value encountered in subtract\n",
      "  w = w - gamma * gradient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "[nan nan nan ... nan nan nan]\n",
      "[1 1 1 ... 1 1 1]\n",
      "[0 0 0 ... 1 0 0]\n",
      "0.25658579550004\n",
      "gradient descent loss 0.12689166466490512\n",
      "gradient descent loss 2.288521501703905e+39\n",
      "gradient descent loss 4.368213387358128e+81\n",
      "gradient descent loss 8.337823430229431e+123\n",
      "gradient descent loss 1.5914813080076442e+166\n",
      "gradient descent loss 3.037738535640932e+208\n",
      "gradient descent loss 5.798280736622684e+250\n",
      "gradient descent loss 1.1067463215232017e+293\n",
      "gradient descent loss inf\n",
      "gradient descent loss inf\n",
      "gradient descent loss inf\n",
      "gradient descent loss inf\n",
      "gradient descent loss inf\n",
      "gradient descent loss inf\n",
      "gradient descent loss inf\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "[nan nan nan ... nan nan nan]\n",
      "[1 1 1 ... 1 1 1]\n",
      "[1 0 0 ... 1 0 0]\n",
      "0.2591880855152534\n",
      "gradient descent loss 0.12839298582752823\n",
      "gradient descent loss 1.4520980615503875e+41\n",
      "gradient descent loss 1.841774066522682e+85\n",
      "gradient descent loss 2.3360211007331018e+129\n",
      "gradient descent loss 2.962901195244347e+173\n",
      "gradient descent loss 3.758006933253084e+217\n",
      "gradient descent loss 4.766482302226338e+261\n",
      "gradient descent loss inf\n",
      "gradient descent loss inf\n",
      "gradient descent loss inf\n",
      "gradient descent loss inf\n",
      "gradient descent loss inf\n",
      "gradient descent loss inf\n",
      "gradient descent loss inf\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "[nan nan nan ... nan nan nan]\n",
      "[1 1 1 ... 1 1 1]\n",
      "[0 0 0 ... 1 0 1]\n",
      "0.25018015853951475\n",
      "gradient descent loss 0.12765900659246804\n",
      "gradient descent loss 2.056074709240244e+41\n",
      "gradient descent loss 3.63176175962743e+85\n",
      "gradient descent loss 6.41498746101729e+129\n",
      "gradient descent loss 1.1331157396524385e+174\n",
      "gradient descent loss 2.0014868107695776e+218\n",
      "gradient descent loss 3.535340048239901e+262\n",
      "gradient descent loss inf\n",
      "gradient descent loss inf\n",
      "gradient descent loss inf\n",
      "gradient descent loss inf\n",
      "gradient descent loss inf\n",
      "gradient descent loss inf\n",
      "gradient descent loss inf\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "[nan nan nan ... nan nan nan]\n",
      "[1 1 1 ... 1 1 1]\n",
      "[0 0 0 ... 1 1 1]\n",
      "0.2545840339498759\n",
      "gradient descent loss 0.12732537966744067\n",
      "gradient descent loss inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-5e9d2c03dbe7>:3: RuntimeWarning: overflow encountered in matmul\n",
      "  e = y - tx @ w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "[nan nan nan ... nan nan nan]\n",
      "[1 1 1 ... 1 1 1]\n",
      "[0 0 0 ... 1 0 0]\n",
      "0.25658579550004\n",
      "gradient descent loss 0.12689166466490512\n",
      "gradient descent loss inf\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "[nan nan nan ... nan nan nan]\n",
      "[1 1 1 ... 1 1 1]\n",
      "[1 0 0 ... 1 0 0]\n",
      "0.2591880855152534\n",
      "gradient descent loss 0.12839298582752823\n",
      "gradient descent loss inf\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "[nan nan nan ... nan nan nan]\n",
      "[1 1 1 ... 1 1 1]\n",
      "[0 0 0 ... 1 0 1]\n",
      "0.25018015853951475\n",
      "gradient descent loss 0.12765900659246804\n",
      "gradient descent loss inf\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "gradient descent loss nan\n",
      "[nan nan nan ... nan nan nan]\n",
      "[1 1 1 ... 1 1 1]\n",
      "[0 0 0 ... 1 1 1]\n",
      "0.2545840339498759\n",
      "[0.8183001  0.82694771 0.25513452 0.25513452]\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "degrees = np.arange(1, 5)\n",
    "k_fold = 4\n",
    "seed = 1\n",
    "testing_acc = np.zeros(len(degrees))\n",
    "k_indices = build_k_indices(y_0, k_fold, seed)\n",
    "for index in range(len(degrees)):\n",
    "    current_sum_test = 0\n",
    "    for k in range(k_fold):\n",
    "        current_test_acc = cross_validation_grad_desc(y_0, tX_tilda_0, k_indices, k, degrees[index], gamma = 5*10e-4)\n",
    "        print(current_test_acc)\n",
    "        current_sum_test += current_test_acc\n",
    "    testing_acc[index] = current_sum_test / k_fold\n",
    "best_result = np.where(testing_acc == np.amax(testing_acc))\n",
    "print(testing_acc)\n",
    "degree_opt = degrees[best_result[0]]\n",
    "print(degree_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "degree=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = np.arange(1, 5)\n",
    "k_fold = 4\n",
    "seed = 1\n",
    "testing_acc = np.zeros(len(degrees))\n",
    "k_indices = build_k_indices(y_1, k_fold, seed)\n",
    "for index in range(len(degrees)):\n",
    "    current_sum_test = 0\n",
    "    for k in range(k_fold):\n",
    "        current_test_acc = cross_validation_grad_desc(y_1, tX_tilda_1, k_indices, k, degrees[index], gamma = 5*10e-4)\n",
    "        print(current_test_acc)\n",
    "        current_sum_test += current_test_acc\n",
    "    testing_acc[index] = current_sum_test / k_fold\n",
    "best_result = np.where(testing_acc == np.amax(testing_acc))\n",
    "print(testing_acc)\n",
    "degree_opt = degrees[best_result[0]]\n",
    "print(degree_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained on colab \n",
    "#[0.73121067 0.77632054 0.7937687  0.35734551]\n",
    "# [3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient descent loss 0.2236201224107192\n",
      "gradient descent loss 0.10262907520410727\n",
      "gradient descent loss 0.08726719153685297\n",
      "gradient descent loss 0.0846866825602424\n",
      "gradient descent loss 0.08398919737630413\n",
      "gradient descent loss 0.0836628176426034\n",
      "gradient descent loss 0.0834558852571109\n",
      "gradient descent loss 0.08330955837237403\n",
      "gradient descent loss 0.083201796179067\n",
      "gradient descent loss 0.08312084167429568\n",
      "gradient descent loss 0.08305926949627276\n",
      "gradient descent loss 0.08301202536538124\n",
      "gradient descent loss 0.08297553119102864\n",
      "gradient descent loss 0.08294719051001828\n",
      "gradient descent loss 0.08292508594823815\n",
      "gradient descent loss 0.08290778282908313\n",
      "gradient descent loss 0.08289419654678115\n",
      "gradient descent loss 0.08288350037316669\n",
      "gradient descent loss 0.08287505991832408\n",
      "gradient descent loss 0.08286838569533451\n",
      "[ 0.00424628  0.09774473  0.34984029 ... -0.05067441  0.28867761\n",
      "  0.22763769]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[0 0 0 ... 0 0 0]\n",
      "0.7666390956713537\n",
      "gradient descent loss 0.2233352325987465\n",
      "gradient descent loss 0.10233871481486918\n",
      "gradient descent loss 0.08693962323919098\n",
      "gradient descent loss 0.08435304149124782\n",
      "gradient descent loss 0.08365687241798632\n",
      "gradient descent loss 0.08333314095165473\n",
      "gradient descent loss 0.08312877369634507\n",
      "gradient descent loss 0.08298459595577008\n",
      "gradient descent loss 0.08287853553628581\n",
      "gradient descent loss 0.08279889437573185\n",
      "gradient descent loss 0.08273832469691962\n",
      "gradient descent loss 0.0826918435102213\n",
      "gradient descent loss 0.0826559308886713\n",
      "gradient descent loss 0.08262803536949107\n",
      "gradient descent loss 0.08260627351282297\n",
      "gradient descent loss 0.0825892359279545\n",
      "gradient descent loss 0.08257585676987013\n",
      "gradient descent loss 0.08256532323367363\n",
      "gradient descent loss 0.082557011295544\n",
      "gradient descent loss 0.08255043922586944\n",
      "[0.3827699  0.07645504 0.47344514 ... 0.47457814 0.21247257 0.2644237 ]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[0 0 0 ... 0 0 0]\n",
      "0.7638819961400607\n",
      "gradient descent loss 0.22350984248350397\n",
      "gradient descent loss 0.10219162770895798\n",
      "gradient descent loss 0.08695109046301701\n",
      "gradient descent loss 0.08442324119290917\n",
      "gradient descent loss 0.0837541580015589\n",
      "gradient descent loss 0.08344564767771817\n",
      "gradient descent loss 0.0832507444197057\n",
      "gradient descent loss 0.08311283319726205\n",
      "gradient descent loss 0.08301111205723168\n",
      "gradient descent loss 0.08293457283766621\n",
      "gradient descent loss 0.08287626940097961\n",
      "gradient descent loss 0.08283146755272425\n",
      "gradient descent loss 0.08279681009830482\n",
      "gradient descent loss 0.0827698569365892\n",
      "gradient descent loss 0.08274880360343206\n",
      "gradient descent loss 0.08273229824357531\n",
      "gradient descent loss 0.08271931774933096\n",
      "gradient descent loss 0.08270908148785293\n",
      "gradient descent loss 0.08270098986431454\n",
      "gradient descent loss 0.08269457979318355\n",
      "[0.20588852 0.36977736 0.43556507 ... 0.85280522 0.07115881 0.91159563]\n",
      "[0 0 0 ... 1 0 1]\n",
      "[0 0 1 ... 1 0 1]\n",
      "0.7624483043837883\n",
      "gradient descent loss 0.22453912180417962\n",
      "gradient descent loss 0.10290605098542457\n",
      "gradient descent loss 0.08740456734654126\n",
      "gradient descent loss 0.08478303169187913\n",
      "gradient descent loss 0.08406604821090961\n",
      "gradient descent loss 0.08372734232207552\n",
      "gradient descent loss 0.08351191225709846\n",
      "gradient descent loss 0.08335959929794522\n",
      "gradient descent loss 0.08324757771054467\n",
      "gradient descent loss 0.08316356329879766\n",
      "gradient descent loss 0.0830997714986486\n",
      "gradient descent loss 0.0830509027845506\n",
      "gradient descent loss 0.08301321041818017\n",
      "gradient descent loss 0.08298398057953682\n",
      "gradient descent loss 0.08296121309268337\n",
      "gradient descent loss 0.08294341405153009\n",
      "gradient descent loss 0.0829294559501251\n",
      "gradient descent loss 0.08291848065260779\n",
      "gradient descent loss 0.08290983057105528\n",
      "gradient descent loss 0.08290299896302675\n",
      "[0.12178354 0.53731274 0.06464239 ... 0.36045942 0.08522428 0.39676587]\n",
      "[0 1 0 ... 0 0 0]\n",
      "[0 0 0 ... 0 0 1]\n",
      "0.7703336090432865\n",
      "gradient descent loss 0.2236201224107192\n",
      "gradient descent loss 0.08543679722034518\n",
      "gradient descent loss 0.07956229926375837\n",
      "gradient descent loss 0.07756856944962981\n",
      "gradient descent loss 0.07663793815477361\n",
      "gradient descent loss 0.07608916044264899\n",
      "gradient descent loss 0.07571193312513705\n",
      "gradient descent loss 0.07542776251481054\n",
      "gradient descent loss 0.07520194824384833\n",
      "gradient descent loss 0.07501668107081097\n",
      "gradient descent loss 0.07486159436806934\n",
      "gradient descent loss 0.0747300139722309\n",
      "gradient descent loss 0.07461730038050372\n",
      "gradient descent loss 0.0745200421934514\n",
      "gradient descent loss 0.07443562820860584\n",
      "gradient descent loss 0.07436200179180923\n",
      "gradient descent loss 0.07429750954612932\n",
      "gradient descent loss 0.07424080215022437\n",
      "gradient descent loss 0.07419076592741132\n",
      "gradient descent loss 0.07414647359058671\n",
      "[ 0.17868665  0.06181664  0.27616401 ... -0.16524553  0.43005604\n",
      "  0.18287787]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[0 0 0 ... 0 0 0]\n",
      "0.8056244830438379\n",
      "gradient descent loss 0.2233352325987465\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-89b9c0116c80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mcurrent_sum_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mcurrent_test_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation_grad_desc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_2_3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX_tilda_2_3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegrees\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m10e-4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_test_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mcurrent_sum_test\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcurrent_test_acc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-7988bfe99f2f>\u001b[0m in \u001b[0;36mcross_validation_grad_desc\u001b[1;34m(y, x, k_indices, k, degree, gamma)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mx_training_augmented\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_training\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mx_testing_augmented\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_testing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mws\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mleast_squares_GD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_training\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_training_augmented\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_training_augmented\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mw_opt_training\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mws\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m#loss_tr = compute_loss(y_training, x_training_augmented, w_opt_training)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-fb8f159f0002>\u001b[0m in \u001b[0;36mleast_squares_GD\u001b[1;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mws\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-fa4e2ee648d3>\u001b[0m in \u001b[0;36mcompute_gradient\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtx\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m@\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "degrees = np.arange(1, 5)\n",
    "k_fold = 4\n",
    "seed = 1\n",
    "testing_acc = np.zeros(len(degrees))\n",
    "k_indices = build_k_indices(y_2_3, k_fold, seed)\n",
    "for index in range(len(degrees)):\n",
    "    current_sum_test = 0\n",
    "    for k in range(k_fold):\n",
    "        current_test_acc = cross_validation_grad_desc(y_2_3, tX_tilda_2_3, k_indices, k, degrees[index], gamma = 5*10e-4)\n",
    "        print(current_test_acc)\n",
    "        current_sum_test += current_test_acc\n",
    "    testing_acc[index] = current_sum_test / k_fold\n",
    "best_result = np.where(testing_acc == np.amax(testing_acc))\n",
    "print(testing_acc)\n",
    "degree_opt = degrees[best_result[0]]\n",
    "print(degree_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[0.76582575 0.8072236  0.81856907 0.44751861]\n",
    "#[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#degree = 5, lr = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters tuning for Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "degrees = np.arange(2, 7)\n",
    "lambdas = np.logspace(-5,0,15)\n",
    "k_fold = 5\n",
    "seed = 1\n",
    "training_loss = np.zeros((len(lambdas), len(degrees)))\n",
    "testing_loss = np.zeros((len(lambdas), len(degrees)))\n",
    "k_indices = build_k_indices(y_0, k_fold, seed)\n",
    "for index1 in range(len(lambdas)):\n",
    "    for index2 in range(len(degrees)):\n",
    "        train_loss = 0\n",
    "        test_loss = 0\n",
    "        for k in range(k_fold):\n",
    "            loss_tr, loss_te = cross_validation_ridge(y_0, tX_tilda_0, k_indices, k,\n",
    "                                                lambdas[index1], degrees[index2])\n",
    "            train_loss += loss_tr\n",
    "            test_loss += loss_te\n",
    "        training_loss[index1, index2] = train_loss / k_fold\n",
    "        testing_loss[index1, index2] = test_loss / k_fold\n",
    "best_result = np.where(testing_loss == np.amin(testing_loss))\n",
    "print(testing_loss)\n",
    "lambda_opt, degree_opt = lambdas[best_result[0]],degrees[best_result[1]]\n",
    "print(lambda_opt, degree_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = np.arange(2, 7)\n",
    "lambdas = np.logspace(-5,0,15)\n",
    "k_fold = 5\n",
    "seed = 1\n",
    "training_loss = np.zeros((len(lambdas), len(degrees)))\n",
    "testing_loss = np.zeros((len(lambdas), len(degrees)))\n",
    "k_indices = build_k_indices(y_1, k_fold, seed)\n",
    "for index1 in range(len(lambdas)):\n",
    "    for index2 in range(len(degrees)):\n",
    "        train_loss = 0\n",
    "        test_loss = 0\n",
    "        for k in range(k_fold):\n",
    "            loss_tr, loss_te = cross_validation_ridge(y_1, tX_tilda_1, k_indices, k, \n",
    "                                                lambdas[index1], degrees[index2])\n",
    "            train_loss += loss_tr\n",
    "            test_loss += loss_te\n",
    "        training_loss[index1, index2] = train_loss / k_fold\n",
    "        testing_loss[index1, index2] = test_loss / k_fold\n",
    "best_result = np.where(testing_loss == np.amin(testing_loss))\n",
    "print(testing_loss)\n",
    "lambda_opt, degree_opt = lambdas[best_result[0]], degrees[best_result[1]]\n",
    "print(lambda_opt, degree_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = np.arange(2, 7)\n",
    "lambdas = np.logspace(-5,0,15)\n",
    "k_fold = 5\n",
    "seed = 1\n",
    "training_loss = np.zeros((len(lambdas), len(degrees)))\n",
    "testing_loss = np.zeros((len(lambdas), len(degrees)))\n",
    "k_indices = build_k_indices(y_2_3, k_fold, seed)\n",
    "for index1 in range(len(lambdas)):\n",
    "    for index2 in range(len(degrees)):\n",
    "        train_loss = 0\n",
    "        test_loss = 0\n",
    "        for k in range(k_fold):\n",
    "            loss_tr, loss_te = cross_validation_ridge(y_2_3, tX_tilda_2_3, k_indices, k,\n",
    "                                            lambdas[index1], degrees[index2])\n",
    "            train_loss += loss_tr\n",
    "            test_loss += loss_te\n",
    "        training_loss[index1, index2] = train_loss / k_fold\n",
    "        testing_loss[index1, index2] = test_loss / k_fold\n",
    "best_result = np.where(testing_loss == np.amin(testing_loss))\n",
    "lambda_opt, degree_opt = lambdas[best_result[0]], degrees[best_result[1]]\n",
    "print(testing_loss)\n",
    "print(lambda_opt, degree_opt) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model for ridge regression with the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_tilda_0_augmented = build_poly(tX_tilda_0, degree = 6)\n",
    "losses_0, ws_0 = least_squares_GD(y_0, tX_tilda_0_augmented, np.zeros(tX_tilda_0_augmented.shape[1]) , 2000, 5*10e-4)\n",
    "w_opt_training = ws_0[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_tilda_1_augmented = build_poly(tX_tilda_1, degree=6)\n",
    "w_ridge_1 = ridge_regression(y_1, tX_tilda_1_augmented, lambda_= 5.17947468e-05)\n",
    "#print(w_ridge_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_tilda_2_3_augmented = build_poly(tX_tilda_2_3, degree=6)\n",
    "w_ridge_2_3 = ridge_regression(y_2_3, tX_tilda_2_3_augmented, lambda_= 0.00026827)\n",
    "#print(w_ridge_2_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model for gradient descend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient descent loss 0.12757098675847986\n",
      "gradient descent loss 0.06693729315965935\n",
      "gradient descent loss 0.06423988053222848\n",
      "gradient descent loss 0.06349600121068293\n",
      "gradient descent loss 0.06317784862147327\n",
      "gradient descent loss 0.06300341150092016\n",
      "gradient descent loss 0.06288906199676206\n",
      "gradient descent loss 0.0628047769813995\n",
      "gradient descent loss 0.0627380400167114\n",
      "gradient descent loss 0.06268287144006163\n",
      "gradient descent loss 0.06263604502702481\n",
      "gradient descent loss 0.06259562738149475\n",
      "gradient descent loss 0.06256035290731772\n",
      "gradient descent loss 0.06252933119056518\n",
      "gradient descent loss 0.06250189909053598\n",
      "gradient descent loss 0.062477540811954696\n",
      "gradient descent loss 0.06245584199224412\n",
      "gradient descent loss 0.06243646175207266\n",
      "gradient descent loss 0.06241911471243574\n",
      "gradient descent loss 0.06240355881388663\n"
     ]
    }
   ],
   "source": [
    "tX_tilda_0_augmented = build_poly(tX_tilda_0, degree = 2)\n",
    "losses_0, ws_0 = least_squares_GD(y_0, tX_tilda_0_augmented, np.zeros(tX_tilda_0_augmented.shape[1]) , 2000, 5*10e-4)\n",
    "w_opt_training_0 = ws_0[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient descent loss 0.17867275353347778\n",
      "gradient descent loss 0.08789943780458429\n",
      "gradient descent loss 0.08252041452103928\n",
      "gradient descent loss 0.0803449184060501\n",
      "gradient descent loss 0.07916736832316855\n",
      "gradient descent loss 0.07841798061295603\n",
      "gradient descent loss 0.07788462300948416\n",
      "gradient descent loss 0.07747528776071969\n",
      "gradient descent loss 0.07714512111338145\n",
      "gradient descent loss 0.07686991708105946\n",
      "gradient descent loss 0.07663541615435647\n",
      "gradient descent loss 0.07643255855078472\n",
      "gradient descent loss 0.0762551995663821\n",
      "gradient descent loss 0.07609893304282081\n",
      "gradient descent loss 0.07596045029452171\n",
      "gradient descent loss 0.0758371729669143\n",
      "gradient descent loss 0.07572703272459488\n",
      "gradient descent loss 0.07562833291373816\n",
      "gradient descent loss 0.07553965777832575\n",
      "gradient descent loss 0.07545981032904109\n"
     ]
    }
   ],
   "source": [
    "tX_tilda_1_augmented = build_poly(tX_tilda_1, degree = 3)\n",
    "losses_1, ws_1 = least_squares_GD(y_1, tX_tilda_1_augmented, np.zeros(tX_tilda_1_augmented.shape[1]) , 2000, 5*10e-4)\n",
    "w_opt_training_1= ws_1[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient descent loss 0.22376383662103855\n",
      "gradient descent loss 0.08451346904069876\n",
      "gradient descent loss 0.07782752986711347\n",
      "gradient descent loss 0.07478524180658218\n",
      "gradient descent loss 0.07306438656955777\n",
      "gradient descent loss 0.07197319267478261\n",
      "gradient descent loss 0.0712203720913397\n",
      "gradient descent loss 0.07066568518289455\n",
      "gradient descent loss 0.07023572564349713\n",
      "gradient descent loss 0.06988939848806114\n",
      "gradient descent loss 0.0696023115260533\n",
      "gradient descent loss 0.0693591987810031\n",
      "gradient descent loss 0.06915002612973116\n",
      "gradient descent loss 0.06896789250973727\n",
      "gradient descent loss 0.06880785188808104\n",
      "gradient descent loss 0.06866622732844295\n",
      "gradient descent loss 0.06854019747655961\n",
      "gradient descent loss 0.06842753869095734\n",
      "gradient descent loss 0.0683264587989534\n",
      "gradient descent loss 0.06823548639681055\n"
     ]
    }
   ],
   "source": [
    "tX_tilda_2_3_augmented = build_poly(tX_tilda_2_3, degree=3)\n",
    "losses_2_3, ws_2_3 = least_squares_GD(y_2_3, tX_tilda_2_3_augmented, np.zeros(tX_tilda_2_3_augmented.shape[1]) , 2000, 5*10e-4)\n",
    "w_opt_training_2_3 = ws_2_3[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_logistic(y, x, k_indices, k, lambda_, degree, gamma):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    N = y.shape[0]\n",
    "    k_fold = k_indices.shape[0]\n",
    "    list_ = []\n",
    "    interval = int(N/k_fold)\n",
    "    for i in range(k_fold):\n",
    "        if i != k:\n",
    "            list_.append(i)\n",
    "    x_training = np.zeros((int((k_fold-1)/k_fold*N), x.shape[1]))\n",
    "    y_training = np.zeros(int((k_fold-1)/k_fold*N))\n",
    "    for j in range(len(list_)):\n",
    "        x_training[interval*(j):interval*(j+1), :] = x[np.array([k_indices[list_[j]]]), :]\n",
    "    x_testing = x[k_indices[k], :]\n",
    "    for j in range(len(list_)):\n",
    "        y_training[interval*(j):interval*(j+1)] = y[np.array([k_indices[list_[j]]])]\n",
    "    y_testing = y[k_indices[k]]\n",
    "    x_training_augmented = build_poly(x_training, degree)\n",
    "    x_testing_augmented = build_poly(x_testing, degree)\n",
    "    #w_opt_training = ridge_regression(y_training, x_training_augmented, lambda_)\n",
    "    _,  w_opt_training = learning_by_penalized_gradient(y_training, x_training_augmented,\n",
    "                                                        np.zeros(x_training_augmented.shape[1]), gamma, 10, lambda_)\n",
    "    loss_tr = compute_loss(y_training, x_training_augmented, w_opt_training) + lambda_*np.linalg.norm(w_opt_training) ** 2\n",
    "    loss_te = compute_loss(y_testing, x_testing_augmented, w_opt_training) + lambda_*np.linalg.norm(w_opt_training) ** 2\n",
    "    return loss_tr, loss_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform cross validation in order to find the best parameters degree, lamdba and gamma caracterizing logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = np.arange(2, 6)\n",
    "lambdas = np.arange(2, 6) / 10\n",
    "gammas = np.logspace(0.01, 10, 10)\n",
    "k_fold = 3\n",
    "seed = 1\n",
    "training_loss = np.zeros((len(lambdas), len(degrees), len(gammas)))\n",
    "testing_loss = np.zeros((len(lambdas), len(degrees), len(gammas)))\n",
    "k_indices = build_k_indices(y_0, k_fold, seed)\n",
    "for index1 in range(len(lambdas)):\n",
    "    for index2 in range(len(degrees)):\n",
    "        for index3 in range(len(gammas)):\n",
    "            train_loss = 0\n",
    "            test_loss = 0\n",
    "            for k in range(k_fold):\n",
    "                loss_tr, loss_te = cross_validation_logistic(y_0, tX_tilda_0, k_indices, k,\n",
    "                                                lambdas[index1], degrees[index2], gammas[index3])\n",
    "                train_loss += loss_tr\n",
    "                test_loss += loss_te\n",
    "            training_loss[index1, index2, index3] = train_loss / k_fold\n",
    "            testing_loss[index1, index2, index3] = test_loss / k_fold\n",
    "best_result = np.where(testing_loss == np.amin(testing_loss))\n",
    "lambda_opt, degree_opt, gamma_opt = lambdas[best_result[0]],degrees[best_result[1]], gammas[best_result[2]]\n",
    "print(lambda_opt, degree_opt, gamma_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = np.arange(2, 6)\n",
    "lambdas = np.arange(2, 6) / 10\n",
    "gammas = np.logspace(0.01, 10, 10)\n",
    "k_fold = 3\n",
    "seed = 1\n",
    "training_loss = np.zeros((len(lambdas), len(degrees), len(gammas)))\n",
    "testing_loss = np.zeros((len(lambdas), len(degrees), len(gammas)))\n",
    "k_indices = build_k_indices(y_0, k_fold, seed)\n",
    "for index1 in range(len(lambdas)):\n",
    "    for index2 in range(len(degrees)):\n",
    "        for index3 in range(len(gammas)):\n",
    "            train_loss = 0\n",
    "            test_loss = 0\n",
    "            for k in range(k_fold):\n",
    "                loss_tr, loss_te = cross_validation_logistic(y_0, tX_tilda_1, k_indices, k,\n",
    "                                                lambdas[index1], degrees[index2], gammas[index3])\n",
    "                train_loss += loss_tr\n",
    "                test_loss += loss_te\n",
    "            training_loss[index1, index2, index3] = train_loss / k_fold\n",
    "            testing_loss[index1, index2, index3] = test_loss / k_fold\n",
    "best_result = np.where(testing_loss == np.amin(testing_loss))\n",
    "lambda_opt, degree_opt, gamma_opt = lambdas[best_result[0]],degrees[best_result[1]], gammas[best_result[2]]\n",
    "print(lambda_opt, degree_opt, gamma_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = np.arange(2, 6)\n",
    "lambdas = np.arange(2, 6) / 10\n",
    "gammas = np.logspace(0.01, 10, 10)\n",
    "k_fold = 3\n",
    "seed = 1\n",
    "training_loss = np.zeros((len(lambdas), len(degrees), len(gammas)))\n",
    "testing_loss = np.zeros((len(lambdas), len(degrees), len(gammas)))\n",
    "k_indices = build_k_indices(y_0, k_fold, seed)\n",
    "for index1 in range(len(lambdas)):\n",
    "    for index2 in range(len(degrees)):\n",
    "        for index3 in range(len(gammas)):\n",
    "            train_loss = 0\n",
    "            test_loss = 0\n",
    "            for k in range(k_fold):\n",
    "                loss_tr, loss_te = cross_validation_logistic(y_0, tX_tilda_2_3, k_indices, k,\n",
    "                                                lambdas[index1], degrees[index2], gammas[index3])\n",
    "                train_loss += loss_tr\n",
    "                test_loss += loss_te\n",
    "            training_loss[index1, index2, index3] = train_loss / k_fold\n",
    "            testing_loss[index1, index2, index3] = test_loss / k_fold\n",
    "best_result = np.where(testing_loss == np.amin(testing_loss))\n",
    "lambda_opt, degree_opt, gamma_opt = lambdas[best_result[0]],degrees[best_result[1]], gammas[best_result[2]]\n",
    "print(lambda_opt, degree_opt, gamma_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238, 30)\n"
     ]
    }
   ],
   "source": [
    "print(tX_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now format the tX_test as we did for tX_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we split the test into the three subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_indices = []\n",
    "one_indices = []\n",
    "two_three_indices = []\n",
    "zero_indices = np.where(tX_test[:,22]==0)[0]\n",
    "one_indices = np.where(tX_test[:,22]==1)[0]\n",
    "two_three_indices = np.where(np.logical_or(tX_test[:,22]==2, tX_test[:,22]==3))[0]\n",
    "tX_test_0 = tX_test[zero_indices, :]\n",
    "tX_test_0 = np.delete(tX_test_0, 22, axis=1)\n",
    "tX_test_1 = tX_test[one_indices, :]\n",
    "tX_test_1 = np.delete(tX_test_1, 22, axis=1)\n",
    "tX_test_2_3 = tX_test[two_three_indices, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a column of zeros and ones to detect whether the mass has been measured or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the indices where the mass is not calculated, add the column which has 0 in those indices\n",
    "# and 1 everywhere else for all matrices 0,1,2_3\n",
    "zero_indices_0 = np.where(tX_test_0[:,1] == -999.)[0]\n",
    "column_to_add = np.array([0 if i in zero_indices_0 else 1 for i in range(tX_test_0.shape[0])])\n",
    "tX_test_0 = np.insert(tX_test_0, 0, column_to_add, axis=1)\n",
    "zero_indices_1 = np.where(tX_test_1[:,1] == -999.)[0]\n",
    "column_to_add = np.array([0 if i in zero_indices_1 else 1 for i in range(tX_test_1.shape[0])])\n",
    "tX_test_1 = np.insert(tX_test_1, 0, column_to_add, axis=1)\n",
    "zero_indices_2_3 = np.where(tX_test_2_3[:,1] == -999.)[0]\n",
    "column_to_add = np.array([0 if i in zero_indices_2_3 else 1 for i in range(tX_test_2_3.shape[0])])\n",
    "tX_test_2_3 = np.insert(tX_test_2_3, 0, column_to_add, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We drop the same columns we have dropped for the X training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test_0 = np.delete(tX_test_0, col_to_delete_0, axis=1)\n",
    "tX_test_1 = np.delete(tX_test_1, col_to_delete_1, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we substitute the -999 values with the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, tX_test_2_3.shape[1]):\n",
    "    index_column_non_valid =np.where(tX_test_2_3[:,i] == -999.)[0]\n",
    "    index_column_valid =np.where(tX_test_2_3[:,i] != -999.)[0]\n",
    "    median = np.median(tX_test_2_3[index_column_valid, i], axis = 0)\n",
    "    tX_test_2_3[index_column_non_valid,i] =  median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, tX_test_1.shape[1]):\n",
    "    index_column_non_valid =np.where(tX_test_1[:,i] == -999.)[0]\n",
    "    index_column_valid =np.where(tX_test_1[:,i] != -999.)[0]\n",
    "    median = np.median(tX_test_1[index_column_valid, i], axis = 0)\n",
    "    tX_test_1[index_column_non_valid,i] =  median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, tX_test_0.shape[1]):\n",
    "    index_column_non_valid =np.where(tX_test_0[:,i] == -999.)[0]\n",
    "    index_column_valid =np.where(tX_test_0[:,i] != -999.)[0]\n",
    "    median = np.median(tX_test_0[index_column_valid, i], axis = 0)\n",
    "    tX_test_0[index_column_non_valid,i] =  median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We standardize the test set using the mean and the standard deviation of the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227458, 19)\n"
     ]
    }
   ],
   "source": [
    "print(tX_test_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99913, 19)\n"
     ]
    }
   ],
   "source": [
    "print(tX_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_test(x, mean, std):\n",
    "    \"\"\"Standardize the test set.\"\"\"\n",
    "    x = x - mean\n",
    "    x = x / std\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test_0[:,1:] = standardize_test(tX_test_0[:,1:], mean_0, std_0)\n",
    "tX_test_1[:,1:] = standardize_test(tX_test_1[:,1:], mean_1, std_1)\n",
    "tX_test_2_3[:,1:]= standardize_test(tX_test_2_3[:,1:], mean_2_3, std_2_3) #we standardize everything a part from the column added manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We insert the column for the bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_tilda_test_0 = np.insert(tX_test_0, 0, np.ones(tX_test_0.shape[0]), axis=1)\n",
    "tX_tilda_test_1 = np.insert(tX_test_1, 0, np.ones(tX_test_1.shape[0]), axis=1)\n",
    "tX_tilda_test_2_3 = np.insert(tX_test_2_3, 0, np.ones(tX_test_2_3.shape[0]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We make the predictions ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_tilda_test_2_3_augmented = build_poly(tX_tilda_test_2_3, degree=6)\n",
    "predictions_2_3 = tX_tilda_test_2_3_augmented @ w_ridge_2_3\n",
    "print(predictions_2_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_tilda_test_0_augmented = build_poly(tX_tilda_test_0, degree = 6)\n",
    "predictions_0 = tX_tilda_test_0_augmented @ w_ridge_0\n",
    "print(predictions_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_tilda_test_1_augmented = build_poly(tX_tilda_test_1, degree=6)\n",
    "predictions_1 = tX_tilda_test_1_augmented @ w_ridge_1\n",
    "print(predictions_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(zero_indices))\n",
    "print(len(one_indices))\n",
    "print(len(two_three_indices))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to reconstruct a single vector of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_predictions = []\n",
    "count_0 =0\n",
    "count_1 =0\n",
    "count_2_3 =0\n",
    "for index_row in range(tX_test.shape[0]):\n",
    "    if index_row in zero_indices:\n",
    "        stacked_predictions.append(predictions_0[count_0])\n",
    "        count_0 = count_0 + 1\n",
    "    elif index_row in one_indices:\n",
    "        stacked_predictions.append(predictions_1[count_1])\n",
    "        count_1 = count_1 +1\n",
    "    else:\n",
    "        stacked_predictions.append(predictions_2_3[count_2_3])\n",
    "        count_2_3 = count_2_3 +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = np.array([-1 if el <0.5 else 1 for el in stacked_predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we make predictions GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(165442,)\n",
      "(227458,)\n",
      "(175338,)\n"
     ]
    }
   ],
   "source": [
    "tX_tilda_test_2_3_augmented = build_poly(tX_tilda_test_2_3, degree=3)\n",
    "predictions_2_3 = tX_tilda_test_2_3_augmented @ w_opt_training_2_3\n",
    "print(predictions_2_3.shape)\n",
    "tX_tilda_test_0_augmented = build_poly(tX_tilda_test_0, degree = 2)\n",
    "predictions_0 = tX_tilda_test_0_augmented @ w_opt_training_0\n",
    "print(predictions_0.shape)\n",
    "tX_tilda_test_1_augmented = build_poly(tX_tilda_test_1, degree=3)\n",
    "predictions_1 = tX_tilda_test_1_augmented @ w_opt_training_1\n",
    "print(predictions_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_predictions = []\n",
    "count_0 =0\n",
    "count_1 =0\n",
    "count_2_3 =0\n",
    "for index_row in range(tX_test.shape[0]):\n",
    "    if index_row in zero_indices:\n",
    "        stacked_predictions.append(predictions_0[count_0])\n",
    "        count_0 = count_0 + 1\n",
    "    elif index_row in one_indices:\n",
    "        stacked_predictions.append(predictions_1[count_1])\n",
    "        count_1 = count_1 +1\n",
    "    else:\n",
    "        stacked_predictions.append(predictions_2_3[count_2_3])\n",
    "        count_2_3 = count_2_3 +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = np.array([-1 if el <0.5 else 1 for el in stacked_predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(weights, tX_test):\n",
    "    y = np.array(tX_test) @ np.array(weights)\n",
    "    labels = [1 if l > 0 else -1 for l in y]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'submission_GD.csv' # TODO: fill in desired name of output file for submission\n",
    "#y_pred = predict_labels(weights, tX_test)\n",
    "y_pred = final_predictions\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1 -1 ...  1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "print(final_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
