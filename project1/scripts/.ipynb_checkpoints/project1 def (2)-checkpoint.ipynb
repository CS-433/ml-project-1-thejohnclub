{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # train data path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n"
     ]
    }
   ],
   "source": [
    "print(tX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the labels to {0,1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y == 0 non detected Boson, y == 1 detected Boson\n",
    "y_ = np.array([0 if l == -1 else 1 for l in y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the features by the number of jets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing the rows of tX by the number of jets, dropping the column Pri_Jet_Num and adding an extra column of np.ones\n",
    "zero_indices = []\n",
    "one_indices = []\n",
    "two_three_indices = []\n",
    "zero_indices = np.where(tX[:,22]==0)[0]\n",
    "one_indices = np.where(tX[:,22]==1)[0]\n",
    "two_three_indices = np.where(np.logical_or(tX[:,22]==2, tX[:,22]==3))[0]\n",
    "tX_0 = tX[zero_indices, :]\n",
    "tX_0 = np.delete(tX_0, 22, axis=1)\n",
    "tX_1 = tX[one_indices, :]\n",
    "tX_1 = np.delete(tX_1, 22, axis=1)\n",
    "tX_2_3 = tX[two_three_indices, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing also the output by the type of particle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_0 = y_[zero_indices]\n",
    "y_1 = y_[one_indices]\n",
    "y_2_3 = y_[two_three_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a column of zeros and ones to detect whether the mass has been measured or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the indices where the mass is not calculated, add the column which has 0 in those indices\n",
    "# and 1 everywhere else for all matrices 0,1,2_3\n",
    "zero_indices_0 = np.where(tX_0[:,1] == -999.)[0]\n",
    "column_to_add = np.array([0 if i in zero_indices_0 else 1 for i in range(tX_0.shape[0])])\n",
    "tX_0 = np.insert(tX_0, 0, column_to_add, axis=1)\n",
    "zero_indices_1 = np.where(tX_1[:,1] == -999.)[0]\n",
    "column_to_add = np.array([0 if i in zero_indices_1 else 1 for i in range(tX_1.shape[0])])\n",
    "tX_1 = np.insert(tX_1, 0, column_to_add, axis=1)\n",
    "zero_indices_2_3 = np.where(tX_2_3[:,1] == -999.)[0]\n",
    "column_to_add = np.array([0 if i in zero_indices_2_3 else 1 for i in range(tX_2_3.shape[0])])\n",
    "tX_2_3 = np.insert(tX_2_3, 0, column_to_add, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Throwing away the outliers from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, tX_2_3.shape[1]):\n",
    "    index_column_valid =np.where(tX_2_3[:,i] != -999.)[0]\n",
    "    column_25_quantile, column_75_quantile = np.quantile(tX_2_3[index_column_valid,i], \n",
    "                                                         np.array([0.25, 0.75]))\n",
    "    interquantile = column_75_quantile-column_25_quantile\n",
    "    column_15_quantile, column_85_quantile = np.quantile(tX_2_3[index_column_valid,i], \n",
    "                                                         np.array([0.15, 0.85]))\n",
    "    indices_outliers = np.where((column_15_quantile - 1.5 * interquantile >= tX_2_3[index_column_valid,i])\n",
    "                                             | (tX_2_3[index_column_valid,i] >= \n",
    "                                                column_85_quantile + 1.5 * interquantile))[0]\n",
    "    #indices_outliers = np.argwhere((tX_tilda_2_3[index_column_valid,i] >= column_85_quantile + 1.5 * interquantile) | \n",
    "                                  #(column_15_quantile - 1.5 * interquantile >= tX_tilda_2_3[index_column_valid,i]))\n",
    "    median = np.median(tX_2_3[index_column_valid, i], axis = 0)\n",
    "    #print(np.sort(tX_tilda_2_3[index_column_valid[indices_outliers],i]).T)\n",
    "    #print(np.where(tX_tilda_2_3[indices_outliers,i])==-999.)\n",
    "    #print(median)\n",
    "    tX_2_3[index_column_valid[indices_outliers],i] =  median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99913, 19)\n",
      "[5, 6, 7, 13, 23, 24, 25, 26, 27, 28, 29]\n"
     ]
    }
   ],
   "source": [
    "col_to_delete_0 = []\n",
    "for i in range(1, tX_0.shape[1]):\n",
    "    index_column_valid =np.where(tX_0[:,i] != -999.)[0]\n",
    "    if len(index_column_valid)==0:\n",
    "        #we drop the column (we will have to do the same for the test set as well)\n",
    "        col_to_delete_0.append(i)\n",
    "    else :\n",
    "        column_25_quantile, column_75_quantile = np.quantile(tX_0[index_column_valid,i], \n",
    "                                                         np.array([0.25, 0.75]))\n",
    "        interquantile = column_75_quantile-column_25_quantile\n",
    "        column_15_quantile, column_85_quantile = np.quantile(tX_0[index_column_valid,i], \n",
    "                                                         np.array([0.15, 0.85]))\n",
    "        indices_outliers = np.where((column_15_quantile - 1.5 * interquantile >= tX_0[index_column_valid,i])\n",
    "                                             | (tX_0[index_column_valid,i] >= \n",
    "                                                column_85_quantile + 1.5 * interquantile))[0]\n",
    "        #indices_outliers = np.argwhere((tX_tilda_2_3[index_column_valid,i] >= column_85_quantile + 1.5 * interquantile) | \n",
    "                                  #(column_15_quantile - 1.5 * interquantile >= tX_tilda_2_3[index_column_valid,i]))\n",
    "        median = np.median(tX_0[index_column_valid, i], axis = 0)\n",
    "        #print(np.sort(tX_tilda_2_3[index_column_valid[indices_outliers],i]).T)\n",
    "        #print(np.where(tX_tilda_2_3[indices_outliers,i])==-999.)\n",
    "        #print(median)\n",
    "        tX_0[index_column_valid[indices_outliers],i] =  median\n",
    "col_to_delete_0.append(tX_0.shape[1]-1)\n",
    "tX_0 = np.delete(tX_0, col_to_delete_0, axis=1)\n",
    "print(tX_0.shape)\n",
    "print(col_to_delete_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 7, 13, 26, 27, 28]\n"
     ]
    }
   ],
   "source": [
    "col_to_delete_1 = []\n",
    "for i in range(1, tX_1.shape[1]):\n",
    "    index_column_valid =np.where(tX_1[:,i] != -999.)[0]\n",
    "    if len(index_column_valid)==0:\n",
    "        #we drop the column (we will have to do the same for the test set as well)\n",
    "        col_to_delete_1.append(i)\n",
    "    else :\n",
    "        column_25_quantile, column_75_quantile = np.quantile(tX_1[index_column_valid,i], \n",
    "                                                         np.array([0.25, 0.75]))\n",
    "        interquantile = column_75_quantile-column_25_quantile\n",
    "        column_15_quantile, column_85_quantile = np.quantile(tX_1[index_column_valid,i], \n",
    "                                                         np.array([0.15, 0.85]))\n",
    "        indices_outliers = np.where((column_15_quantile - 1.5 * interquantile >= tX_1[index_column_valid,i])\n",
    "                                             | (tX_1[index_column_valid,i] >= \n",
    "                                                column_85_quantile + 1.5 * interquantile))[0]\n",
    "        #indices_outliers = np.argwhere((tX_tilda_2_3[index_column_valid,i] >= column_85_quantile + 1.5 * interquantile) | \n",
    "                                  #(column_15_quantile - 1.5 * interquantile >= tX_tilda_2_3[index_column_valid,i]))\n",
    "        median = np.median(tX_1[index_column_valid, i], axis = 0)\n",
    "        #print(np.sort(tX_tilda_2_3[index_column_valid[indices_outliers],i]).T)\n",
    "        #print(np.where(tX_tilda_2_3[indices_outliers,i])==-999.)\n",
    "        #print(median)\n",
    "        tX_1[index_column_valid[indices_outliers],i] =  median\n",
    "tX_1 = np.delete(tX_1, col_to_delete_1, axis=1)\n",
    "print(col_to_delete_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we substitute the -999 values with the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, tX_2_3.shape[1]):\n",
    "    index_column_non_valid =np.where(tX_2_3[:,i] == -999.)[0]\n",
    "    index_column_valid =np.where(tX_2_3[:,i] != -999.)[0]\n",
    "    median = np.median(tX_2_3[index_column_valid, i], axis = 0)\n",
    "    tX_2_3[index_column_non_valid,i] =  median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, tX_1.shape[1]):\n",
    "    index_column_non_valid =np.where(tX_1[:,i] == -999.)[0]\n",
    "    index_column_valid =np.where(tX_1[:,i] != -999.)[0]\n",
    "    median = np.median(tX_1[index_column_valid, i], axis = 0)\n",
    "    tX_1[index_column_non_valid,i] =  median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, tX_0.shape[1]):\n",
    "    index_column_non_valid =np.where(tX_0[:,i] == -999.)[0]\n",
    "    index_column_valid =np.where(tX_0[:,i] != -999.)[0]\n",
    "    median = np.median(tX_0[index_column_valid, i], axis = 0)\n",
    "    tX_0[index_column_non_valid,i] =  median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tX_2_3[:,1:], mean_2_3,std_2_3 = standardize(tX_2_3[:,1:]) #we standardize everything a part from the column added manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.97351249  0.46588281 ...  0.61614788 -1.36131161\n",
      "  -0.70374641]\n",
      " [ 1.         -0.82851663 -0.77157038 ...  0.11608109  1.71034105\n",
      "   0.2995537 ]\n",
      " [ 1.          1.3538447  -0.27431587 ...  0.07030726 -1.52202162\n",
      "   0.12704911]\n",
      " ...\n",
      " [ 1.          0.04006392  0.02513449 ...  0.25930888  0.22982758\n",
      "   0.42357227]\n",
      " [ 1.          0.66304099 -1.0843679  ...  0.29031696 -1.21821366\n",
      "  -0.20699627]\n",
      " [ 1.          0.04006392  0.31977857 ... -0.02271698 -0.62490751\n",
      "   0.05569682]]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(tX_2_3)\n",
    "print(np.count_nonzero(tX_2_3 == -999.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000e+00  1.43905e+02  8.14170e+01 ...  3.10820e+01  6.00000e-02\n",
      "   8.60620e+01]\n",
      " [ 1.00000e+00  1.75864e+02  1.69150e+01 ...  2.72300e+00 -8.71000e-01\n",
      "   5.31310e+01]\n",
      " [ 1.00000e+00  1.05594e+02  5.05590e+01 ...  3.77910e+01  2.40000e-02\n",
      "   1.29804e+02]\n",
      " ...\n",
      " [ 1.00000e+00  1.11452e+02  5.81790e+01 ...  4.67370e+01 -8.67000e-01\n",
      "   8.04080e+01]\n",
      " [ 1.00000e+00  9.49510e+01  1.93620e+01 ...  1.21500e+01  8.11000e-01\n",
      "   1.12718e+02]\n",
      " [ 1.00000e+00  1.11452e+02  7.27560e+01 ...  4.07290e+01 -1.59600e+00\n",
      "   9.94050e+01]]\n"
     ]
    }
   ],
   "source": [
    "print(tX_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_0[:,1:],mean_0,std_0 = standardize(tX_0[:,1:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          1.05744907  0.7827665  ...  0.01825038  0.04662815\n",
      "  -0.7724943 ]\n",
      " [ 1.          2.14505538 -1.37519852 ... -1.71504715 -0.4674532\n",
      "  -1.44727052]\n",
      " [ 1.         -0.24632406 -0.2496121  ...  0.42830338  0.0267496\n",
      "   0.12380588]\n",
      " ...\n",
      " [ 1.         -0.0469687   0.00532098 ...  0.97508147 -0.46524447\n",
      "  -0.8883482 ]\n",
      " [ 1.         -0.60851918 -1.29333222 ... -1.13887042  0.46131675\n",
      "  -0.22629665]\n",
      " [ 1.         -0.0469687   0.49300595 ...  0.60787347 -0.86778508\n",
      "  -0.49908812]]\n"
     ]
    }
   ],
   "source": [
    "print(tX_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_1[:,1:],mean_1,std_1 = standardize(tX_1[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000000e+00  1.55539992e+00  7.27047143e-01 ...  3.98445313e-01\n",
      "   6.45414781e-01 -4.14297220e-01]\n",
      " [ 1.00000000e+00  1.45598722e-03  3.58462301e+00 ...  1.12748232e+00\n",
      "  -1.10752634e+00 -4.89864560e-01]\n",
      " [ 1.00000000e+00  1.36261180e+00 -1.05809645e+00 ... -3.92076740e-01\n",
      "  -9.40265168e-01 -1.01072441e+00]\n",
      " ...\n",
      " [ 1.00000000e+00  1.45598722e-03  1.01732036e+00 ... -4.67286130e-01\n",
      "  -3.80160315e-01  8.39087556e-01]\n",
      " [ 1.00000000e+00  6.75509966e-01  9.95415259e-01 ... -6.76994064e-01\n",
      "   1.39533906e+00  5.32418071e-01]\n",
      " [ 1.00000000e+00 -2.21030015e-01  4.74893699e-01 ...  9.88591985e-01\n",
      "  -8.30516498e-02 -5.76298292e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(tX_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We insert the column for the bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_tilda_0 = np.insert(tX_0, 0, np.ones(tX_0.shape[0]), axis=1)\n",
    "tX_tilda_1 = np.insert(tX_1, 0, np.ones(tX_1.shape[0]), axis=1)\n",
    "tX_tilda_2_3 = np.insert(tX_2_3, 0, np.ones(tX_2_3.shape[0]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          1.          1.05744907 ...  0.01825038  0.04662815\n",
      "  -0.7724943 ]\n",
      " [ 1.          1.          2.14505538 ... -1.71504715 -0.4674532\n",
      "  -1.44727052]\n",
      " [ 1.          1.         -0.24632406 ...  0.42830338  0.0267496\n",
      "   0.12380588]\n",
      " ...\n",
      " [ 1.          1.         -0.0469687  ...  0.97508147 -0.46524447\n",
      "  -0.8883482 ]\n",
      " [ 1.          1.         -0.60851918 ... -1.13887042  0.46131675\n",
      "  -0.22629665]\n",
      " [ 1.          1.         -0.0469687  ...  0.60787347 -0.86778508\n",
      "  -0.49908812]]\n"
     ]
    }
   ],
   "source": [
    "print(tX_tilda_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors = ['red', 'blue']\n",
    "# x_pos=[]\n",
    "# x_neg=[]\n",
    "\n",
    "# for j in range(len(y)):\n",
    "#  if(y[j]==1):\n",
    "#       x_pos.insert(0,tX[j])\n",
    "#    else:\n",
    "#        x_neg.insert(0,tX[j])\n",
    "# xpos = np.array(x_pos)\n",
    "# xneg = np.array(x_neg)\n",
    "# for i in range(tX.shape[1]):\n",
    "#  plt.hist(xpos[:,i], alpha = 0.5, color = 'r', bins = 100)\n",
    "#  plt.hist(xneg[:,i], alpha = 0.5, color = 'b', bins = 100)\n",
    "#  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    N = y.shape[0]\n",
    "    e = y - tx @ w \n",
    "    loss = 1/(2*N) * np.dot(e,e)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    N = y.shape[0]\n",
    "    e = y - tx @ w\n",
    "    gradient = -(1/N) * (tx.T) @ (e)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        gradient = compute_gradient(y,tx,w)\n",
    "        w = w - gamma * gradient\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        # print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "        # bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_GD(y, x, k_indices, k, degree, gamma = 3.0e-02):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    N = y.shape[0]\n",
    "    k_fold = k_indices.shape[0]\n",
    "    list_ = []\n",
    "    interval = int(N/k_fold)\n",
    "    for i in range(k_fold):\n",
    "        if i != k:\n",
    "            list_.append(i)\n",
    "    x_training = np.zeros((int((k_fold-1)/k_fold*N), x.shape[1]))\n",
    "    y_training = np.zeros(int((k_fold-1)/k_fold*N))\n",
    "    for j in range(len(list_)):\n",
    "        x_training[interval*(j):interval*(j+1), :] = x[np.array([k_indices[list_[j]]]), :]\n",
    "    x_testing = x[k_indices[k], :]\n",
    "    for j in range(len(list_)):\n",
    "        y_training[interval*(j):interval*(j+1)] = y[np.array([k_indices[list_[j]]])]\n",
    "    y_testing = y[k_indices[k]]\n",
    "    x_training_augmented = build_poly(x_training, degree)\n",
    "    x_testing_augmented = build_poly(x_testing, degree)\n",
    "    #w_opt_training = ridge_regression(y_training, x_training_augmented, lambda_)\n",
    "    _,  w_opt_training = least_squares_GD(y_training, x_training_augmented,\n",
    "                                                        np.zeros(x_training_augmented.shape[1]), 1000, gamma)\n",
<<<<<<< HEAD
    "    predictions_test = x_training_augmented @ w_opt_training[-1]\n",
    "    predictions_test = np.array([0 if el < 0.5 else 1 for el in predictions_test])\n",
    "    test_acc = compute_accuracy(y_testing, predictions_test)\n",
    "    return test_acc"
=======
    "    loss_tr = calculate_loss(y_training, x_training_augmented, w_opt_training[-1])\n",
    "    loss_te = calculate_loss(y_testing, x_testing_augmented, w_opt_training[-1])\n",
    "    return loss_tr, loss_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    N = y.shape[0]\n",
    "    random_number = random.randint(0,N)\n",
    "    #random_number =1\n",
    "    xn = tx[random_number,:]\n",
    "    random_gradient = - np.dot(xn, y[random_number] - np.dot(xn,w))\n",
    "    return random_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        stoch_gradient = compute_stoch_gradient(y,tx,w)\n",
    "        w = w - gamma * stoch_gradient\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        # print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "        #    bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "\n",
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    forcing_term = np.transpose(tx) @ y\n",
    "    coefficient_matrix = np.transpose(tx) @ tx\n",
    "    w = np.linalg.solve(coefficient_matrix, forcing_term)\n",
    "    return w\n",
    "\n",
    "def test_your_least_squares(y, tx):\n",
    "    \"\"\"compare the solution of the normal equations with the weights returned by gradient descent algorithm.\"\"\"\n",
    "    w_least_squares = least_squares(y, tx)\n",
    "    initial_w = np.zeros(tx.shape[1])\n",
    "    max_iters = 50\n",
    "    gamma = 0.7\n",
    "    losses_gradient_descent, w_gradient_descent = gradient_descent(y, tx, initial_w, max_iters, gamma)\n",
    "    w = w_gradient_descent[-1]\n",
    "    err = np.linalg.norm(w_least_squares-w)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    N = tx.shape\n",
    "    lambda_prime = 2 * N[0] * lambda_\n",
    "    coefficient_matrix = np.transpose(tx) @ tx + lambda_prime * np.eye(N[1])\n",
    "    forcing_term = np.transpose(tx) @ y\n",
    "    w = np.linalg.solve(coefficient_matrix, forcing_term)\n",
    "    return w\n",
    "\n",
    "def debug_ridge(y, tx):\n",
    "    \"\"\"debugging the ridge regression by setting lambda=0.\"\"\"\n",
    "    w_least_squares = least_squares(y, tx)\n",
    "    w_0 = ridge_regression(y, tx, 0)\n",
    "    err = np.linalg.norm(w_least_squares-w_0)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply the sigmoid function on t.\"\"\"\n",
    "    positive_indices = np.where(t >= 0)[0]\n",
    "    negative_indices = np.where(t < 0)[0]\n",
    "    z = np.zeros(len(t))\n",
    "    z[positive_indices] = 1 / (1+np.exp(-t[positive_indices]))\n",
    "    z[negative_indices] = np.exp(t[negative_indices]) / (1 + np.exp(t[negative_indices]))\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the loss: negative log likelihood.\"\"\"\n",
    "    epsilon = 1.0e-12\n",
    "    # term1 = sigmoid(tx @ w)\n",
    "    # term1[y == 0] = 1\n",
    "    # term2 = 1 - sigmoid(tx @ w)\n",
    "    # term2[y == 1] = 1\n",
    "    # summands = np.multiply(y, np.log(term1)) + np.multiply(1 - y, np.log(term2))\n",
    "    # e = - y * (tx @ w) + np.log(1 + np.exp(tx @ w))\n",
    "    # return e.sum()\n",
    "    pos_ind = np.where(tx @ w >=0)[0]\n",
    "    neg_ind = np.where(tx @ w <0)[0]\n",
    "    loss_pos = - y[pos_ind] * (tx @ w)[pos_ind] + (tx @ w)[pos_ind] + np.log(1+np.exp(-(tx @ w)[pos_ind]))\n",
    "    loss_neg = - y[neg_ind] * (tx @ w)[neg_ind] - (tx @ w)[neg_ind] + np.log(1+np.exp((tx @ w)[neg_ind]))\n",
    "    # loss = - np.sum(y*np.log(sigmoid(tx @ w)+epsilon) + ((1 - y) * np.log(1-sigmoid(tx@w) + epsilon)))\n",
    "    # return e.sum()\n",
    "    return loss_pos.sum() + loss_neg.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    return np.transpose(tx) @ (sigmoid(tx @ w) - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent(y, tx, w_initial, gamma, max_iters):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    w = w_initial\n",
    "    for iter in range(max_iters):\n",
    "        grad = calculate_gradient(y, tx, w)\n",
    "        w = w - gamma * grad\n",
    "        if iter %100 == 0:\n",
    "            gamma = gamma/2\n",
    "        loss = calculate_loss(y, tx, w)\n",
    "        losses.append(loss)\n",
    "    return losses, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sigmoid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-8b7f1be57c9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlosses1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_by_gradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX_tilda_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX_tilda_0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-70a52a7c3c07>\u001b[0m in \u001b[0;36mlearning_by_gradient_descent\u001b[1;34m(y, tx, w_initial, gamma, max_iters)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw_initial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-e30af4a131a3>\u001b[0m in \u001b[0;36mcalculate_gradient\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcalculate_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;34m\"\"\"compute the gradient of loss.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m@\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sigmoid' is not defined"
     ]
    }
   ],
   "source": [
    "losses1, w1 = learning_by_gradient_descent(y_0, tX_tilda_0, np.zeros(tX_tilda_0.shape[1]), 0.8, 10000)\n",
    "print(losses1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hessian(y, tx, w):\n",
    "    \"\"\"return the Hessian of the loss function.\"\"\"\n",
    "    diag = sigmoid(tx @ w) * (1 - sigmoid(tx @ w))\n",
    "    D = diag * np.eye(tx.shape[0])\n",
    "    return np.transpose(tx) @ D @ tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, w):\n",
    "    \"\"\"return the loss, gradient, and Hessian.\"\"\"\n",
    "    grad = calculate_gradient(y, tx, w)\n",
    "    hess = calculate_hessian(y, tx, w)\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    return loss, grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_newton_method(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step on Newton's method.\n",
    "    return the loss and updated w.\n",
    "    \"\"\"\n",
    "    loss, grad, hess = logistic_regression(y, tx, w)\n",
    "    sol = np.linalg.solve(hess, grad)\n",
    "    w = w - gamma * sol\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "    \"\"\"return the loss, gradient\"\"\"\n",
    "    loss = calculate_loss(y, tx, w) + lambda_*np.linalg.norm(w) ** 2\n",
    "    grad = calculate_gradient(y, tx, w) + 2*lambda_*w\n",
    "    hess = calculate_hessian(y, tx, w) + 2*lambda_*np.eye(w.shape[0])\n",
    "    return loss, grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_penalized_gradient(y, tx, w_initial, gamma, max_iters, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent, using the penalized logistic regression.\n",
    "    Return the loss and updated w.\n",
    "    \"\"\"\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "    drop = 0.5\n",
    "    iter_drop = 25\n",
    "    w = w_initial\n",
    "    for iter in range(max_iters):\n",
    "        grad = calculate_gradient(y, tx, w) + 2*lambda_*w\n",
    "        w = w - gamma * grad\n",
    "        # regularizer = lambda_ / 2 * np.linalg.norm(w) ** 2\n",
    "        # summing = np.sum(np.log(1+np.exp(tx.dot(w))))\n",
    "        # y_component = y.T.dot(tx.dot(w)).flatten().flatten()\n",
    "        # loss = summing - y_component*regularizer\n",
    "        loss = calculate_loss(y, tx, w) + lambda_*np.linalg.norm(w) ** 2\n",
    "        losses.append(loss)\n",
    "        if iter % iter_drop == 0:\n",
    "            gamma = gamma * drop ** np.floor((1+iter) / (iter_drop))\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    return losses, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=1 up to j=degree.\"\"\"\n",
    "    powers = np.arange(1, degree + 1)\n",
    "    phi = x[:,0]\n",
    "    for i in range(1, x.shape[1]):\n",
    "        phi_i = np.column_stack([np.power(x[:,i], exponent) for exponent in powers])\n",
    "        phi = np.column_stack([phi, phi_i])\n",
    "    return phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    N = y.shape[0]\n",
    "    np.random.seed(seed)\n",
    "    interval = int(np.floor(N / k_fold))\n",
    "    indices = np.random.permutation(N)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_interval(low, high, size):\n",
    "    sample = np.random.uniform(low, high, size)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_ridge(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    N = y.shape[0]\n",
    "    k_fold = k_indices.shape[0]\n",
    "    list_ = []\n",
    "    interval = int(N/k_fold)\n",
    "    for i in range(k_fold):\n",
    "        if i != k:\n",
    "            list_.append(i)\n",
    "    x_training = np.zeros((int((k_fold-1)/k_fold*N), x.shape[1]))\n",
    "    y_training = np.zeros(int((k_fold-1)/k_fold*N))\n",
    "    for j in range(len(list_)):\n",
    "        x_training[interval*(j):interval*(j+1), :] = x[np.array([k_indices[list_[j]]]), :]\n",
    "    x_testing = x[k_indices[k], :]\n",
    "    for j in range(len(list_)):\n",
    "        y_training[interval*(j):interval*(j+1)] = y[np.array([k_indices[list_[j]]])]\n",
    "    y_testing = y[k_indices[k]]\n",
    "    x_training_augmented = build_poly(x_training, degree)\n",
    "    x_testing_augmented = build_poly(x_testing, degree)\n",
    "    w_opt_training = ridge_regression(y_training, x_training_augmented, lambda_)\n",
    "    predictions_test = x_training_augmented @ w_opt_training\n",
    "    predictions_test = np.array([0 if el < 0.5 else 1 for el in predictions_test])\n",
    "    acc_test = compute_accuracy(y_testing, predictions_test)\n",
    "    return acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-42-eb7fdddd9beb>:3: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  accuracy = np.array([y_test == pred]).sum() / N\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77811511 0.77811511 0.77811511 0.77811511 0.77811511 0.22890177\n",
      " 0.22890177 0.22890177 0.22890177 0.22890177 0.20438385 0.20438385\n",
      " 0.20438385 0.20438385 0.20438385 0.95885669 0.95885669 0.95885669\n",
      " 0.95885669 0.95885669 0.9652478  0.9652478  0.9652478  0.9652478\n",
      " 0.9652478  0.34905057 0.34905057 0.34905057 0.34905057 0.34905057] [2 3 4 5 6 2 3 4 5 6 2 3 4 5 6 2 3 4 5 6 2 3 4 5 6 2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "degrees = np.arange(2, 7)\n",
    "lambdas = random_interval(1.0e-05, 1, 6)\n",
    "k_fold = 4\n",
    "seed = 1\n",
    "testing_acc = np.zeros((len(lambdas), len(degrees)))\n",
    "k_indices = build_k_indices(y_0, k_fold, seed)\n",
    "for index1 in range(len(lambdas)):\n",
    "    for index2 in range(len(degrees)):\n",
    "        test_acc = 0\n",
    "        for k in range(k_fold):\n",
    "            current_test_acc = cross_validation_ridge(y_0, tX_tilda_0, k_indices, k,\n",
    "                                                lambdas[index1], degrees[index2])\n",
    "            test_acc += current_test_acc\n",
    "        testing_acc[index1, index2] = test_acc / k_fold\n",
    "best_result = np.where(testing_acc == np.amax(testing_acc))\n",
    "lambda_opt, degree_opt = lambdas[best_result[0]],degrees[best_result[1]]\n",
    "print(lambda_opt, degree_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(testing_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = np.arange(2, 7)\n",
    "lambdas = np.logspace(-5,0,15)\n",
    "k_fold = 5\n",
    "seed = 1\n",
    "training_loss = np.zeros((len(lambdas), len(degrees)))\n",
    "testing_loss = np.zeros((len(lambdas), len(degrees)))\n",
    "k_indices = build_k_indices(y_1, k_fold, seed)\n",
    "for index1 in range(len(lambdas)):\n",
    "    for index2 in range(len(degrees)):\n",
    "        train_loss = 0\n",
    "        test_loss = 0\n",
    "        for k in range(k_fold):\n",
    "            loss_tr, loss_te = cross_validation_ridge(y_1, tX_tilda_1, k_indices, k, \n",
    "                                                lambdas[index1], degrees[index2])\n",
    "            train_loss += loss_tr\n",
    "            test_loss += loss_te\n",
    "        training_loss[index1, index2] = train_loss / k_fold\n",
    "        testing_loss[index1, index2] = test_loss / k_fold\n",
    "best_result = np.where(testing_loss == np.amin(testing_loss))\n",
    "print(testing_loss)\n",
    "lambda_opt, degree_opt = lambdas[best_result[0]], degrees[best_result[1]]\n",
    "print(lambda_opt, degree_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = np.arange(2, 7)\n",
    "lambdas = np.logspace(-5,0,15)\n",
    "k_fold = 5\n",
    "seed = 1\n",
    "training_loss = np.zeros((len(lambdas), len(degrees)))\n",
    "testing_loss = np.zeros((len(lambdas), len(degrees)))\n",
    "k_indices = build_k_indices(y_2_3, k_fold, seed)\n",
    "for index1 in range(len(lambdas)):\n",
    "    for index2 in range(len(degrees)):\n",
    "        train_loss = 0\n",
    "        test_loss = 0\n",
    "        for k in range(k_fold):\n",
    "            loss_tr, loss_te = cross_validation_ridge(y_2_3, tX_tilda_2_3, k_indices, k,\n",
    "                                            lambdas[index1], degrees[index2])\n",
    "            train_loss += loss_tr\n",
    "            test_loss += loss_te\n",
    "        training_loss[index1, index2] = train_loss / k_fold\n",
    "        testing_loss[index1, index2] = test_loss / k_fold\n",
    "best_result = np.where(testing_loss == np.amin(testing_loss))\n",
    "lambda_opt, degree_opt = lambdas[best_result[0]], degrees[best_result[1]]\n",
    "print(testing_loss)\n",
    "print(lambda_opt, degree_opt) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model for ridge regression with the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_tilda_0_augmented = build_poly(tX_tilda_0, degree = 6)\n",
    "w_ridge_0 = ridge_regression(y_0, tX_tilda_0_augmented, lambda_= 0.00061054)\n",
    "#print(w_ridge_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_tilda_1_augmented = build_poly(tX_tilda_1, degree=6)\n",
    "w_ridge_1 = ridge_regression(y_1, tX_tilda_1_augmented, lambda_= 5.17947468e-05)\n",
    "#print(w_ridge_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_tilda_2_3_augmented = build_poly(tX_tilda_2_3, degree=6)\n",
    "w_ridge_2_3 = ridge_regression(y_2_3, tX_tilda_2_3_augmented, lambda_= 0.00026827)\n",
    "#print(w_ridge_2_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_test, pred):\n",
    "    N = y_test.shape[0]\n",
    "    accuracy = np.array([y_test == pred]).sum() / N\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_logistic(y, x, k_indices, k, lambda_, degree, gamma):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    N = y.shape[0]\n",
    "    k_fold = k_indices.shape[0]\n",
    "    list_ = []\n",
    "    interval = int(N/k_fold)\n",
    "    for i in range(k_fold):\n",
    "        if i != k:\n",
    "            list_.append(i)\n",
    "    x_training = np.zeros((int((k_fold-1)/k_fold*N), x.shape[1]))\n",
    "    y_training = np.zeros(int((k_fold-1)/k_fold*N))\n",
    "    for j in range(len(list_)):\n",
    "        x_training[interval*(j):interval*(j+1), :] = x[np.array([k_indices[list_[j]]]), :]\n",
    "    x_testing = x[k_indices[k], :]\n",
    "    for j in range(len(list_)):\n",
    "        y_training[interval*(j):interval*(j+1)] = y[np.array([k_indices[list_[j]]])]\n",
    "    y_testing = y[k_indices[k]]\n",
    "    x_training_augmented = build_poly(x_training, degree)\n",
    "    x_testing_augmented = build_poly(x_testing, degree)\n",
    "    #w_opt_training = ridge_regression(y_training, x_training_augmented, lambda_)\n",
    "    # _,  w_opt_training = learning_by_penalized_gradient(y_training, x_training_augmented,\n",
    "                                                        # np.ones(x_training_augmented.shape[1]), gamma, 1000, lambda_)\n",
    "    _, w_opt_training = learning_by_penalized_gradient(y_training, x_training_augmented,\n",
    "                                                       np.zeros(x_training_augmented.shape[1]), gamma, 1000, lambda_)\n",
    "    predictions_test = x_testing_augmented @ w_opt_training\n",
    "    predictions_test = np.array([0 if el < 0.5 else 1 for el in predictions_test])\n",
    "    acc_test = compute_accuracy(y_testing, predictions_test)\n",
    "    # loss_tr = calculate_loss(y_training, x_training_augmented, w_opt_training) + lambda_*np.linalg.norm(w_opt_training) ** 2\n",
    "    # loss_te = calculate_loss(y_testing, x_testing_augmented, w_opt_training) + lambda_*np.linalg.norm(w_opt_training) ** 2\n",
    "    return acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform cross validation in order to find the best parameters degree, lamdba and gamma caracterizing logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          1.          1.05744907 ...  0.01825038  0.04662815\n",
      "  -0.7724943 ]\n",
      " [ 1.          1.          2.14505538 ... -1.71504715 -0.4674532\n",
      "  -1.44727052]\n",
      " [ 1.          1.         -0.24632406 ...  0.42830338  0.0267496\n",
      "   0.12380588]\n",
      " ...\n",
      " [ 1.          1.         -0.0469687  ...  0.97508147 -0.46524447\n",
      "  -0.8883482 ]\n",
      " [ 1.          1.         -0.60851918 ... -1.13887042  0.46131675\n",
      "  -0.22629665]\n",
      " [ 1.          1.         -0.0469687  ...  0.60787347 -0.86778508\n",
      "  -0.49908812]]\n"
     ]
    }
   ],
   "source": [
    "print(tX_tilda_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=652257.7635883437\n",
      "Current iteration=25, loss=480163.35908409214\n",
      "Current iteration=50, loss=186591.49588769602\n",
      "Current iteration=75, loss=161669.67546850652\n",
      "Current iteration=100, loss=161305.5928811124\n",
      "Current iteration=125, loss=161285.57917131865\n",
      "Current iteration=150, loss=161284.958393035\n",
      "Current iteration=175, loss=161284.948695521\n",
      "Current iteration=200, loss=161284.94861975955\n",
      "Current iteration=225, loss=161284.94861946345\n",
      "Current iteration=0, loss=653719.3948142235\n",
      "Current iteration=25, loss=480664.0542517896\n",
      "Current iteration=50, loss=186113.83334324262\n",
      "Current iteration=75, loss=161574.5517215124\n",
      "Current iteration=100, loss=161224.06752454006\n",
      "Current iteration=125, loss=161204.8417374806\n",
      "Current iteration=150, loss=161204.24540151752\n",
      "Current iteration=175, loss=161204.23608587298\n",
      "Current iteration=200, loss=161204.2360130947\n",
      "Current iteration=225, loss=161204.23601281046\n",
      "Current iteration=0, loss=648805.4647387654\n",
      "Current iteration=25, loss=477851.6991345791\n",
      "Current iteration=50, loss=184725.17914203944\n",
      "Current iteration=75, loss=160262.85633076495\n",
      "Current iteration=100, loss=159909.71188696648\n",
      "Current iteration=125, loss=159890.2787065396\n",
      "Current iteration=150, loss=159889.6758328615\n",
      "Current iteration=175, loss=159889.66641558113\n",
      "Current iteration=200, loss=159889.666342009\n",
      "Current iteration=225, loss=159889.66634172172\n",
      "Current iteration=0, loss=650901.1220030573\n",
      "Current iteration=25, loss=479129.95470412256\n",
      "Current iteration=50, loss=184605.00287967926\n",
      "Current iteration=75, loss=160429.15432737747\n",
      "Current iteration=100, loss=160075.22433993153\n",
      "Current iteration=125, loss=160055.69980955953\n",
      "Current iteration=150, loss=160055.09403398566\n",
      "Current iteration=175, loss=160055.0845707933\n",
      "Current iteration=200, loss=160055.08449686217\n",
      "Current iteration=225, loss=160055.08449657352\n",
      "Current iteration=0, loss=6436070.170931685\n",
      "Current iteration=25, loss=7828429.610505147\n",
      "Current iteration=50, loss=2741751.0532208476\n",
      "Current iteration=75, loss=485142.51455282187\n",
      "Current iteration=100, loss=434324.12899942824\n",
      "Current iteration=125, loss=431354.8569548337\n",
      "Current iteration=150, loss=431262.4630957798\n",
      "Current iteration=175, loss=431261.019629071\n",
      "Current iteration=200, loss=431261.00835200987\n",
      "Current iteration=225, loss=431261.00830795884\n",
      "Current iteration=0, loss=6441115.293363008\n",
      "Current iteration=25, loss=4062498.280717784\n",
      "Current iteration=50, loss=1442388.9130497843\n",
      "Current iteration=75, loss=371096.019674289\n",
      "Current iteration=100, loss=334734.2075652165\n",
      "Current iteration=125, loss=332689.31209286617\n",
      "Current iteration=150, loss=332625.8322219506\n",
      "Current iteration=175, loss=332624.84055540146\n",
      "Current iteration=200, loss=332624.8328080312\n",
      "Current iteration=225, loss=332624.83277776773\n",
      "Current iteration=0, loss=6387307.115247098\n",
      "Current iteration=25, loss=1070477.015422216\n",
      "Current iteration=50, loss=1777223.1350534107\n",
      "Current iteration=75, loss=413394.77340923157\n",
      "Current iteration=100, loss=373263.1564477168\n",
      "Current iteration=125, loss=370969.59107598296\n",
      "Current iteration=150, loss=370898.32308621984\n",
      "Current iteration=175, loss=370897.20971965836\n",
      "Current iteration=200, loss=370897.2010215059\n",
      "Current iteration=225, loss=370897.20098752854\n",
      "Current iteration=0, loss=6404680.261604694\n",
      "Current iteration=25, loss=1045925.5667848907\n",
      "Current iteration=50, loss=984100.6199699589\n",
      "Current iteration=75, loss=265373.80357485707\n",
      "Current iteration=100, loss=311981.3000036643\n",
      "Current iteration=125, loss=309444.1137600284\n",
      "Current iteration=150, loss=309365.8833295965\n",
      "Current iteration=175, loss=309364.6614863355\n",
      "Current iteration=200, loss=309364.65194074565\n",
      "Current iteration=225, loss=309364.65190345835\n",
      "Current iteration=0, loss=12345238.3231913\n",
      "Current iteration=25, loss=2449316.6612869957\n",
      "Current iteration=50, loss=3814616.961670047\n",
      "Current iteration=75, loss=960420.4937298674\n",
      "Current iteration=100, loss=867043.3740693472\n",
      "Current iteration=125, loss=877742.518756433\n",
      "Current iteration=150, loss=877681.201346792\n",
      "Current iteration=175, loss=877680.2153404934\n",
      "Current iteration=200, loss=877680.2076357831\n",
      "Current iteration=225, loss=877680.2076056869\n",
      "Current iteration=0, loss=12468964.82593753\n",
      "Current iteration=25, loss=4088274.741093354\n",
      "Current iteration=50, loss=7139010.371028858\n",
      "Current iteration=75, loss=1878874.4635686493\n",
      "Current iteration=100, loss=853223.4213460841\n",
      "Current iteration=125, loss=863894.4031566082\n",
      "Current iteration=150, loss=863834.5005326243\n",
      "Current iteration=175, loss=863833.5405320082\n",
      "Current iteration=200, loss=863833.5330292423\n",
      "Current iteration=225, loss=863833.5329999344\n",
      "Current iteration=0, loss=12524993.247150641\n",
      "Current iteration=25, loss=15411177.220194364\n",
      "Current iteration=50, loss=2867260.565156994\n",
      "Current iteration=75, loss=1017545.3535783885\n",
      "Current iteration=100, loss=986058.859368841\n",
      "Current iteration=125, loss=946439.5269043272\n",
      "Current iteration=150, loss=946310.6887145133\n",
      "Current iteration=175, loss=946308.7201324649\n",
      "Current iteration=200, loss=946308.7047580442\n",
      "Current iteration=225, loss=946308.7046979889\n",
      "Current iteration=0, loss=12428470.162247779\n",
      "Current iteration=25, loss=4419171.98359768\n",
      "Current iteration=50, loss=7679235.962796414\n",
      "Current iteration=75, loss=1311778.7375131221\n",
      "Current iteration=100, loss=905973.1431960654\n",
      "Current iteration=125, loss=917042.6234650826\n",
      "Current iteration=150, loss=916977.8172242749\n",
      "Current iteration=175, loss=916976.7717830638\n",
      "Current iteration=200, loss=916976.7636117674\n",
      "Current iteration=225, loss=916976.7635798482\n",
      "Current iteration=0, loss=652252.6998271362\n",
      "Current iteration=25, loss=480152.07860340504\n",
      "Current iteration=50, loss=186590.6563007317\n",
      "Current iteration=75, loss=161674.9347698831\n",
      "Current iteration=100, loss=161310.73229808427\n",
      "Current iteration=125, loss=161290.7107851619\n",
      "Current iteration=150, loss=161290.0897584877\n",
      "Current iteration=175, loss=161290.0800575599\n",
      "Current iteration=200, loss=161290.0799817717\n",
      "Current iteration=225, loss=161290.07998147557\n",
      "Current iteration=0, loss=653714.3259083014\n",
      "Current iteration=25, loss=480652.34889347426\n",
      "Current iteration=50, loss=186113.00052849913\n",
      "Current iteration=75, loss=161579.72939021993\n",
      "Current iteration=100, loss=161229.12879584963\n",
      "Current iteration=125, loss=161209.89606244687\n",
      "Current iteration=150, loss=161209.29951556204\n",
      "Current iteration=175, loss=161209.29019662197\n",
      "Current iteration=200, loss=161209.29012381786\n",
      "Current iteration=225, loss=161209.29012353357\n",
      "Current iteration=0, loss=648800.4447713244\n",
      "Current iteration=25, loss=477840.2591986415\n",
      "Current iteration=50, loss=184724.08712418017\n",
      "Current iteration=75, loss=160267.9228408145\n",
      "Current iteration=100, loss=159914.6658679603\n",
      "Current iteration=125, loss=159895.2256581067\n",
      "Current iteration=150, loss=159894.62255697354\n",
      "Current iteration=175, loss=159894.61313558288\n",
      "Current iteration=200, loss=159894.61306197828\n",
      "Current iteration=225, loss=159894.6130616909\n",
      "Current iteration=0, loss=650896.0938818706\n",
      "Current iteration=25, loss=479117.741081957\n",
      "Current iteration=50, loss=184603.97184530576\n",
      "Current iteration=75, loss=160434.14470903293\n",
      "Current iteration=100, loss=160080.10782245925\n",
      "Current iteration=125, loss=160060.5766698865\n",
      "Current iteration=150, loss=160059.97069606057\n",
      "Current iteration=175, loss=160059.9612297706\n",
      "Current iteration=200, loss=160059.96115581546\n",
      "Current iteration=225, loss=160059.96115552643\n",
      "Current iteration=0, loss=6436039.206453713\n",
      "Current iteration=25, loss=7756997.788421995\n",
      "Current iteration=50, loss=2716682.6633752384\n",
      "Current iteration=75, loss=479319.1420046431\n",
      "Current iteration=100, loss=429267.55372159276\n",
      "Current iteration=125, loss=426346.0171962842\n",
      "Current iteration=150, loss=426255.1121313008\n",
      "Current iteration=175, loss=426253.6919305937\n",
      "Current iteration=200, loss=426253.6808352983\n",
      "Current iteration=225, loss=426253.6807919578\n",
      "Current iteration=0, loss=6441084.279221807\n",
      "Current iteration=25, loss=3835894.84654649\n",
      "Current iteration=50, loss=4008839.0157838333\n",
      "Current iteration=75, loss=642177.0125297883\n",
      "Current iteration=100, loss=567211.7527535852\n",
      "Current iteration=125, loss=562888.8766399462\n",
      "Current iteration=150, loss=562754.4734553073\n",
      "Current iteration=175, loss=562752.3737331049\n",
      "Current iteration=200, loss=562752.3573290649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=225, loss=562752.357264987\n",
      "Current iteration=0, loss=6387276.576891084\n",
      "Current iteration=25, loss=1066093.5484307588\n",
      "Current iteration=50, loss=1856746.1194416238\n",
      "Current iteration=75, loss=418042.23146715056\n",
      "Current iteration=100, loss=377195.88555543165\n",
      "Current iteration=125, loss=374857.42788441526\n",
      "Current iteration=150, loss=374784.75996446045\n",
      "Current iteration=175, loss=374783.6247263244\n",
      "Current iteration=200, loss=374783.6158573\n",
      "Current iteration=225, loss=374783.6158226558\n",
      "Current iteration=0, loss=6404649.569578427\n",
      "Current iteration=25, loss=1049343.6140047428\n",
      "Current iteration=50, loss=1852709.4906434228\n",
      "Current iteration=75, loss=317774.9537684487\n",
      "Current iteration=100, loss=284986.9112058019\n",
      "Current iteration=125, loss=283731.6961071869\n",
      "Current iteration=150, loss=283692.73562577646\n",
      "Current iteration=175, loss=283692.1269949114\n",
      "Current iteration=200, loss=283692.1222399979\n",
      "Current iteration=225, loss=283692.1222214237\n",
      "Current iteration=0, loss=12345171.365891738\n",
      "Current iteration=25, loss=2452802.481620305\n",
      "Current iteration=50, loss=1989364.5981474477\n",
      "Current iteration=75, loss=945366.8028252658\n",
      "Current iteration=100, loss=809093.4756738071\n",
      "Current iteration=125, loss=819917.481165683\n",
      "Current iteration=150, loss=819858.387248023\n",
      "Current iteration=175, loss=819857.4375919205\n",
      "Current iteration=200, loss=819857.4301697217\n",
      "Current iteration=225, loss=819857.4301407279\n",
      "Current iteration=0, loss=12468897.600691963\n",
      "Current iteration=25, loss=4100103.3208754263\n",
      "Current iteration=50, loss=7333768.841957644\n",
      "Current iteration=75, loss=1945000.3328191836\n",
      "Current iteration=100, loss=857352.2195718657\n",
      "Current iteration=125, loss=868140.8320200233\n",
      "Current iteration=150, loss=868080.5140971274\n",
      "Current iteration=175, loss=868079.5467945734\n",
      "Current iteration=200, loss=868079.5392346613\n",
      "Current iteration=225, loss=868079.5392051296\n",
      "Current iteration=0, loss=12524925.556526547\n",
      "Current iteration=25, loss=15443547.693687689\n",
      "Current iteration=50, loss=1930122.3327223286\n",
      "Current iteration=75, loss=1856960.205153752\n",
      "Current iteration=100, loss=970934.8648812328\n",
      "Current iteration=125, loss=929303.359069305\n",
      "Current iteration=150, loss=929181.9792523814\n",
      "Current iteration=175, loss=929180.1268122154\n",
      "Current iteration=200, loss=929180.1123451223\n",
      "Current iteration=225, loss=929180.11228861\n",
      "Current iteration=0, loss=12428403.01445106\n",
      "Current iteration=25, loss=4391873.26202456\n",
      "Current iteration=50, loss=6180441.815369959\n",
      "Current iteration=75, loss=945024.6742786035\n",
      "Current iteration=100, loss=844936.4262557925\n",
      "Current iteration=125, loss=854784.5253800729\n",
      "Current iteration=150, loss=854721.3274247432\n",
      "Current iteration=175, loss=854720.3193529326\n",
      "Current iteration=200, loss=854720.3114750022\n",
      "Current iteration=225, loss=854720.311444229\n",
      "Current iteration=0, loss=652252.4737712848\n",
      "Current iteration=25, loss=480151.575084629\n",
      "Current iteration=50, loss=186590.6188256937\n",
      "Current iteration=75, loss=161675.16961814725\n",
      "Current iteration=100, loss=161310.9618068862\n",
      "Current iteration=125, loss=161290.93994599715\n",
      "Current iteration=150, loss=161290.3189101339\n",
      "Current iteration=175, loss=161290.30920859528\n",
      "Current iteration=200, loss=161290.30913280215\n",
      "Current iteration=225, loss=161290.30913250602\n",
      "Current iteration=0, loss=653714.0996227802\n",
      "Current iteration=25, loss=480651.82639234647\n",
      "Current iteration=50, loss=186112.96336693814\n",
      "Current iteration=75, loss=161579.9605942116\n",
      "Current iteration=100, loss=161229.3548019853\n",
      "Current iteration=125, loss=161210.1217584337\n",
      "Current iteration=150, loss=161209.52520213125\n",
      "Current iteration=175, loss=161209.51588304405\n",
      "Current iteration=200, loss=161209.51581023895\n",
      "Current iteration=225, loss=161209.5158099546\n",
      "Current iteration=0, loss=648800.2206705094\n",
      "Current iteration=25, loss=477839.7485348994\n",
      "Current iteration=50, loss=184724.03836640416\n",
      "Current iteration=75, loss=160268.14908261833\n",
      "Current iteration=100, loss=159914.88708871612\n",
      "Current iteration=125, loss=159895.44655815876\n",
      "Current iteration=150, loss=159894.84344794374\n",
      "Current iteration=175, loss=159894.83402641118\n",
      "Current iteration=200, loss=159894.83395280564\n",
      "Current iteration=225, loss=159894.83395251827\n",
      "Current iteration=0, loss=650895.8694170569\n",
      "Current iteration=25, loss=479117.1958737438\n",
      "Current iteration=50, loss=184603.92580965586\n",
      "Current iteration=75, loss=160434.36756035648\n",
      "Current iteration=100, loss=160080.32588691296\n",
      "Current iteration=125, loss=160060.79443867077\n",
      "Current iteration=150, loss=160060.18845599316\n",
      "Current iteration=175, loss=160060.17898956488\n",
      "Current iteration=200, loss=160060.1789156088\n",
      "Current iteration=225, loss=160060.1789153198\n",
      "Current iteration=0, loss=6436037.824141012\n",
      "Current iteration=25, loss=7753937.246305168\n",
      "Current iteration=50, loss=2715726.7856718856\n",
      "Current iteration=75, loss=479162.47986250394\n",
      "Current iteration=100, loss=429130.3643418471\n",
      "Current iteration=125, loss=426210.03966522595\n",
      "Current iteration=150, loss=426119.1723466507\n",
      "Current iteration=175, loss=426117.7527377962\n",
      "Current iteration=200, loss=426117.7416471251\n",
      "Current iteration=225, loss=426117.7416038025\n",
      "Current iteration=0, loss=6441082.894692045\n",
      "Current iteration=25, loss=3824436.4474874525\n",
      "Current iteration=50, loss=4070827.7780093327\n",
      "Current iteration=75, loss=649889.8953199602\n",
      "Current iteration=100, loss=573815.8353791581\n",
      "Current iteration=125, loss=569420.396725432\n",
      "Current iteration=150, loss=569283.7269864902\n",
      "Current iteration=175, loss=569281.5918454345\n",
      "Current iteration=200, loss=569281.5751646841\n",
      "Current iteration=225, loss=569281.5750995252\n",
      "Current iteration=0, loss=6387275.213601271\n",
      "Current iteration=25, loss=1065906.9747655762\n",
      "Current iteration=50, loss=1858740.1302126462\n",
      "Current iteration=75, loss=418190.04575307603\n",
      "Current iteration=100, loss=377321.28522646823\n",
      "Current iteration=125, loss=374981.4119783135\n",
      "Current iteration=150, loss=374908.7001455958\n",
      "Current iteration=175, loss=374907.5642173919\n",
      "Current iteration=200, loss=374907.55534297647\n",
      "Current iteration=225, loss=374907.5553083111\n",
      "Current iteration=0, loss=6404648.199428484\n",
      "Current iteration=25, loss=1049497.9548588379\n",
      "Current iteration=50, loss=1882650.1640976619\n",
      "Current iteration=75, loss=298802.57086150895\n",
      "Current iteration=100, loss=285845.8186848801\n",
      "Current iteration=125, loss=284578.78329497436\n",
      "Current iteration=150, loss=284539.4565731387\n",
      "Current iteration=175, loss=284538.8422202412\n",
      "Current iteration=200, loss=284538.83742062456\n",
      "Current iteration=225, loss=284538.83740187605\n",
      "Current iteration=0, loss=12345168.376791593\n",
      "Current iteration=25, loss=2452957.1670255787\n",
      "Current iteration=50, loss=2288679.722181547\n",
      "Current iteration=75, loss=926265.2147991025\n",
      "Current iteration=100, loss=844251.5677245423\n",
      "Current iteration=125, loss=807978.735678135\n",
      "Current iteration=150, loss=807872.2481823381\n",
      "Current iteration=175, loss=807870.6166095157\n",
      "Current iteration=200, loss=807870.603866563\n",
      "Current iteration=225, loss=807870.6038167849\n",
      "Current iteration=0, loss=12468894.599630201\n",
      "Current iteration=25, loss=4100649.3597180475\n",
      "Current iteration=50, loss=7327304.496417056\n",
      "Current iteration=75, loss=1944327.3035144515\n",
      "Current iteration=100, loss=856868.0088821837\n",
      "Current iteration=125, loss=867658.2366564223\n",
      "Current iteration=150, loss=867597.9329465579\n",
      "Current iteration=175, loss=867596.9658410375\n",
      "Current iteration=200, loss=867596.9582826674\n",
      "Current iteration=225, loss=867596.9582531418\n",
      "Current iteration=0, loss=12524922.534689412\n",
      "Current iteration=25, loss=15444933.109050132\n",
      "Current iteration=50, loss=2050242.011864014\n",
      "Current iteration=75, loss=1026807.0517385484\n",
      "Current iteration=100, loss=917646.0835611926\n",
      "Current iteration=125, loss=927098.650782727\n",
      "Current iteration=150, loss=927033.5120456008\n",
      "Current iteration=175, loss=927032.4753089064\n",
      "Current iteration=200, loss=927032.4672072217\n",
      "Current iteration=225, loss=927032.4671755746\n",
      "Current iteration=0, loss=12428400.016846761\n",
      "Current iteration=25, loss=4391249.602502807\n",
      "Current iteration=50, loss=5788758.601223233\n",
      "Current iteration=75, loss=930915.7059795791\n",
      "Current iteration=100, loss=843495.356527989\n",
      "Current iteration=125, loss=853426.2009989704\n",
      "Current iteration=150, loss=853364.1679169473\n",
      "Current iteration=175, loss=853363.1777908867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=200, loss=853363.1700531279\n",
      "Current iteration=225, loss=853363.170022902\n",
      "Current iteration=0, loss=652259.4300303926\n",
      "Current iteration=25, loss=480167.0718241209\n",
      "Current iteration=50, loss=186591.77241744107\n",
      "Current iteration=75, loss=161667.9452587088\n",
      "Current iteration=100, loss=161303.90213882353\n",
      "Current iteration=125, loss=161283.8909842744\n",
      "Current iteration=150, loss=161283.27027787946\n",
      "Current iteration=175, loss=161283.26058148872\n",
      "Current iteration=200, loss=161283.26050573593\n",
      "Current iteration=225, loss=161283.2605054402\n",
      "Current iteration=0, loss=653721.0629493555\n",
      "Current iteration=25, loss=480667.90715288726\n",
      "Current iteration=50, loss=186114.10742856507\n",
      "Current iteration=75, loss=161572.8483900588\n",
      "Current iteration=100, loss=161222.40247886034\n",
      "Current iteration=125, loss=161203.17897617782\n",
      "Current iteration=150, loss=161202.58270961198\n",
      "Current iteration=175, loss=161202.57339505182\n",
      "Current iteration=200, loss=161202.57332228206\n",
      "Current iteration=225, loss=161202.5733219978\n",
      "Current iteration=0, loss=648807.1167686469\n",
      "Current iteration=25, loss=477855.46426613495\n",
      "Current iteration=50, loss=184725.53846415612\n",
      "Current iteration=75, loss=160261.1895587963\n",
      "Current iteration=100, loss=159908.08216300642\n",
      "Current iteration=125, loss=159888.65126272975\n",
      "Current iteration=150, loss=159888.0484915881\n",
      "Current iteration=175, loss=159888.0390753533\n",
      "Current iteration=200, loss=159888.03900178938\n",
      "Current iteration=225, loss=159888.039001502\n",
      "Current iteration=0, loss=650902.7767162692\n",
      "Current iteration=25, loss=479133.9746031856\n",
      "Current iteration=50, loss=184605.34210652398\n",
      "Current iteration=75, loss=160427.51273569284\n",
      "Current iteration=100, loss=160073.6177564326\n",
      "Current iteration=125, loss=160054.09552449215\n",
      "Current iteration=150, loss=160053.48994815527\n",
      "Current iteration=175, loss=160053.48048807585\n",
      "Current iteration=200, loss=160053.48041416923\n",
      "Current iteration=225, loss=160053.48041388064\n",
      "Current iteration=0, loss=6436080.361085999\n",
      "Current iteration=25, loss=7853258.995595379\n",
      "Current iteration=50, loss=2748068.5915254145\n",
      "Current iteration=75, loss=488073.56835623644\n",
      "Current iteration=100, loss=436860.50277778466\n",
      "Current iteration=125, loss=433866.76576548204\n",
      "Current iteration=150, loss=433773.60779973556\n",
      "Current iteration=175, loss=433772.1523959599\n",
      "Current iteration=200, loss=433772.1410256406\n",
      "Current iteration=225, loss=433772.14098122483\n",
      "Current iteration=0, loss=6441125.499861081\n",
      "Current iteration=25, loss=4124898.800375573\n",
      "Current iteration=50, loss=1021020.1393016928\n",
      "Current iteration=75, loss=441963.2346998621\n",
      "Current iteration=100, loss=394069.7802823095\n",
      "Current iteration=125, loss=391357.17755620374\n",
      "Current iteration=150, loss=391272.91527501296\n",
      "Current iteration=175, loss=391271.59891831136\n",
      "Current iteration=200, loss=391271.588634304\n",
      "Current iteration=225, loss=391271.5885941322\n",
      "Current iteration=0, loss=6387317.165168189\n",
      "Current iteration=25, loss=1072005.7702950784\n",
      "Current iteration=50, loss=1745416.7647230118\n",
      "Current iteration=75, loss=411912.77551375417\n",
      "Current iteration=100, loss=372012.96681952855\n",
      "Current iteration=125, loss=369733.85823582305\n",
      "Current iteration=150, loss=369663.0437972778\n",
      "Current iteration=175, loss=369661.93752016366\n",
      "Current iteration=200, loss=369661.92887739715\n",
      "Current iteration=225, loss=369661.9288436364\n",
      "Current iteration=0, loss=6404690.362097398\n",
      "Current iteration=25, loss=1044778.9689008391\n",
      "Current iteration=50, loss=652569.2856451557\n",
      "Current iteration=75, loss=267472.2546794621\n",
      "Current iteration=100, loss=306181.8090140647\n",
      "Current iteration=125, loss=303841.5559100359\n",
      "Current iteration=150, loss=303769.44040839566\n",
      "Current iteration=175, loss=303768.31409620965\n",
      "Current iteration=200, loss=303768.305296954\n",
      "Current iteration=225, loss=303768.30526258174\n",
      "Current iteration=0, loss=12345260.358286342\n",
      "Current iteration=25, loss=2448163.189080677\n",
      "Current iteration=50, loss=5587148.102231342\n",
      "Current iteration=75, loss=1320289.2477374817\n",
      "Current iteration=100, loss=847721.1259434703\n",
      "Current iteration=125, loss=857745.6362137407\n",
      "Current iteration=150, loss=857682.1865421868\n",
      "Current iteration=175, loss=857681.173709097\n",
      "Current iteration=200, loss=857681.1657938411\n",
      "Current iteration=225, loss=857681.1657629205\n",
      "Current iteration=0, loss=12468986.94921139\n",
      "Current iteration=25, loss=4084574.0843291758\n",
      "Current iteration=50, loss=7219277.612699222\n",
      "Current iteration=75, loss=2098938.832890376\n",
      "Current iteration=100, loss=846761.0186508432\n",
      "Current iteration=125, loss=857514.8942049165\n",
      "Current iteration=150, loss=857454.0069317857\n",
      "Current iteration=175, loss=857453.0312719052\n",
      "Current iteration=200, loss=857453.023646768\n",
      "Current iteration=225, loss=857453.0236169823\n",
      "Current iteration=0, loss=12525015.523576735\n",
      "Current iteration=25, loss=15400110.04987437\n",
      "Current iteration=50, loss=2365652.8516487693\n",
      "Current iteration=75, loss=2014342.6558837944\n",
      "Current iteration=100, loss=935479.8461560193\n",
      "Current iteration=125, loss=945399.9602776482\n",
      "Current iteration=150, loss=945337.1852605856\n",
      "Current iteration=175, loss=945336.1811788814\n",
      "Current iteration=200, loss=945336.1733318183\n",
      "Current iteration=225, loss=945336.173301165\n",
      "Current iteration=0, loss=12428492.26003386\n",
      "Current iteration=25, loss=4430542.737902379\n",
      "Current iteration=50, loss=4873881.508006918\n",
      "Current iteration=75, loss=993631.1391237441\n",
      "Current iteration=100, loss=904252.246555599\n",
      "Current iteration=125, loss=914238.3227052352\n",
      "Current iteration=150, loss=914177.0235202197\n",
      "Current iteration=175, loss=914176.0400542994\n",
      "Current iteration=200, loss=914176.0323680053\n",
      "Current iteration=225, loss=914176.0323379809\n",
      "[1.15562258] [2]\n",
      "0.8287092641524542\n"
     ]
    }
   ],
   "source": [
    "lambdas = random_interval(1, 1.2, 4)\n",
    "degrees = np.arange(1, 4)\n",
    "gamma = 1.7783e-04\n",
    "k_fold = 4\n",
    "seed = 1\n",
    "testing_acc = np.zeros((len(lambdas), len(degrees)))\n",
    "k_indices = build_k_indices(y_0, k_fold, seed)\n",
    "for index1 in range(len(lambdas)):\n",
    "    for index2 in range(len(degrees)):\n",
    "        #  for index3 in range(len(gammas)):\n",
    "        test_acc = 0\n",
    "        for k in range(k_fold):\n",
    "        # loss_tr, loss_te = cross_validation_logistic(y_0, tX_tilda_0, k_indices, k,\n",
    "                                         #       lambdas[index1], degrees[index2], gamma)\n",
    "            current_test_acc = cross_validation_logistic(y_0, tX_tilda_0, \n",
    "                                                            k_indices, k, lambdas[index1], degrees[index2], gamma)\n",
    "            test_acc += current_test_acc\n",
    "        testing_acc[index1, index2] = test_acc / k_fold\n",
    "best_result = np.where(testing_acc == np.amax(testing_acc))\n",
    "lambda_opt, degree_opt = lambdas[best_result[0]], degrees[best_result[1]]\n",
    "print(lambda_opt, degree_opt)\n",
    "print(np.amax(testing_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.81372608 0.82828889 0.81358596]\n",
      " [0.81372608 0.82813876 0.81346585]\n",
      " [0.81372608 0.82787853 0.81353591]\n",
      " [0.81372608 0.82801866 0.81331572]\n",
      " [0.81372608 0.82807871 0.81351589]]\n",
      "[0.95] [2]\n"
     ]
    }
   ],
   "source": [
    "print(testing_acc)\n",
    "best_result = np.where(testing_acc == np.amax(testing_acc))\n",
    "lambda_opt, degree_opt = lambdas[best_result[0]], degrees[best_result[1]]\n",
    "print(lambda_opt, degree_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimal degree is 2 and optimal lambda is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With randomized grid search on lambda one gets optimal lambda = 2.95531731"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With a slightly lower accuracy with respect to before (0.82950997)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda = 2.13 and degree = 2 (0.8291)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda = 2.14434261 and degree = 2 (0.8293)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01       0.03162278 0.1        0.31622777 1.        ]\n"
     ]
    }
   ],
   "source": [
    "print(np.logspace(-2,0,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=5279231.503736983\n",
      "Current iteration=25, loss=6061023.709601447\n",
      "Current iteration=50, loss=5245442.205686721\n",
      "Current iteration=75, loss=656627.1277374778\n",
      "Current iteration=100, loss=120434.62550969239\n",
      "Current iteration=125, loss=114648.77524708615\n",
      "Current iteration=150, loss=114484.68519551256\n",
      "Current iteration=175, loss=114482.12870965016\n",
      "Current iteration=200, loss=114482.10873798579\n",
      "Current iteration=225, loss=114482.1086599714\n",
      "Current iteration=0, loss=5365729.055493684\n",
      "Current iteration=25, loss=5974717.147740176\n",
      "Current iteration=50, loss=5351458.295494573\n",
      "Current iteration=75, loss=658175.9999135114\n",
      "Current iteration=100, loss=119715.18554802176\n",
      "Current iteration=125, loss=113885.20667559553\n",
      "Current iteration=150, loss=113720.59626994412\n",
      "Current iteration=175, loss=113718.03198710919\n",
      "Current iteration=200, loss=113718.01195457339\n",
      "Current iteration=225, loss=113718.01187632134\n",
      "Current iteration=0, loss=5290339.746762615\n",
      "Current iteration=25, loss=6126887.141883835\n",
      "Current iteration=50, loss=5275744.9252635855\n",
      "Current iteration=75, loss=666944.7954755927\n",
      "Current iteration=100, loss=115145.70739912377\n",
      "Current iteration=125, loss=109896.08517471622\n",
      "Current iteration=150, loss=109749.24122253242\n",
      "Current iteration=175, loss=109746.95434848922\n",
      "Current iteration=200, loss=109746.93648318044\n",
      "Current iteration=225, loss=109746.9364133941\n",
      "Current iteration=0, loss=5355002.421821018\n",
      "Current iteration=25, loss=6027816.978447156\n",
      "Current iteration=50, loss=5175742.086526256\n",
      "Current iteration=75, loss=669223.067333213\n",
      "Current iteration=100, loss=121830.71829081632\n",
      "Current iteration=125, loss=115961.63237995406\n",
      "Current iteration=150, loss=115794.9825630369\n",
      "Current iteration=175, loss=115792.3861066448\n",
      "Current iteration=200, loss=115792.36582271357\n",
      "Current iteration=225, loss=115792.36574347956\n",
      "Current iteration=0, loss=52204204.954409026\n",
      "Current iteration=25, loss=76996119.27955703\n",
      "Current iteration=50, loss=34035693.73904624\n",
      "Current iteration=75, loss=11701824.158162097\n",
      "Current iteration=100, loss=11187944.566301674\n",
      "Current iteration=125, loss=11156477.319272313\n",
      "Current iteration=150, loss=11155494.252191948\n",
      "Current iteration=175, loss=11155478.891861282\n",
      "Current iteration=200, loss=11155478.771858709\n",
      "Current iteration=225, loss=11155478.771389948\n",
      "Current iteration=250, loss=11155478.771389019\n",
      "Current iteration=0, loss=52703188.865502834\n",
      "Current iteration=25, loss=76812666.60128617\n",
      "Current iteration=50, loss=31782232.567210026\n",
      "Current iteration=75, loss=11071370.704516577\n",
      "Current iteration=100, loss=10560799.711278621\n",
      "Current iteration=125, loss=10529526.096234692\n",
      "Current iteration=150, loss=10528549.790311076\n",
      "Current iteration=175, loss=10528534.536062978\n",
      "Current iteration=200, loss=10528534.41688922\n",
      "Current iteration=225, loss=10528534.416423699\n",
      "Current iteration=250, loss=10528534.41642279\n",
      "Current iteration=0, loss=52375972.750789315\n",
      "Current iteration=25, loss=77443919.06169033\n",
      "Current iteration=50, loss=34889488.85519254\n",
      "Current iteration=75, loss=12208421.119572466\n",
      "Current iteration=100, loss=11667327.044980822\n",
      "Current iteration=125, loss=11634622.028931443\n",
      "Current iteration=150, loss=11633601.91943537\n",
      "Current iteration=175, loss=11633585.98109599\n",
      "Current iteration=200, loss=11633585.856577821\n",
      "Current iteration=225, loss=11633585.856091406\n",
      "Current iteration=250, loss=11633585.85609045\n",
      "Current iteration=0, loss=53837572.38339494\n",
      "Current iteration=25, loss=78463743.58701642\n",
      "Current iteration=50, loss=31214568.514341954\n",
      "Current iteration=75, loss=11526903.033364577\n",
      "Current iteration=100, loss=10998898.45754708\n",
      "Current iteration=125, loss=10966306.240273606\n",
      "Current iteration=150, loss=10965288.360180711\n",
      "Current iteration=175, loss=10965272.456091657\n",
      "Current iteration=200, loss=10965272.331840994\n",
      "Current iteration=225, loss=10965272.331355635\n",
      "Current iteration=250, loss=10965272.331354696\n",
      "Current iteration=0, loss=61888896.53444538\n",
      "Current iteration=25, loss=39490617.46525986\n",
      "Current iteration=50, loss=61894267.44511203\n",
      "Current iteration=75, loss=21243719.930186387\n",
      "Current iteration=100, loss=17497532.68807462\n",
      "Current iteration=125, loss=17458823.242120884\n",
      "Current iteration=150, loss=17457623.283746466\n",
      "Current iteration=175, loss=17457604.538892865\n",
      "Current iteration=200, loss=17457604.392449226\n",
      "Current iteration=225, loss=17457604.391877167\n",
      "Current iteration=250, loss=17457604.39187605\n",
      "Current iteration=0, loss=62309932.33514253\n",
      "Current iteration=25, loss=39666575.22523936\n",
      "Current iteration=50, loss=60613812.01228262\n",
      "Current iteration=75, loss=16351750.836566597\n",
      "Current iteration=100, loss=16343049.781908626\n",
      "Current iteration=125, loss=16307106.86085137\n",
      "Current iteration=150, loss=16305996.172488457\n",
      "Current iteration=175, loss=16305978.824178088\n",
      "Current iteration=200, loss=16305978.68864511\n",
      "Current iteration=225, loss=16305978.688115673\n",
      "Current iteration=250, loss=16305978.688114647\n",
      "Current iteration=0, loss=62149812.07551838\n",
      "Current iteration=25, loss=39876162.58209744\n",
      "Current iteration=50, loss=62202060.61980392\n",
      "Current iteration=75, loss=20830580.861701157\n",
      "Current iteration=100, loss=16933824.720838815\n",
      "Current iteration=125, loss=16895356.944826443\n",
      "Current iteration=150, loss=16894177.25939407\n",
      "Current iteration=175, loss=16894158.83746998\n",
      "Current iteration=200, loss=16894158.693549883\n",
      "Current iteration=225, loss=16894158.692987688\n",
      "Current iteration=250, loss=16894158.692986596\n",
      "Current iteration=0, loss=64941855.66633813\n",
      "Current iteration=25, loss=36891547.4593501\n",
      "Current iteration=50, loss=68172119.75799978\n",
      "Current iteration=75, loss=16654508.12272843\n",
      "Current iteration=100, loss=16768466.565960241\n",
      "Current iteration=125, loss=16733848.136533596\n",
      "Current iteration=150, loss=16732769.00952039\n",
      "Current iteration=175, loss=16732752.149301033\n",
      "Current iteration=200, loss=16732752.017580708\n",
      "Current iteration=225, loss=16732752.017066171\n",
      "Current iteration=250, loss=16732752.017065175\n",
      "Current iteration=0, loss=5285183.279959998\n",
      "Current iteration=25, loss=6123610.16300319\n",
      "Current iteration=50, loss=5281576.758053218\n",
      "Current iteration=75, loss=652046.526069499\n",
      "Current iteration=100, loss=118999.02705923689\n",
      "Current iteration=125, loss=113384.66813011437\n",
      "Current iteration=150, loss=113225.54419413922\n",
      "Current iteration=175, loss=113223.06522848032\n",
      "Current iteration=200, loss=113223.04586242372\n",
      "Current iteration=225, loss=113223.04578677504\n",
      "Current iteration=0, loss=5371764.8659862755\n",
      "Current iteration=25, loss=6038261.492286308\n",
      "Current iteration=50, loss=5389420.834654603\n",
      "Current iteration=75, loss=657496.2120171918\n",
      "Current iteration=100, loss=117734.94452374753\n",
      "Current iteration=125, loss=112153.20878580026\n",
      "Current iteration=150, loss=111996.10598342856\n",
      "Current iteration=175, loss=111993.6589631257\n",
      "Current iteration=200, loss=111993.63984668844\n",
      "Current iteration=225, loss=111993.63977201491\n",
      "Current iteration=0, loss=5296285.980715699\n",
      "Current iteration=25, loss=6183160.647173156\n",
      "Current iteration=50, loss=5308993.70364645\n",
      "Current iteration=75, loss=659636.4624574797\n",
      "Current iteration=100, loss=116346.4485531418\n",
      "Current iteration=125, loss=110999.0572506564\n",
      "Current iteration=150, loss=110848.55718125521\n",
      "Current iteration=175, loss=110846.21293025005\n",
      "Current iteration=200, loss=110846.19461666027\n",
      "Current iteration=225, loss=110846.1945451228\n",
      "Current iteration=0, loss=5361041.598590344\n",
      "Current iteration=25, loss=6092224.19286318\n",
      "Current iteration=50, loss=5211806.295281333\n",
      "Current iteration=75, loss=668707.1508348946\n",
      "Current iteration=100, loss=120348.6559170342\n",
      "Current iteration=125, loss=114643.12362974868\n",
      "Current iteration=150, loss=114481.46227776566\n",
      "Current iteration=175, loss=114478.94381783316\n",
      "Current iteration=200, loss=114478.92414324304\n",
      "Current iteration=225, loss=114478.92406638921\n",
      "Current iteration=0, loss=52235564.788956955\n",
      "Current iteration=25, loss=80020515.76357944\n",
      "Current iteration=50, loss=37757249.33730004\n",
      "Current iteration=75, loss=11813510.175190745\n",
      "Current iteration=100, loss=11218153.06038175\n",
      "Current iteration=125, loss=11181655.94457049\n",
      "Current iteration=150, loss=11180516.380729688\n",
      "Current iteration=175, loss=11180498.575547984\n",
      "Current iteration=200, loss=11180498.436445057\n",
      "Current iteration=225, loss=11180498.435901694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=250, loss=11180498.435900636\n",
      "Current iteration=0, loss=52734918.58666133\n",
      "Current iteration=25, loss=80111244.52841474\n",
      "Current iteration=50, loss=38133883.9158744\n",
      "Current iteration=75, loss=11549035.783974115\n",
      "Current iteration=100, loss=10947663.72240162\n",
      "Current iteration=125, loss=10910491.45934639\n",
      "Current iteration=150, loss=10909330.882493498\n",
      "Current iteration=175, loss=10909312.748955337\n",
      "Current iteration=200, loss=10909312.607287116\n",
      "Current iteration=225, loss=10909312.60673372\n",
      "Current iteration=250, loss=10909312.606732642\n",
      "Current iteration=0, loss=52407374.50766412\n",
      "Current iteration=25, loss=80676086.3198031\n",
      "Current iteration=50, loss=38405715.9873867\n",
      "Current iteration=75, loss=11936721.377736485\n",
      "Current iteration=100, loss=11323174.182609629\n",
      "Current iteration=125, loss=11285690.441531032\n",
      "Current iteration=150, loss=11284520.123311182\n",
      "Current iteration=175, loss=11284501.837575546\n",
      "Current iteration=200, loss=11284501.694718296\n",
      "Current iteration=225, loss=11284501.69416025\n",
      "Current iteration=250, loss=11284501.694159156\n",
      "Current iteration=0, loss=53870172.75056897\n",
      "Current iteration=25, loss=82682573.65664026\n",
      "Current iteration=50, loss=38236247.73924771\n",
      "Current iteration=75, loss=11938509.00309145\n",
      "Current iteration=100, loss=11334389.383630889\n",
      "Current iteration=125, loss=11296919.127230037\n",
      "Current iteration=150, loss=11295749.202239227\n",
      "Current iteration=175, loss=11295730.92267036\n",
      "Current iteration=200, loss=11295730.77986128\n",
      "Current iteration=225, loss=11295730.77930344\n",
      "Current iteration=250, loss=11295730.77930235\n",
      "Current iteration=0, loss=61941109.2120513\n",
      "Current iteration=25, loss=38882568.22554524\n",
      "Current iteration=50, loss=63568308.14538554\n",
      "Current iteration=75, loss=15262311.941463605\n",
      "Current iteration=100, loss=15064201.944755442\n",
      "Current iteration=125, loss=15024199.59149313\n",
      "Current iteration=150, loss=15022959.330842037\n",
      "Current iteration=175, loss=15022939.955650939\n",
      "Current iteration=200, loss=15022939.804282727\n",
      "Current iteration=225, loss=15022939.803691437\n",
      "Current iteration=250, loss=15022939.803690279\n",
      "Current iteration=0, loss=62362572.11983997\n",
      "Current iteration=25, loss=39187542.410772264\n",
      "Current iteration=50, loss=62530565.26809515\n",
      "Current iteration=75, loss=14447595.275259167\n",
      "Current iteration=100, loss=14171965.6401868\n",
      "Current iteration=125, loss=14132282.048290856\n",
      "Current iteration=150, loss=14131052.38865424\n",
      "Current iteration=175, loss=14131033.180300564\n",
      "Current iteration=200, loss=14131033.03023589\n",
      "Current iteration=225, loss=14131033.0296497\n",
      "Current iteration=250, loss=14131033.029648548\n",
      "Current iteration=0, loss=62202007.06978971\n",
      "Current iteration=25, loss=39096161.19874364\n",
      "Current iteration=50, loss=64262951.435245425\n",
      "Current iteration=75, loss=14836431.618412187\n",
      "Current iteration=100, loss=14635185.45207264\n",
      "Current iteration=125, loss=14594547.271513125\n",
      "Current iteration=150, loss=14593284.299322678\n",
      "Current iteration=175, loss=14593264.570108617\n",
      "Current iteration=200, loss=14593264.415974708\n",
      "Current iteration=225, loss=14593264.415372638\n",
      "Current iteration=250, loss=14593264.415371466\n",
      "Current iteration=0, loss=64995787.240412705\n",
      "Current iteration=25, loss=36376173.27000173\n",
      "Current iteration=50, loss=72970427.91363946\n",
      "Current iteration=75, loss=14488165.854269788\n",
      "Current iteration=100, loss=14381097.000497196\n",
      "Current iteration=125, loss=14343666.479801154\n",
      "Current iteration=150, loss=14342504.333546711\n",
      "Current iteration=175, loss=14342486.178824076\n",
      "Current iteration=200, loss=14342486.036990754\n",
      "Current iteration=225, loss=14342486.03643672\n",
      "Current iteration=250, loss=14342486.036435628\n",
      "Current iteration=0, loss=5280923.795454135\n",
      "Current iteration=25, loss=6077892.576851835\n",
      "Current iteration=50, loss=5256173.427736195\n",
      "Current iteration=75, loss=655706.2254303867\n",
      "Current iteration=100, loss=119961.86937245681\n",
      "Current iteration=125, loss=114231.098435263\n",
      "Current iteration=150, loss=114068.65168015982\n",
      "Current iteration=175, loss=114066.1208996527\n",
      "Current iteration=200, loss=114066.1011288072\n",
      "Current iteration=225, loss=114066.10105157743\n",
      "Current iteration=0, loss=5367445.241002039\n",
      "Current iteration=25, loss=5991740.408979973\n",
      "Current iteration=50, loss=5362347.143930338\n",
      "Current iteration=75, loss=657872.4882541598\n",
      "Current iteration=100, loss=119140.11113192473\n",
      "Current iteration=125, loss=113381.80727835174\n",
      "Current iteration=150, loss=113219.37017301962\n",
      "Current iteration=175, loss=113216.83985340963\n",
      "Current iteration=200, loss=113216.8200862069\n",
      "Current iteration=225, loss=113216.82000899127\n",
      "Current iteration=0, loss=5292030.4626245685\n",
      "Current iteration=25, loss=6141905.271311086\n",
      "Current iteration=50, loss=5285491.640248022\n",
      "Current iteration=75, loss=660609.1341502535\n",
      "Current iteration=100, loss=115826.89442817247\n",
      "Current iteration=125, loss=110544.24032376769\n",
      "Current iteration=150, loss=110395.82588727857\n",
      "Current iteration=175, loss=110393.51427399256\n",
      "Current iteration=200, loss=110393.4962153843\n",
      "Current iteration=225, loss=110393.49614484282\n",
      "Current iteration=0, loss=5356719.56447595\n",
      "Current iteration=25, loss=6046366.764765635\n",
      "Current iteration=50, loss=5186252.875006782\n",
      "Current iteration=75, loss=669119.3198789582\n",
      "Current iteration=100, loss=121385.99848396583\n",
      "Current iteration=125, loss=115565.78725123592\n",
      "Current iteration=150, loss=115400.620682135\n",
      "Current iteration=175, loss=115398.04752089783\n",
      "Current iteration=200, loss=115398.02741895814\n",
      "Current iteration=225, loss=115398.02734043486\n",
      "Current iteration=0, loss=52213121.61832046\n",
      "Current iteration=25, loss=77840462.73279606\n",
      "Current iteration=50, loss=35929907.12878111\n",
      "Current iteration=75, loss=11877772.141369987\n",
      "Current iteration=100, loss=11326569.37636214\n",
      "Current iteration=125, loss=11292807.623368664\n",
      "Current iteration=150, loss=11291754.736355716\n",
      "Current iteration=175, loss=11291738.286036493\n",
      "Current iteration=200, loss=11291738.157518495\n",
      "Current iteration=225, loss=11291738.157016471\n",
      "Current iteration=250, loss=11291738.157015504\n",
      "Current iteration=0, loss=52712210.70071506\n",
      "Current iteration=25, loss=77273014.44455963\n",
      "Current iteration=50, loss=31100639.99568086\n",
      "Current iteration=75, loss=11140964.108192995\n",
      "Current iteration=100, loss=10599003.854756847\n",
      "Current iteration=125, loss=10565619.512017526\n",
      "Current iteration=150, loss=10564578.440966245\n",
      "Current iteration=175, loss=10564562.175402367\n",
      "Current iteration=200, loss=10564562.048327774\n",
      "Current iteration=225, loss=10564562.047831394\n",
      "Current iteration=250, loss=10564562.047830425\n",
      "Current iteration=0, loss=52384901.33463925\n",
      "Current iteration=25, loss=78205503.46346708\n",
      "Current iteration=50, loss=42298208.85646026\n",
      "Current iteration=75, loss=12180122.159758206\n",
      "Current iteration=100, loss=11606950.726924766\n",
      "Current iteration=125, loss=11571986.85163738\n",
      "Current iteration=150, loss=11570896.889556093\n",
      "Current iteration=175, loss=11570879.860243455\n",
      "Current iteration=200, loss=11570879.7272021\n",
      "Current iteration=225, loss=11570879.72668241\n",
      "Current iteration=250, loss=11570879.726681389\n",
      "Current iteration=0, loss=53846841.77278014\n",
      "Current iteration=25, loss=79389442.03025985\n",
      "Current iteration=50, loss=40283559.41010239\n",
      "Current iteration=75, loss=12270256.930141477\n",
      "Current iteration=100, loss=11699021.177023256\n",
      "Current iteration=125, loss=11663797.30350489\n",
      "Current iteration=150, loss=11662697.901945487\n",
      "Current iteration=175, loss=11662680.724438611\n",
      "Current iteration=200, loss=11662680.590239415\n",
      "Current iteration=225, loss=11662680.589715203\n",
      "Current iteration=250, loss=11662680.589714175\n",
      "Current iteration=0, loss=61903742.36845055\n",
      "Current iteration=25, loss=39331184.884087965\n",
      "Current iteration=50, loss=62109582.48548695\n",
      "Current iteration=75, loss=20698730.898847114\n",
      "Current iteration=100, loss=16724773.593280258\n",
      "Current iteration=125, loss=16684931.164326921\n",
      "Current iteration=150, loss=16683699.225607103\n",
      "Current iteration=175, loss=16683679.981609814\n",
      "Current iteration=200, loss=16683679.83126669\n",
      "Current iteration=225, loss=16683679.830679405\n",
      "Current iteration=250, loss=16683679.830678258\n",
      "Current iteration=0, loss=62324899.61017072\n",
      "Current iteration=25, loss=39521382.100577414\n",
      "Current iteration=50, loss=61046533.11622019\n",
      "Current iteration=75, loss=15778404.092890965\n",
      "Current iteration=100, loss=15688896.32357631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=125, loss=15651823.91746065\n",
      "Current iteration=150, loss=15650674.909372373\n",
      "Current iteration=175, loss=15650656.961233644\n",
      "Current iteration=200, loss=15650656.821014274\n",
      "Current iteration=225, loss=15650656.82046654\n",
      "Current iteration=250, loss=15650656.820465468\n",
      "Current iteration=0, loss=62164652.881552145\n",
      "Current iteration=25, loss=39650340.26992456\n",
      "Current iteration=50, loss=62710936.61258238\n",
      "Current iteration=75, loss=20318670.875052962\n",
      "Current iteration=100, loss=16149559.645093098\n",
      "Current iteration=125, loss=16110018.861543698\n",
      "Current iteration=150, loss=16108804.605190909\n",
      "Current iteration=175, loss=16108785.642358193\n",
      "Current iteration=200, loss=16108785.49421225\n",
      "Current iteration=225, loss=16108785.493633566\n",
      "Current iteration=250, loss=16108785.493632428\n",
      "Current iteration=0, loss=64957190.24087177\n",
      "Current iteration=25, loss=36716693.623447135\n",
      "Current iteration=50, loss=69136895.3520715\n",
      "Current iteration=75, loss=15980078.854351517\n",
      "Current iteration=100, loss=16028820.64607686\n",
      "Current iteration=125, loss=15993515.690631336\n",
      "Current iteration=150, loss=15992415.453610161\n",
      "Current iteration=175, loss=15992398.263327058\n",
      "Current iteration=200, loss=15992398.129028078\n",
      "Current iteration=225, loss=15992398.12850347\n",
      "Current iteration=250, loss=15992398.12850243\n",
      "Current iteration=0, loss=5286493.761684782\n",
      "Current iteration=25, loss=6138617.837177803\n",
      "Current iteration=50, loss=5289000.70677718\n",
      "Current iteration=75, loss=649634.9005260323\n",
      "Current iteration=100, loss=118715.73133877387\n",
      "Current iteration=125, loss=113144.79182222915\n",
      "Current iteration=150, loss=112986.82489089241\n",
      "Current iteration=175, loss=112984.36389647347\n",
      "Current iteration=200, loss=112984.34467080649\n",
      "Current iteration=225, loss=112984.34459570634\n",
      "Current iteration=0, loss=5373093.85065378\n",
      "Current iteration=25, loss=6052845.870406155\n",
      "Current iteration=50, loss=5397483.921919647\n",
      "Current iteration=75, loss=657361.2251154534\n",
      "Current iteration=100, loss=117326.83337658117\n",
      "Current iteration=125, loss=111797.57793054437\n",
      "Current iteration=150, loss=111642.03881457697\n",
      "Current iteration=175, loss=111639.61626111438\n",
      "Current iteration=200, loss=111639.59733582096\n",
      "Current iteration=225, loss=111639.59726189407\n",
      "Current iteration=0, loss=5297595.242125205\n",
      "Current iteration=25, loss=6195461.940833372\n",
      "Current iteration=50, loss=5315935.380611131\n",
      "Current iteration=75, loss=658219.2171254713\n",
      "Current iteration=100, loss=116105.38727998424\n",
      "Current iteration=125, loss=110793.32977582449\n",
      "Current iteration=150, loss=110643.82890180456\n",
      "Current iteration=175, loss=110641.50038558799\n",
      "Current iteration=200, loss=110641.48219492042\n",
      "Current iteration=225, loss=110641.48212386316\n",
      "Current iteration=0, loss=5362371.324455766\n",
      "Current iteration=25, loss=6105106.026017193\n",
      "Current iteration=50, loss=5219452.685314702\n",
      "Current iteration=75, loss=668235.736946577\n",
      "Current iteration=100, loss=120032.2842472028\n",
      "Current iteration=125, loss=114364.19810084002\n",
      "Current iteration=150, loss=114203.65647739668\n",
      "Current iteration=175, loss=114201.1554556847\n",
      "Current iteration=200, loss=114201.1359173281\n",
      "Current iteration=225, loss=114201.13584100649\n",
      "Current iteration=0, loss=52242469.70078956\n",
      "Current iteration=25, loss=80675390.43233438\n",
      "Current iteration=50, loss=38599378.785724044\n",
      "Current iteration=75, loss=11615261.895348947\n",
      "Current iteration=100, loss=11007575.439093867\n",
      "Current iteration=125, loss=10970531.938859005\n",
      "Current iteration=150, loss=10969375.27865579\n",
      "Current iteration=175, loss=10969357.206313446\n",
      "Current iteration=200, loss=10969357.065123314\n",
      "Current iteration=225, loss=10969357.064571785\n",
      "Current iteration=250, loss=10969357.0645707\n",
      "Current iteration=0, loss=52741904.94134817\n",
      "Current iteration=25, loss=81220093.2961747\n",
      "Current iteration=50, loss=38493747.08081817\n",
      "Current iteration=75, loss=11432800.624965627\n",
      "Current iteration=100, loss=10824074.186914694\n",
      "Current iteration=125, loss=10786524.242333142\n",
      "Current iteration=150, loss=10785351.768705362\n",
      "Current iteration=175, loss=10785333.449185573\n",
      "Current iteration=200, loss=10785333.306064378\n",
      "Current iteration=225, loss=10785333.305505304\n",
      "Current iteration=250, loss=10785333.305504214\n",
      "Current iteration=0, loss=52414288.650092974\n",
      "Current iteration=25, loss=81629155.54507934\n",
      "Current iteration=50, loss=38181464.31336699\n",
      "Current iteration=75, loss=11740047.105402732\n",
      "Current iteration=100, loss=11118013.452758785\n",
      "Current iteration=125, loss=11080021.585894536\n",
      "Current iteration=150, loss=11078836.210686736\n",
      "Current iteration=175, loss=11078817.689972883\n",
      "Current iteration=200, loss=11078817.5452799\n",
      "Current iteration=225, loss=11078817.544714695\n",
      "Current iteration=250, loss=11078817.544713598\n",
      "Current iteration=0, loss=53877350.80696794\n",
      "Current iteration=25, loss=84054419.64602566\n",
      "Current iteration=50, loss=38053039.96424524\n",
      "Current iteration=75, loss=11816656.021345165\n",
      "Current iteration=100, loss=11203600.943042338\n",
      "Current iteration=125, loss=11165661.803250462\n",
      "Current iteration=150, loss=11164476.766063156\n",
      "Current iteration=175, loss=11164458.250071568\n",
      "Current iteration=200, loss=11164458.10541541\n",
      "Current iteration=225, loss=11164458.104850346\n",
      "Current iteration=250, loss=11164458.104849244\n",
      "Current iteration=0, loss=61952605.571665764\n",
      "Current iteration=25, loss=38743137.496556334\n",
      "Current iteration=50, loss=64064002.7596525\n",
      "Current iteration=75, loss=14885960.628996631\n",
      "Current iteration=100, loss=14626502.504424397\n",
      "Current iteration=125, loss=14585919.746299995\n",
      "Current iteration=150, loss=14584660.958993765\n",
      "Current iteration=175, loss=14584641.29464604\n",
      "Current iteration=200, loss=14584641.141018832\n",
      "Current iteration=225, loss=14584641.140418712\n",
      "Current iteration=250, loss=14584641.140417537\n",
      "Current iteration=0, loss=62374162.521302946\n",
      "Current iteration=25, loss=39085696.53726904\n",
      "Current iteration=50, loss=63386339.01287029\n",
      "Current iteration=75, loss=14077763.532243935\n",
      "Current iteration=100, loss=13751398.00679052\n",
      "Current iteration=125, loss=13711220.727798164\n",
      "Current iteration=150, loss=13709975.20902766\n",
      "Current iteration=175, loss=13709955.75189196\n",
      "Current iteration=200, loss=13709955.59988357\n",
      "Current iteration=225, loss=13709955.599289784\n",
      "Current iteration=250, loss=13709955.599288624\n",
      "Current iteration=0, loss=62213499.53582922\n",
      "Current iteration=25, loss=38981970.11120195\n",
      "Current iteration=50, loss=64874936.46704705\n",
      "Current iteration=75, loss=14475272.127370356\n",
      "Current iteration=100, loss=14216334.081946166\n",
      "Current iteration=125, loss=14175257.988834118\n",
      "Current iteration=150, loss=14173977.469506364\n",
      "Current iteration=175, loss=14173957.463321459\n",
      "Current iteration=200, loss=14173957.30702337\n",
      "Current iteration=225, loss=14173957.30641284\n",
      "Current iteration=250, loss=14173957.30641165\n",
      "Current iteration=0, loss=65007662.07232259\n",
      "Current iteration=25, loss=36264247.83441935\n",
      "Current iteration=50, loss=74429920.49552782\n",
      "Current iteration=75, loss=14084757.99534679\n",
      "Current iteration=100, loss=13933941.321843859\n",
      "Current iteration=125, loss=13895617.086013028\n",
      "Current iteration=150, loss=13894428.47503907\n",
      "Current iteration=175, loss=13894409.906443283\n",
      "Current iteration=200, loss=13894409.761376528\n",
      "Current iteration=225, loss=13894409.760809852\n",
      "Current iteration=250, loss=13894409.760808758\n",
      "[1.05419043] [2]\n"
     ]
    }
   ],
   "source": [
    "degrees = np.arange(1, 4)\n",
    "lambdas = random_interval(0, 3, 4)\n",
    "gamma = 3.16228e-03\n",
    "k_fold = 4\n",
    "seed = 1\n",
    "testing_acc = np.zeros((len(lambdas), len(degrees)))\n",
    "k_indices = build_k_indices(y_1, k_fold, seed)\n",
    "for index1 in range(len(lambdas)):\n",
    "    for index2 in range(len(degrees)):\n",
    "    #  for index3 in range(len(gammas)):\n",
    "        test_acc = 0\n",
    "        for k in range(k_fold):\n",
    "            current_test_acc = cross_validation_logistic(y_1, tX_tilda_1, k_indices, k,\n",
    "                                            lambdas[index1], degrees[index2], gamma)\n",
    "            test_acc += current_test_acc\n",
    "        testing_acc[index1, index2] = test_acc / k_fold\n",
    "best_result = np.where(testing_acc == np.amax(testing_acc))\n",
    "lambda_opt, degree_opt = lambdas[best_result[0]], degrees[best_result[1]]\n",
    "print(lambda_opt, degree_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.71706386 0.79186011 0.78450944]\n",
      " [0.71681884 0.79216961 0.78474157]\n",
      " [0.71692201 0.79235015 0.78470288]\n",
      " [0.71692201 0.79207934 0.78479315]]\n"
     ]
    }
   ],
   "source": [
    "print(testing_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimal lambda = 1.5 and optimal degree = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with randomized grid search lambda = 1.22889406 and degree = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The accuracy goes up to 0.79247"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree = 2 and lambda = 1.34904406 (0.7926)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=4436759.4362167725\n",
      "Current iteration=25, loss=6631794.415068468\n",
      "Current iteration=50, loss=3315321.825154567\n",
      "Current iteration=75, loss=784016.6307658348\n",
      "Current iteration=100, loss=202130.61120240038\n",
      "Current iteration=125, loss=191837.27254886474\n",
      "Current iteration=150, loss=191529.85502868326\n",
      "Current iteration=175, loss=191525.05815925746\n",
      "Current iteration=200, loss=191525.02068449892\n",
      "Current iteration=225, loss=191525.02053811323\n",
      "Current iteration=250, loss=191525.02053782734\n",
      "Current iteration=0, loss=4458100.070333392\n",
      "Current iteration=25, loss=6548709.820659388\n",
      "Current iteration=50, loss=3281787.3886253126\n",
      "Current iteration=75, loss=760271.2440130847\n",
      "Current iteration=100, loss=207199.490318955\n",
      "Current iteration=125, loss=196873.21688440273\n",
      "Current iteration=150, loss=196563.83261007525\n",
      "Current iteration=175, loss=196559.0046235547\n",
      "Current iteration=200, loss=196558.96690570418\n",
      "Current iteration=225, loss=196558.96675836892\n",
      "Current iteration=250, loss=196558.96675808143\n",
      "Current iteration=0, loss=4474845.504852015\n",
      "Current iteration=25, loss=6588177.430132922\n",
      "Current iteration=50, loss=3297631.272161312\n",
      "Current iteration=75, loss=720666.6021227795\n",
      "Current iteration=100, loss=195084.7059990599\n",
      "Current iteration=125, loss=184399.24091928275\n",
      "Current iteration=150, loss=184081.45391861617\n",
      "Current iteration=175, loss=184076.49589916161\n",
      "Current iteration=200, loss=184076.45716552463\n",
      "Current iteration=225, loss=184076.45701422138\n",
      "Current iteration=250, loss=184076.45701392592\n",
      "Current iteration=0, loss=4385273.3253672635\n",
      "Current iteration=25, loss=6592497.365204233\n",
      "Current iteration=50, loss=3297926.760867302\n",
      "Current iteration=75, loss=782129.8593400276\n",
      "Current iteration=100, loss=211164.55203182693\n",
      "Current iteration=125, loss=200035.95094815193\n",
      "Current iteration=150, loss=199703.0914552312\n",
      "Current iteration=175, loss=199697.89751264107\n",
      "Current iteration=200, loss=199697.8569357835\n",
      "Current iteration=225, loss=199697.85677727987\n",
      "Current iteration=250, loss=199697.85677697026\n",
      "Current iteration=0, loss=23985386.988014456\n",
      "Current iteration=25, loss=19206155.154529955\n",
      "Current iteration=50, loss=15041051.100528918\n",
      "Current iteration=75, loss=12868347.478395462\n",
      "Current iteration=100, loss=12585575.26010129\n",
      "Current iteration=125, loss=12568366.852669498\n",
      "Current iteration=150, loss=12567829.923272591\n",
      "Current iteration=175, loss=12567821.534120552\n",
      "Current iteration=200, loss=12567821.46858036\n",
      "Current iteration=225, loss=12567821.468324346\n",
      "Current iteration=250, loss=12567821.468323855\n",
      "Current iteration=0, loss=24227238.985083915\n",
      "Current iteration=25, loss=19024924.612972703\n",
      "Current iteration=50, loss=15163782.716264293\n",
      "Current iteration=75, loss=12775310.321195673\n",
      "Current iteration=100, loss=12495744.47459404\n",
      "Current iteration=125, loss=12478559.144121729\n",
      "Current iteration=150, loss=12478022.430752767\n",
      "Current iteration=175, loss=12478014.044730157\n",
      "Current iteration=200, loss=12478013.979214368\n",
      "Current iteration=225, loss=12478013.978958428\n",
      "Current iteration=250, loss=12478013.978957932\n",
      "Current iteration=0, loss=24102065.434241842\n",
      "Current iteration=25, loss=19054119.91190745\n",
      "Current iteration=50, loss=14714893.142169995\n",
      "Current iteration=75, loss=12600069.283188814\n",
      "Current iteration=100, loss=12319021.342137763\n",
      "Current iteration=125, loss=12301755.476303305\n",
      "Current iteration=150, loss=12301215.762126094\n",
      "Current iteration=175, loss=12301207.328998089\n",
      "Current iteration=200, loss=12301207.263114253\n",
      "Current iteration=225, loss=12301207.262856904\n",
      "Current iteration=250, loss=12301207.262856407\n",
      "Current iteration=0, loss=23402207.968039364\n",
      "Current iteration=25, loss=19179970.26224924\n",
      "Current iteration=50, loss=14682966.492363168\n",
      "Current iteration=75, loss=12618413.381848408\n",
      "Current iteration=100, loss=12337975.029827433\n",
      "Current iteration=125, loss=12320612.139297722\n",
      "Current iteration=150, loss=12320069.333705585\n",
      "Current iteration=175, loss=12320060.852275014\n",
      "Current iteration=200, loss=12320060.78601383\n",
      "Current iteration=225, loss=12320060.785755008\n",
      "Current iteration=250, loss=12320060.7857545\n",
      "Current iteration=0, loss=51866885.849734165\n",
      "Current iteration=25, loss=33259271.88708541\n",
      "Current iteration=50, loss=61321554.59140058\n",
      "Current iteration=75, loss=19740317.735460453\n",
      "Current iteration=100, loss=19362598.75359454\n",
      "Current iteration=125, loss=19339564.084085137\n",
      "Current iteration=150, loss=19338846.692297176\n",
      "Current iteration=175, loss=19338835.484438736\n",
      "Current iteration=200, loss=19338835.3968775\n",
      "Current iteration=225, loss=19338835.39653548\n",
      "Current iteration=250, loss=19338835.396534823\n",
      "Current iteration=0, loss=52476611.12310998\n",
      "Current iteration=25, loss=32601308.738142196\n",
      "Current iteration=50, loss=60015333.285983294\n",
      "Current iteration=75, loss=19377895.931356285\n",
      "Current iteration=100, loss=19011554.699884027\n",
      "Current iteration=125, loss=18988611.279879436\n",
      "Current iteration=150, loss=18987896.11436401\n",
      "Current iteration=175, loss=18987884.94094802\n",
      "Current iteration=200, loss=18987884.85365583\n",
      "Current iteration=225, loss=18987884.85331483\n",
      "Current iteration=250, loss=18987884.85331417\n",
      "Current iteration=0, loss=52211608.91968097\n",
      "Current iteration=25, loss=32413604.197355635\n",
      "Current iteration=50, loss=59288988.96481675\n",
      "Current iteration=75, loss=19222016.522374045\n",
      "Current iteration=100, loss=18843062.440282777\n",
      "Current iteration=125, loss=18820755.68326974\n",
      "Current iteration=150, loss=18820059.34978659\n",
      "Current iteration=175, loss=18820048.469855297\n",
      "Current iteration=200, loss=18820048.38485585\n",
      "Current iteration=225, loss=18820048.384523794\n",
      "Current iteration=250, loss=18820048.384523135\n",
      "Current iteration=0, loss=51610374.95613518\n",
      "Current iteration=25, loss=32604312.906204887\n",
      "Current iteration=50, loss=60421773.64999863\n",
      "Current iteration=75, loss=19175759.980809145\n",
      "Current iteration=100, loss=18809910.104961466\n",
      "Current iteration=125, loss=18786814.39781824\n",
      "Current iteration=150, loss=18786094.167574964\n",
      "Current iteration=175, loss=18786082.91502672\n",
      "Current iteration=200, loss=18786082.827116307\n",
      "Current iteration=225, loss=18786082.8267729\n",
      "Current iteration=250, loss=18786082.82677221\n",
      "Current iteration=0, loss=4436759.198885951\n",
      "Current iteration=25, loss=6631792.753414578\n",
      "Current iteration=50, loss=3315321.47337604\n",
      "Current iteration=75, loss=784018.1050654307\n",
      "Current iteration=100, loss=202132.37054131346\n",
      "Current iteration=125, loss=191838.90057161544\n",
      "Current iteration=150, loss=191531.47917893805\n",
      "Current iteration=175, loss=191526.68224945184\n",
      "Current iteration=200, loss=191526.6447742241\n",
      "Current iteration=225, loss=191526.64462783636\n",
      "Current iteration=250, loss=191526.6446275505\n",
      "Current iteration=0, loss=4458099.83101742\n",
      "Current iteration=25, loss=6548708.294041884\n",
      "Current iteration=50, loss=3281787.27446887\n",
      "Current iteration=75, loss=760274.67159388\n",
      "Current iteration=100, loss=207201.22195130837\n",
      "Current iteration=125, loss=196874.8241481577\n",
      "Current iteration=150, loss=196565.436161846\n",
      "Current iteration=175, loss=196560.60811001156\n",
      "Current iteration=200, loss=196560.5703917172\n",
      "Current iteration=225, loss=196560.57024438033\n",
      "Current iteration=250, loss=196560.5702440928\n",
      "Current iteration=0, loss=4474845.263415002\n",
      "Current iteration=25, loss=6588175.938098509\n",
      "Current iteration=50, loss=3297631.2461018167\n",
      "Current iteration=75, loss=720682.3416639237\n",
      "Current iteration=100, loss=195087.4221380471\n",
      "Current iteration=125, loss=184401.8741037712\n",
      "Current iteration=150, loss=184084.08430910812\n",
      "Current iteration=175, loss=184079.1262460575\n",
      "Current iteration=200, loss=184079.08751207986\n",
      "Current iteration=225, loss=184079.0873607753\n",
      "Current iteration=250, loss=184079.08736047984\n",
      "Current iteration=0, loss=4385273.090446728\n",
      "Current iteration=25, loss=6592495.552253978\n",
      "Current iteration=50, loss=3297926.2987979366\n",
      "Current iteration=75, loss=782128.9100896694\n",
      "Current iteration=100, loss=211166.81124347472\n",
      "Current iteration=125, loss=200038.02702900989\n",
      "Current iteration=150, loss=199705.16204517544\n",
      "Current iteration=175, loss=199699.96801950727\n",
      "Current iteration=200, loss=199699.92744200065\n",
      "Current iteration=225, loss=199699.92728349473\n",
      "Current iteration=250, loss=199699.92728318518\n",
      "Current iteration=0, loss=23985386.371604677\n",
      "Current iteration=25, loss=19206161.550160803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=50, loss=15041198.007849403\n",
      "Current iteration=75, loss=12868454.304792957\n",
      "Current iteration=100, loss=12585682.056886697\n",
      "Current iteration=125, loss=12568473.640855493\n",
      "Current iteration=150, loss=12567936.711468464\n",
      "Current iteration=175, loss=12567928.32231671\n",
      "Current iteration=200, loss=12567928.256776506\n",
      "Current iteration=225, loss=12567928.25652049\n",
      "Current iteration=250, loss=12567928.256519997\n",
      "Current iteration=0, loss=24227238.370829176\n",
      "Current iteration=25, loss=19024924.103275962\n",
      "Current iteration=50, loss=15163874.031597178\n",
      "Current iteration=75, loss=12775408.512215858\n",
      "Current iteration=100, loss=12495843.433344012\n",
      "Current iteration=125, loss=12478658.153208861\n",
      "Current iteration=150, loss=12478121.44167915\n",
      "Current iteration=175, loss=12478113.05568538\n",
      "Current iteration=200, loss=12478112.990169825\n",
      "Current iteration=225, loss=12478112.989913907\n",
      "Current iteration=250, loss=12478112.989913411\n",
      "Current iteration=0, loss=24102064.811778933\n",
      "Current iteration=25, loss=19054126.510011993\n",
      "Current iteration=50, loss=14715014.892025951\n",
      "Current iteration=75, loss=12600172.74855297\n",
      "Current iteration=100, loss=12319125.25980113\n",
      "Current iteration=125, loss=12301859.497870194\n",
      "Current iteration=150, loss=12301319.78714504\n",
      "Current iteration=175, loss=12301311.35407102\n",
      "Current iteration=200, loss=12301311.288187612\n",
      "Current iteration=225, loss=12301311.287930265\n",
      "Current iteration=250, loss=12301311.287929764\n",
      "Current iteration=0, loss=23402207.367629163\n",
      "Current iteration=25, loss=19179974.58428738\n",
      "Current iteration=50, loss=14683116.22707925\n",
      "Current iteration=75, loss=12618517.366049415\n",
      "Current iteration=100, loss=12338079.742077693\n",
      "Current iteration=125, loss=12320716.963757986\n",
      "Current iteration=150, loss=12320174.16126605\n",
      "Current iteration=175, loss=12320165.67989105\n",
      "Current iteration=200, loss=12320165.613630297\n",
      "Current iteration=225, loss=12320165.613371477\n",
      "Current iteration=250, loss=12320165.61337097\n",
      "Current iteration=0, loss=51866883.727298185\n",
      "Current iteration=25, loss=33259233.87782265\n",
      "Current iteration=50, loss=61321504.044556156\n",
      "Current iteration=75, loss=19740418.42090801\n",
      "Current iteration=100, loss=19362701.246364504\n",
      "Current iteration=125, loss=19339666.675620075\n",
      "Current iteration=150, loss=19338949.28689195\n",
      "Current iteration=175, loss=19338938.07898082\n",
      "Current iteration=200, loss=19338937.991419207\n",
      "Current iteration=225, loss=19338937.99107718\n",
      "Current iteration=250, loss=19338937.99107652\n",
      "Current iteration=0, loss=52476608.98738265\n",
      "Current iteration=25, loss=32601313.804567523\n",
      "Current iteration=50, loss=60015239.49830791\n",
      "Current iteration=75, loss=19377995.642246343\n",
      "Current iteration=100, loss=19011656.790042695\n",
      "Current iteration=125, loss=18988713.46690752\n",
      "Current iteration=150, loss=18987998.304215174\n",
      "Current iteration=175, loss=18987987.13084324\n",
      "Current iteration=200, loss=18987987.043551393\n",
      "Current iteration=225, loss=18987987.043210395\n",
      "Current iteration=250, loss=18987987.043209735\n",
      "Current iteration=0, loss=52211606.76373469\n",
      "Current iteration=25, loss=32413621.249723017\n",
      "Current iteration=50, loss=59289038.03132722\n",
      "Current iteration=75, loss=19222111.676297855\n",
      "Current iteration=100, loss=18843159.619334053\n",
      "Current iteration=125, loss=18820852.991980568\n",
      "Current iteration=150, loss=18820156.663035642\n",
      "Current iteration=175, loss=18820145.783175454\n",
      "Current iteration=200, loss=18820145.698176585\n",
      "Current iteration=225, loss=18820145.69784453\n",
      "Current iteration=250, loss=18820145.697843876\n",
      "Current iteration=0, loss=51610372.87128626\n",
      "Current iteration=25, loss=32604306.99308598\n",
      "Current iteration=50, loss=60421725.25454849\n",
      "Current iteration=75, loss=19175861.967581384\n",
      "Current iteration=100, loss=18810015.112770796\n",
      "Current iteration=125, loss=18786919.551909138\n",
      "Current iteration=150, loss=18786199.325478543\n",
      "Current iteration=175, loss=18786188.072989706\n",
      "Current iteration=200, loss=18786187.98507976\n",
      "Current iteration=225, loss=18786187.984736364\n",
      "Current iteration=250, loss=18786187.98473567\n",
      "Current iteration=0, loss=4436759.368735576\n",
      "Current iteration=25, loss=6631793.94260191\n",
      "Current iteration=50, loss=3315321.725126645\n",
      "Current iteration=75, loss=784017.0500495493\n",
      "Current iteration=100, loss=202131.11145845376\n",
      "Current iteration=125, loss=191837.73546509765\n",
      "Current iteration=150, loss=191530.3168437719\n",
      "Current iteration=175, loss=191525.51995726785\n",
      "Current iteration=200, loss=191525.482482376\n",
      "Current iteration=225, loss=191525.48233598968\n",
      "Current iteration=250, loss=191525.48233570377\n",
      "Current iteration=0, loss=4458100.002287749\n",
      "Current iteration=25, loss=6548709.386590253\n",
      "Current iteration=50, loss=3281787.356167468\n",
      "Current iteration=75, loss=760272.2186416906\n",
      "Current iteration=100, loss=207199.98266677212\n",
      "Current iteration=125, loss=196873.6738707991\n",
      "Current iteration=150, loss=196564.28854105293\n",
      "Current iteration=175, loss=196559.46053596187\n",
      "Current iteration=200, loss=196559.42281798512\n",
      "Current iteration=225, loss=196559.4226706491\n",
      "Current iteration=250, loss=196559.42267036156\n",
      "Current iteration=0, loss=4474845.436203289\n",
      "Current iteration=25, loss=6588177.00587796\n",
      "Current iteration=50, loss=3297631.264749824\n",
      "Current iteration=75, loss=720671.078301705\n",
      "Current iteration=100, loss=195085.4784320518\n",
      "Current iteration=125, loss=184399.9897545733\n",
      "Current iteration=150, loss=184082.20195915055\n",
      "Current iteration=175, loss=184077.24392729506\n",
      "Current iteration=200, loss=184077.20519356118\n",
      "Current iteration=225, loss=184077.20504225767\n",
      "Current iteration=250, loss=184077.20504196218\n",
      "Current iteration=0, loss=4385273.258571393\n",
      "Current iteration=25, loss=6592496.849713203\n",
      "Current iteration=50, loss=3297926.629487215\n",
      "Current iteration=75, loss=782129.5895513836\n",
      "Current iteration=100, loss=211165.19442581938\n",
      "Current iteration=125, loss=200036.54127000843\n",
      "Current iteration=150, loss=199703.68021577978\n",
      "Current iteration=175, loss=199698.48624956695\n",
      "Current iteration=200, loss=199698.44567252483\n",
      "Current iteration=225, loss=199698.4455140208\n",
      "Current iteration=250, loss=199698.44551371117\n",
      "Current iteration=0, loss=23985386.81274826\n",
      "Current iteration=25, loss=19206156.973241627\n",
      "Current iteration=50, loss=15041092.870231519\n",
      "Current iteration=75, loss=12868377.85276357\n",
      "Current iteration=100, loss=12585605.626053026\n",
      "Current iteration=125, loss=12568397.216178037\n",
      "Current iteration=150, loss=12567860.28678402\n",
      "Current iteration=175, loss=12567851.897632066\n",
      "Current iteration=200, loss=12567851.832091875\n",
      "Current iteration=225, loss=12567851.831835862\n",
      "Current iteration=250, loss=12567851.831835369\n",
      "Current iteration=0, loss=24227238.81043047\n",
      "Current iteration=25, loss=19024924.467959594\n",
      "Current iteration=50, loss=15163808.680718645\n",
      "Current iteration=75, loss=12775338.240163814\n",
      "Current iteration=100, loss=12495772.611849738\n",
      "Current iteration=125, loss=12478587.295691676\n",
      "Current iteration=150, loss=12478050.58284574\n",
      "Current iteration=175, loss=12478042.196831329\n",
      "Current iteration=200, loss=12478042.131315602\n",
      "Current iteration=225, loss=12478042.131059682\n",
      "Current iteration=250, loss=12478042.13105919\n",
      "Current iteration=0, loss=24102065.257254537\n",
      "Current iteration=25, loss=19054121.78754333\n",
      "Current iteration=50, loss=14714927.759064736\n",
      "Current iteration=75, loss=12600098.701542795\n",
      "Current iteration=100, loss=12319050.889104052\n",
      "Current iteration=125, loss=12301785.052814497\n",
      "Current iteration=150, loss=12301245.339618845\n",
      "Current iteration=175, loss=12301236.906506194\n",
      "Current iteration=200, loss=12301236.840622477\n",
      "Current iteration=225, loss=12301236.84036513\n",
      "Current iteration=250, loss=12301236.84036463\n",
      "Current iteration=0, loss=23402207.797322392\n",
      "Current iteration=25, loss=19179971.49057514\n",
      "Current iteration=50, loss=14683009.065314818\n",
      "Current iteration=75, loss=12618442.947338928\n",
      "Current iteration=100, loss=12338004.802322192\n",
      "Current iteration=125, loss=12320641.943696784\n",
      "Current iteration=150, loss=12320099.138986107\n",
      "Current iteration=175, loss=12320090.657571336\n",
      "Current iteration=200, loss=12320090.591310272\n",
      "Current iteration=225, loss=12320090.591051448\n",
      "Current iteration=250, loss=12320090.59105094\n",
      "Current iteration=0, loss=51866885.24625368\n",
      "Current iteration=25, loss=33259261.104822252\n",
      "Current iteration=50, loss=61321540.23043217\n",
      "Current iteration=75, loss=19740346.364274114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=100, loss=19362627.896576993\n",
      "Current iteration=125, loss=19339593.255155325\n",
      "Current iteration=150, loss=19338875.86416872\n",
      "Current iteration=175, loss=19338864.656324737\n",
      "Current iteration=200, loss=19338864.568763632\n",
      "Current iteration=225, loss=19338864.56842161\n",
      "Current iteration=250, loss=19338864.56842095\n",
      "Current iteration=0, loss=52476610.51585031\n",
      "Current iteration=25, loss=32601310.178346045\n",
      "Current iteration=50, loss=60015306.584615074\n",
      "Current iteration=75, loss=19377924.28465334\n",
      "Current iteration=100, loss=19011583.72946242\n",
      "Current iteration=125, loss=18988640.3369889\n",
      "Current iteration=150, loss=18987925.172275748\n",
      "Current iteration=175, loss=18987913.998872273\n",
      "Current iteration=200, loss=18987913.91158021\n",
      "Current iteration=225, loss=18987913.91123921\n",
      "Current iteration=250, loss=18987913.911238547\n",
      "Current iteration=0, loss=52211608.306672364\n",
      "Current iteration=25, loss=32413609.05075235\n",
      "Current iteration=50, loss=59289002.76342344\n",
      "Current iteration=75, loss=19222043.577180386\n",
      "Current iteration=100, loss=18843090.071540006\n",
      "Current iteration=125, loss=18820783.351431407\n",
      "Current iteration=150, loss=18820087.01923994\n",
      "Current iteration=175, loss=18820076.139328886\n",
      "Current iteration=200, loss=18820076.05432961\n",
      "Current iteration=225, loss=18820076.053997558\n",
      "Current iteration=250, loss=18820076.0539969\n",
      "Current iteration=0, loss=51610374.363341965\n",
      "Current iteration=25, loss=32604311.25493825\n",
      "Current iteration=50, loss=60421759.860226706\n",
      "Current iteration=75, loss=19175788.97742653\n",
      "Current iteration=100, loss=18809939.959973957\n",
      "Current iteration=125, loss=18786844.294454053\n",
      "Current iteration=150, loss=18786124.06529588\n",
      "Current iteration=175, loss=18786112.81276454\n",
      "Current iteration=200, loss=18786112.72485428\n",
      "Current iteration=225, loss=18786112.72451088\n",
      "Current iteration=250, loss=18786112.724510185\n",
      "[7.61138191e-05 3.65477087e-05 6.48638328e-05] [2 2 2]\n"
     ]
    }
   ],
   "source": [
    "degrees = np.arange(1, 4)\n",
    "lambdas = random_interval(1.0e-04, 1.0e-08, 3)\n",
    "gamma = 3.16228e-03\n",
    "k_fold = 4\n",
    "seed = 1\n",
    "testing_acc = np.zeros((len(lambdas), len(degrees)))\n",
    "k_indices = build_k_indices(y_2_3, k_fold, seed)\n",
    "for index1 in range(len(lambdas)):\n",
    "    for index2 in range(len(degrees)):\n",
    "    #  for index3 in range(len(gammas)):\n",
    "        test_acc = 0\n",
    "        for k in range(k_fold):\n",
    "            current_test_acc = cross_validation_logistic(y_2_3, tX_tilda_2_3, k_indices, k,\n",
    "                                            lambdas[index1], degrees[index2], gamma)\n",
    "            test_acc += current_test_acc\n",
    "        testing_acc[index1, index2] = test_acc / k_fold\n",
    "best_result = np.where(testing_acc == np.amax(testing_acc))\n",
    "lambda_opt, degree_opt = lambdas[best_result[0]], degrees[best_result[1]]\n",
    "print(lambda_opt, degree_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99851058 0.94805659 0.92442061]\n"
     ]
    }
   ],
   "source": [
    "print(lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.74330025 0.81450234 0.80964985]\n",
      " [0.74330025 0.81450234 0.80964985]\n",
      " [0.74330025 0.81450234 0.80964985]]\n"
     ]
    }
   ],
   "source": [
    "print(testing_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimal degree = 2 and optimal lambda = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with randomized grid search lambda = 0.63458637 and degree = 2 (0.81519162)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimal degree = 2 and optimal lambda = 0.06711277 ( 0.81457127)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimal degree = 2 and lambda = 0.65319 or 0.7627 (0.81526)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=11408908.764839454\n",
      "Current iteration=25, loss=5384015.940568853\n",
      "Current iteration=50, loss=1216495.0535235677\n",
      "Current iteration=75, loss=728908.4353324835\n",
      "Current iteration=100, loss=638630.6388267685\n",
      "Current iteration=125, loss=633381.4257645909\n",
      "Current iteration=150, loss=633218.1689454443\n",
      "Current iteration=175, loss=633215.6184294892\n",
      "Current iteration=200, loss=633215.5985036286\n",
      "Current iteration=225, loss=633215.5984257926\n"
     ]
    }
   ],
   "source": [
    "tX_tilda_0_augmented = build_poly(tX_tilda_0, degree = 2)\n",
    "_, w_logistic_0 = learning_by_penalized_gradient(y_0, tX_tilda_0_augmented, np.zeros(tX_tilda_0_augmented.shape[1]), 0.00017783,\n",
    "                                              1000, lambda_ = 2.14434261)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=93862471.73446892\n",
      "Current iteration=25, loss=140407464.00300688\n",
      "Current iteration=50, loss=72358373.41790447\n",
      "Current iteration=75, loss=21165951.34823247\n",
      "Current iteration=100, loss=20133611.228061605\n",
      "Current iteration=125, loss=20070957.458448704\n",
      "Current iteration=150, loss=20069003.495453235\n",
      "Current iteration=175, loss=20068972.966472466\n",
      "Current iteration=200, loss=20068972.727965005\n",
      "Current iteration=225, loss=20068972.72703333\n",
      "Current iteration=250, loss=20068972.72703152\n"
     ]
    }
   ],
   "source": [
    "tX_tilda_1_augmented = build_poly(tX_tilda_1, degree = 2)\n",
    "_, w_logistic_1 = learning_by_penalized_gradient(y_1, tX_tilda_1_augmented, np.zeros(tX_tilda_1_augmented.shape[1]), 0.00316228,\n",
    "                                              1000, lambda_ = 1.34904406)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=2393204.533529861\n",
      "Current iteration=25, loss=1911812.6220919045\n",
      "Current iteration=50, loss=1505442.4676446328\n",
      "Current iteration=75, loss=1268799.7368653126\n",
      "Current iteration=100, loss=1240801.4320589744\n",
      "Current iteration=125, loss=1239064.6269209725\n",
      "Current iteration=150, loss=1239010.376161187\n",
      "Current iteration=175, loss=1239009.5285039416\n",
      "Current iteration=200, loss=1239009.5218816197\n",
      "Current iteration=225, loss=1239009.5218557518\n"
     ]
    }
   ],
   "source": [
    "tX_tilda_2_3_augmented = build_poly(tX_tilda_2_3, degree = 2)\n",
    "_, w_logistic_2_3 = learning_by_penalized_gradient(y_2_3, tX_tilda_2_3_augmented, np.zeros(tX_tilda_2_3_augmented.shape[1]),\n",
    "                                                0.00017783, 1000, lambda_ = 0.63458637)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238, 30)\n"
     ]
    }
   ],
   "source": [
    "print(tX_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now format the tX_test as we did for tX_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we split the test into the three subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_indices = []\n",
    "one_indices = []\n",
    "two_three_indices = []\n",
    "zero_indices = np.where(tX_test[:,22]==0)[0]\n",
    "one_indices = np.where(tX_test[:,22]==1)[0]\n",
    "two_three_indices = np.where(np.logical_or(tX_test[:,22]==2, tX_test[:,22]==3))[0]\n",
    "tX_test_0 = tX_test[zero_indices, :]\n",
    "tX_test_0 = np.delete(tX_test_0, 22, axis=1)\n",
    "tX_test_1 = tX_test[one_indices, :]\n",
    "tX_test_1 = np.delete(tX_test_1, 22, axis=1)\n",
    "tX_test_2_3 = tX_test[two_three_indices, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a column of zeros and ones to detect whether the mass has been measured or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the indices where the mass is not calculated, add the column which has 0 in those indices\n",
    "# and 1 everywhere else for all matrices 0,1,2_3\n",
    "zero_indices_0 = np.where(tX_test_0[:,1] == -999.)[0]\n",
    "column_to_add = np.array([0 if i in zero_indices_0 else 1 for i in range(tX_test_0.shape[0])])\n",
    "tX_test_0 = np.insert(tX_test_0, 0, column_to_add, axis=1)\n",
    "zero_indices_1 = np.where(tX_test_1[:,1] == -999.)[0]\n",
    "column_to_add = np.array([0 if i in zero_indices_1 else 1 for i in range(tX_test_1.shape[0])])\n",
    "tX_test_1 = np.insert(tX_test_1, 0, column_to_add, axis=1)\n",
    "zero_indices_2_3 = np.where(tX_test_2_3[:,1] == -999.)[0]\n",
    "column_to_add = np.array([0 if i in zero_indices_2_3 else 1 for i in range(tX_test_2_3.shape[0])])\n",
    "tX_test_2_3 = np.insert(tX_test_2_3, 0, column_to_add, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We drop the same columns we have dropped for the X training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test_0 = np.delete(tX_test_0, col_to_delete_0, axis=1)\n",
    "tX_test_1 = np.delete(tX_test_1, col_to_delete_1, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we substitute the -999 values with the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, tX_test_2_3.shape[1]):\n",
    "    index_column_non_valid =np.where(tX_test_2_3[:,i] == -999.)[0]\n",
    "    index_column_valid =np.where(tX_test_2_3[:,i] != -999.)[0]\n",
    "    median = np.median(tX_test_2_3[index_column_valid, i], axis = 0)\n",
    "    tX_test_2_3[index_column_non_valid,i] =  median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, tX_test_1.shape[1]):\n",
    "    index_column_non_valid =np.where(tX_test_1[:,i] == -999.)[0]\n",
    "    index_column_valid =np.where(tX_test_1[:,i] != -999.)[0]\n",
    "    median = np.median(tX_test_1[index_column_valid, i], axis = 0)\n",
    "    tX_test_1[index_column_non_valid,i] =  median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, tX_test_0.shape[1]):\n",
    "    index_column_non_valid =np.where(tX_test_0[:,i] == -999.)[0]\n",
    "    index_column_valid =np.where(tX_test_0[:,i] != -999.)[0]\n",
    "    median = np.median(tX_test_0[index_column_valid, i], axis = 0)\n",
    "    tX_test_0[index_column_non_valid,i] =  median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We standardize the test set using the mean and the standard deviation of the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227458, 19)\n"
     ]
    }
   ],
   "source": [
    "print(tX_test_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99913, 19)\n"
     ]
    }
   ],
   "source": [
    "print(tX_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_test(x, mean, std):\n",
    "    \"\"\"Standardize the test set.\"\"\"\n",
    "    x = x - mean\n",
    "    x = x / std\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test_0[:,1:] = standardize_test(tX_test_0[:,1:], mean_0, std_0)\n",
    "tX_test_1[:,1:] = standardize_test(tX_test_1[:,1:], mean_1, std_1)\n",
    "tX_test_2_3[:,1:]= standardize_test(tX_test_2_3[:,1:], mean_2_3, std_2_3) #we standardize everything a part from the column added manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We insert the column for the bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_tilda_test_0 = np.insert(tX_test_0, 0, np.ones(tX_test_0.shape[0]), axis=1)\n",
    "tX_tilda_test_1 = np.insert(tX_test_1, 0, np.ones(tX_test_1.shape[0]), axis=1)\n",
    "tX_tilda_test_2_3 = np.insert(tX_test_2_3, 0, np.ones(tX_test_2_3.shape[0]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We make the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w_ridge_2_3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-54e7e5ff956c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtX_tilda_test_2_3_augmented\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX_tilda_test_2_3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpredictions_ridge_2_3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtX_tilda_test_2_3_augmented\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mw_ridge_2_3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# print(predictions_2_3.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'w_ridge_2_3' is not defined"
     ]
    }
   ],
   "source": [
    "tX_tilda_test_2_3_augmented = build_poly(tX_tilda_test_2_3, degree=6)\n",
    "predictions_ridge_2_3 = tX_tilda_test_2_3_augmented @ w_ridge_2_3\n",
    "# print(predictions_2_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2776248801976629\n"
     ]
    }
   ],
   "source": [
    "tX_tilda_test_0_augmented = build_poly(tX_tilda_test_0, degree = 6)\n",
    "predictions_ridge_0 = tX_tilda_test_0_augmented @ w_ridge_0\n",
    "print(1 / len(predictions_ridge_0) * np.count_nonzero(predictions_ridge_0 >= 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37080381890976283\n"
     ]
    }
   ],
   "source": [
    "tX_tilda_test_1_augmented = build_poly(tX_tilda_test_1, degree = 6)\n",
    "predictions_ridge_1 = tX_tilda_test_1_augmented @ w_ridge_1\n",
    "print(1 / len(predictions_ridge_1) * np.count_nonzero(predictions_ridge_1 >= 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(zero_indices))\n",
    "print(len(one_indices))\n",
    "print(len(two_three_indices))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Predictions with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43570556448785674\n"
     ]
    }
   ],
   "source": [
    "tX_tilda_test_2_3_augmented = build_poly(tX_tilda_test_2_3, degree = 2)\n",
    "predictions_logistic_2_3 = sigmoid(tX_tilda_test_2_3_augmented @ w_logistic_2_3)\n",
    "print(1 / len(predictions_logistic_2_3) * np.count_nonzero(predictions_logistic_2_3 >= 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3373655454037345\n"
     ]
    }
   ],
   "source": [
    "tX_tilda_test_1_augmented = build_poly(tX_tilda_test_1, degree = 2)\n",
    "predictions_logistic_1 = sigmoid(tX_tilda_test_1_augmented @ w_logistic_1)\n",
    "print(1 / len(predictions_logistic_1) * np.count_nonzero(predictions_logistic_1 >= 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1856210816942029\n"
     ]
    }
   ],
   "source": [
    "tX_tilda_test_0_augmented = build_poly(tX_tilda_test_0, degree = 2)\n",
    "predictions_logistic_0 = sigmoid(tX_tilda_test_0_augmented @ w_logistic_0)\n",
    "print(1 / len(predictions_logistic_0) * np.count_nonzero(predictions_logistic_0 >= 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to reconstruct a single vector of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_predictions = []\n",
    " # stacked_predictions = np.zeros(y.shape[0])\n",
    "# stacked_predictions[zero_indices] = predictions_logistic_0\n",
    "# stacked_predictions[one_indices] = predictions_logistic_1\n",
    "#stacked_predictions[two_three_indices] = predictions_logistic_2_3\n",
    "count_0 = 0\n",
    "count_1 = 0\n",
    "count_2_3 = 0\n",
    "for index_row in range(tX_test.shape[0]):\n",
    "    if index_row in zero_indices:\n",
    "        stacked_predictions.append(predictions_logistic_0[count_0])\n",
    "        count_0 = count_0 + 1\n",
    "    elif index_row in one_indices:\n",
    "        stacked_predictions.append(predictions_logistic_1[count_1])\n",
    "        count_1 = count_1 + 1\n",
    "    else:\n",
    "        stacked_predictions.append(predictions_logistic_2_3[count_2_3])\n",
    "        count_2_3 = count_2_3 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = np.array([-1 if el < 0.5 else 1 for el in stacked_predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1  1 ...  1 -1 -1]\n",
      "0.30525589629697414\n"
     ]
    }
   ],
   "source": [
    "print(final_predictions) \n",
    "print(1 / len(final_predictions) * np.count_nonzero(final_predictions == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(weights, tX_test):\n",
    "    y = np.array(tX_test) @ np.array(weights)\n",
    "    labels = [1 if l > 0 else -1 for l in y]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'submission6.csv' # TODO: fill in desired name of output file for submission\n",
    "#y_pred = predict_labels(weights, tX_test)\n",
    "y_pred = final_predictions\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
>>>>>>> Stashed changes
