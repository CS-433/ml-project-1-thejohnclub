{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # train data path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the features by the number of jets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[ 160.937   68.768  103.235 ... -999.    -999.      46.226]\n",
      " [-999.     162.172  125.953 ... -999.    -999.      44.251]\n",
      " [ 154.916   10.418   94.714 ... -999.    -999.      30.638]\n",
      " ...\n",
      " [-999.      78.256   79.699 ... -999.    -999.      78.984]\n",
      " [ 133.457   77.54    88.989 ... -999.    -999.      70.969]\n",
      " [ 105.457   60.526   75.839 ... -999.    -999.      41.992]]\n"
     ]
    }
   ],
   "source": [
    "# dividing the rows of tX by the number of jets and adding an extra column of np.ones\n",
    "zero_indices = []\n",
    "one_indices = []\n",
    "two_three_indices = []\n",
    "zero_indices = np.where(tX[:,22]==0)[0]\n",
    "one_indices = np.where(tX[:,22]==1)[0]\n",
    "two_three_indices = np.where(np.logical_or(tX[:,22]==2, tX[:,22]==3))[0]\n",
    "tX_0 = tX[zero_indices, :]\n",
    "print(np.count_nonzero(tX_0[:,-2]==-999.)-tX_0.shape[0])\n",
    "tX_1 = tX[one_indices, :]\n",
    "print(tX_1)\n",
    "tX_2_3 = tX[two_three_indices, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a column of zeros and ones to detect whether the mass has been measured or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 97.827  59.149 107.782 ... 749.97   66.781  62.824]\n"
     ]
    }
   ],
   "source": [
    "# take the indices where the mass is not calculated, add the column which has 0 in those indices\n",
    "# and 1 everywhere else for all matrices 0,1,2_3\n",
    "zero_indices_0 = np.where(tX_0[:,1] == -999.)[0]\n",
    "column_to_add = np.array([0 if i in zero_indices_0 else 1 for i in range(tX_0.shape[0])])\n",
    "tX_0 = np.insert(tX_0, 0, column_to_add, axis=1)\n",
    "zero_indices_1 = np.where(tX_1[:,1] == -999.)[0]\n",
    "column_to_add = np.array([0 if i in zero_indices_1 else 1 for i in range(tX_1.shape[0])])\n",
    "tX_1 = np.insert(tX_1, 0, column_to_add, axis=1)\n",
    "zero_indices_2_3 = np.where(tX_2_3[:,1] == -999.)[0]\n",
    "column_to_add = np.array([0 if i in zero_indices_2_3 else 1 for i in range(tX_2_3.shape[0])])\n",
    "tX_2_3 = np.insert(tX_2_3, 0, column_to_add, axis=1)\n",
    "print(tX_2_3[:,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the labels to {0,1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y == 0 non detected Boson, y == 1 detected Boson\n",
    "y_ = np.array([0 if l == -1 else 1 for l in y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Throwing away the outliers from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, tX_2_3.shape[1]):\n",
    "    index_column_valid =np.where(tX_2_3[:,i] != -999.)[0]\n",
    "    column_25_quantile, column_75_quantile = np.quantile(tX_2_3[index_column_valid,i], \n",
    "                                                         np.array([0.25, 0.75]))\n",
    "    interquantile = column_75_quantile-column_25_quantile\n",
    "    column_15_quantile, column_85_quantile = np.quantile(tX_2_3[index_column_valid,i], \n",
    "                                                         np.array([0.15, 0.85]))\n",
    "    indices_outliers = np.where((column_15_quantile - 1.5 * interquantile >= tX_2_3[index_column_valid,i])\n",
    "                                             | (tX_2_3[index_column_valid,i] >= \n",
    "                                                column_85_quantile + 1.5 * interquantile))[0]\n",
    "    #indices_outliers = np.argwhere((tX_tilda_2_3[index_column_valid,i] >= column_85_quantile + 1.5 * interquantile) | \n",
    "                                  #(column_15_quantile - 1.5 * interquantile >= tX_tilda_2_3[index_column_valid,i]))\n",
    "    median = np.median(tX_2_3[index_column_valid, i], axis = 0)\n",
    "    #print(np.sort(tX_tilda_2_3[index_column_valid[indices_outliers],i]).T)\n",
    "    #print(np.where(tX_tilda_2_3[indices_outliers,i])==-999.)\n",
    "    #print(median)\n",
    "    tX_2_3[index_column_valid[indices_outliers],i] =  median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   1.     143.905   81.417 ... -999.    -999.       0.   ]\n",
      " [   1.     175.864   16.915 ... -999.    -999.       0.   ]\n",
      " [   1.     105.594   50.559 ... -999.    -999.       0.   ]\n",
      " ...\n",
      " [   1.    -999.      58.179 ... -999.    -999.       0.   ]\n",
      " [   1.      94.951   19.362 ... -999.    -999.       0.   ]\n",
      " [   1.    -999.      72.756 ... -999.    -999.       0.   ]]\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[   1.     143.905   81.417 ...   86.062    0.       0.   ]\n",
      " [   1.     175.864   16.915 ...   53.131    0.       0.   ]\n",
      " [   1.     105.594   50.559 ...  129.804    0.       0.   ]\n",
      " ...\n",
      " [   1.    -999.      58.179 ...   80.408    0.       0.   ]\n",
      " [   1.      94.951   19.362 ...  112.718    0.       0.   ]\n",
      " [   1.    -999.      72.756 ...   99.405    0.       0.   ]]\n",
      "[5, 6, 7, 13, 24, 25, 26, 27, 28, 29]\n"
     ]
    }
   ],
   "source": [
    "print(tX_0)\n",
    "col_to_delete = []\n",
    "for i in range(1, tX_0.shape[1]):\n",
    "    index_column_valid =np.where(tX_0[:,i] != -999.)[0]\n",
    "    print(np.count_nonzero(tX_0[:,23] == -999.))\n",
    "    if len(index_column_valid)==0:\n",
    "        #we drop the column (we will have to do the same for the test set as well)\n",
    "        col_to_delete.append(i)\n",
    "    else :\n",
    "        column_25_quantile, column_75_quantile = np.quantile(tX_0[index_column_valid,i], \n",
    "                                                         np.array([0.25, 0.75]))\n",
    "        interquantile = column_75_quantile-column_25_quantile\n",
    "        column_15_quantile, column_85_quantile = np.quantile(tX_0[index_column_valid,i], \n",
    "                                                         np.array([0.15, 0.85]))\n",
    "        indices_outliers = np.where((column_15_quantile - 1.5 * interquantile >= tX_0[index_column_valid,i])\n",
    "                                             | (tX_0[index_column_valid,i] >= \n",
    "                                                column_85_quantile + 1.5 * interquantile))[0]\n",
    "        #indices_outliers = np.argwhere((tX_tilda_2_3[index_column_valid,i] >= column_85_quantile + 1.5 * interquantile) | \n",
    "                                  #(column_15_quantile - 1.5 * interquantile >= tX_tilda_2_3[index_column_valid,i]))\n",
    "        median = np.median(tX_0[index_column_valid, i], axis = 0)\n",
    "        #print(np.sort(tX_tilda_2_3[index_column_valid[indices_outliers],i]).T)\n",
    "        #print(np.where(tX_tilda_2_3[indices_outliers,i])==-999.)\n",
    "        #print(median)\n",
    "        tX_0[index_column_valid[indices_outliers],i] =  median\n",
    "tX_0 = np.delete(tX_0, col_to_delete, axis=1)\n",
    "print(tX_0)\n",
    "print(col_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_delete = []\n",
    "for i in range(1, tX_1.shape[1]):\n",
    "    index_column_valid =np.where(tX_1[:,i] != -999.)[0]\n",
    "    if len(index_column_valid)==0:\n",
    "        #we drop the column (we will have to do the same for the test set as well)\n",
    "        col_to_delete.append(i)\n",
    "    else :\n",
    "        column_25_quantile, column_75_quantile = np.quantile(tX_1[index_column_valid,i], \n",
    "                                                         np.array([0.25, 0.75]))\n",
    "        interquantile = column_75_quantile-column_25_quantile\n",
    "        column_15_quantile, column_85_quantile = np.quantile(tX_1[index_column_valid,i], \n",
    "                                                         np.array([0.15, 0.85]))\n",
    "        indices_outliers = np.where((column_15_quantile - 1.5 * interquantile >= tX_1[index_column_valid,i])\n",
    "                                             | (tX_1[index_column_valid,i] >= \n",
    "                                                column_85_quantile + 1.5 * interquantile))[0]\n",
    "        #indices_outliers = np.argwhere((tX_tilda_2_3[index_column_valid,i] >= column_85_quantile + 1.5 * interquantile) | \n",
    "                                  #(column_15_quantile - 1.5 * interquantile >= tX_tilda_2_3[index_column_valid,i]))\n",
    "        median = np.median(tX_1[index_column_valid, i], axis = 0)\n",
    "        #print(np.sort(tX_tilda_2_3[index_column_valid[indices_outliers],i]).T)\n",
    "        #print(np.where(tX_tilda_2_3[indices_outliers,i])==-999.)\n",
    "        #print(median)\n",
    "        tX_1[index_column_valid[indices_outliers],i] =  median\n",
    "tX_1 = np.delete(tX_1, col_to_delete, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we substitute the -999 values with the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, tX_2_3.shape[1]):\n",
    "    index_column_non_valid =np.where(tX_2_3[:,i] == -999.)[0]\n",
    "    index_column_valid =np.where(tX_2_3[:,i] != -999.)[0]\n",
    "    median = np.median(tX_2_3[index_column_valid, i], axis = 0)\n",
    "    tX_2_3[index_column_non_valid,i] =  median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, tX_1.shape[1]):\n",
    "    index_column_non_valid =np.where(tX_1[:,i] == -999.)[0]\n",
    "    index_column_valid =np.where(tX_1[:,i] != -999.)[0]\n",
    "    median = np.median(tX_1[index_column_valid, i], axis = 0)\n",
    "    tX_1[index_column_non_valid,i] =  median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, tX_0.shape[1]):\n",
    "    index_column_non_valid =np.where(tX_0[:,i] == -999.)[0]\n",
    "    index_column_valid =np.where(tX_0[:,i] != -999.)[0]\n",
    "    median = np.median(tX_0[index_column_valid, i], axis = 0)\n",
    "    tX_0[index_column_non_valid,i] =  median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_2_3[:,1:],_,_ = standardize(tX_2_3[:,1:]) #we standardize everything a part from the column added manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.    143.905  81.417 ...  86.062   0.      0.   ]\n",
      " [  1.    175.864  16.915 ...  53.131   0.      0.   ]\n",
      " [  1.    105.594  50.559 ... 129.804   0.      0.   ]\n",
      " ...\n",
      " [  1.    111.452  58.179 ...  80.408   0.      0.   ]\n",
      " [  1.     94.951  19.362 ... 112.718   0.      0.   ]\n",
      " [  1.    111.452  72.756 ...  99.405   0.      0.   ]]\n"
     ]
    }
   ],
   "source": [
    "print(tX_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luca\\Documents\\Luca\\Machine Learning\\Machine-Learning-P1\\project1\\scripts\\proj1_helpers.py:55: RuntimeWarning: invalid value encountered in true_divide\n",
      "  x = x / std_x\n"
     ]
    }
   ],
   "source": [
    "tX_0[:,1:],_,_ = standardize(tX_0[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_1[:,1:],_,_ = standardize(tX_1[:,1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We insert the column for the bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_tilda_0 = np.insert(tX_0, 0, np.ones(tX_0.shape[0]), axis=1)\n",
    "tX_tilda_1 = np.insert(tX_1, 0, np.ones(tX_1.shape[0]), axis=1)\n",
    "tX_tilda_2_3 = np.insert(tX_2_3, 0, np.ones(tX_2_3.shape[0]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors = ['red', 'blue']\n",
    "# x_pos=[]\n",
    "# x_neg=[]\n",
    "\n",
    "# for j in range(len(y)):\n",
    "#  if(y[j]==1):\n",
    "#       x_pos.insert(0,tX[j])\n",
    "#    else:\n",
    "#        x_neg.insert(0,tX[j])\n",
    "# xpos = np.array(x_pos)\n",
    "# xneg = np.array(x_neg)\n",
    "# for i in range(tX.shape[1]):\n",
    "#  plt.hist(xpos[:,i], alpha = 0.5, color = 'r', bins = 100)\n",
    "#  plt.hist(xneg[:,i], alpha = 0.5, color = 'b', bins = 100)\n",
    "#  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    N = y.shape[0]\n",
    "    e = y - tx @ w \n",
    "    loss = 1/(2*N) * np.dot(e,e)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    N = y.shape[0]\n",
    "    e = y - tx @ w\n",
    "    gradient = -(1/N) * (tx.T) @ (e)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        gradient = compute_gradient(y,tx,w)\n",
    "        w = w - gamma * gradient\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    N = y.shape[0]\n",
    "    random_number = random.randint(0,N)\n",
    "    #random_number =1\n",
    "    xn = tx[random_number,:]\n",
    "    random_gradient = - np.dot(xn, y[random_number] - np.dot(xn,w))\n",
    "    return random_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        stoch_gradient = compute_stoch_gradient(y,tx,w)\n",
    "        w = w - gamma * stoch_gradient\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "\n",
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    forcing_term = np.transpose(tx) @ y\n",
    "    coefficient_matrix = np.transpose(tx) @ tx\n",
    "    w = np.linalg.solve(coefficient_matrix, forcing_term)\n",
    "    return w\n",
    "\n",
    "def test_your_least_squares(y, tx):\n",
    "    \"\"\"compare the solution of the normal equations with the weights returned by gradient descent algorithm.\"\"\"\n",
    "    w_least_squares = least_squares(y, tx)\n",
    "    initial_w = np.zeros(tx.shape[1])\n",
    "    max_iters = 50\n",
    "    gamma = 0.7\n",
    "    losses_gradient_descent, w_gradient_descent = gradient_descent(y, tx, initial_w, max_iters, gamma)\n",
    "    w = w_gradient_descent[-1]\n",
    "    err = np.linalg.norm(w_least_squares-w)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    N = tx.shape\n",
    "    lambda_prime = 2 * N[0] * lambda_\n",
    "    coefficient_matrix = np.transpose(tx) @ tx + lambda_prime * np.eye(N[1])\n",
    "    forcing_term = np.transpose(tx) @ y\n",
    "    w = np.linalg.solve(coefficient_matrix, forcing_term)\n",
    "    return w\n",
    "\n",
    "def debug_ridge(y, tx):\n",
    "    \"\"\"debugging the ridge regression by setting lambda=0.\"\"\"\n",
    "    w_least_squares = least_squares(y, tx)\n",
    "    w_0 = ridge_regression(y, tx, 0)\n",
    "    err = np.linalg.norm(w_least_squares-w_0)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply the sigmoid function on t.\"\"\"\n",
    "    return np.exp(t) / (1+np.exp(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the loss: negative log likelihood.\"\"\"\n",
    "    N = y.shape[0]\n",
    "    e = - (y*np.log(sigmoid(tx @ w)) +\n",
    "                  (1-y)*np.log(1-sigmoid(tx @ w)))\n",
    "    return e.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    return np.transpose(tx) @ (sigmoid(tx @ w) - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    grad = calculate_gradient(y, tx, w)\n",
    "    w = w - gamma * grad\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hessian(y, tx, w):\n",
    "    \"\"\"return the Hessian of the loss function.\"\"\"\n",
    "    diag = sigmoid(tx @ w) * (1 - sigmoid(tx @ w))\n",
    "    D = diag * np.eye(tx.shape[0])\n",
    "    return np.transpose(tx) @ D @ tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, w):\n",
    "    \"\"\"return the loss, gradient, and Hessian.\"\"\"\n",
    "    grad = calculate_gradient(y, tx, w)\n",
    "    hess = calculate_hessian(y, tx, w)\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    return loss, grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_newton_method(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step on Newton's method.\n",
    "    return the loss and updated w.\n",
    "    \"\"\"\n",
    "    loss, grad, hess = logistic_regression(y, tx, w)\n",
    "    sol = np.linalg.solve(hess, grad)\n",
    "    w = w - gamma * sol\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "    \"\"\"return the loss, gradient\"\"\"\n",
    "    loss = calculate_loss(y, tx, w) + lambda_*np.linalg.norm(w) ** 2\n",
    "    grad = calculate_gradient(y, tx, w) + 2*lambda_*w\n",
    "    hess = calculate_hessian(y, tx, w) + 2*lambda_*np.eye(w.shape[0])\n",
    "    return loss, grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_penalized_gradient(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent, using the penalized logistic regression.\n",
    "    Return the loss and updated w.\n",
    "    \"\"\"\n",
    "    loss, grad, hess = penalized_logistic_regression(y, tx, w, lambda_)\n",
    "    sol = np.linalg.solve(hess, grad)\n",
    "    w = w - gamma * sol\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "../data/test.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-6286f0f2ac1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mDATA_TEST_PATH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'../data/test.csv'\u001b[0m \u001b[1;31m# TODO: download train data and supply path here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mids_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_csv_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_TEST_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\Luca\\Machine Learning\\Machine-Learning-P1\\project1\\scripts\\proj1_helpers.py\u001b[0m in \u001b[0;36mload_csv_data\u001b[1;34m(data_path, sub_sample)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_csv_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msub_sample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;34m\"\"\"Loads data and returns y (class labels), tX (features) and ids (event ids)\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_header\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_header\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\introml\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[1;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding)\u001b[0m\n\u001b[0;32m   1747\u001b[0m             \u001b[0mfname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1748\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1749\u001b[1;33m             \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1750\u001b[0m             \u001b[0mfid_ctx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1751\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\introml\\lib\\site-packages\\numpy\\lib\\_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\introml\\lib\\site-packages\\numpy\\lib\\_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    533\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[0;32m    534\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 535\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s not found.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: ../data/test.csv not found."
     ]
    }
   ],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(weights, tX_test):\n",
    "    y = np.array(tX_test) @ np.array(weights)\n",
    "    labels = [1 if l > 0 else -1 for l in y]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #indeces_lepton = [0, 1, 2, 3, 7, 8, 9, 10, 11, 12, 16, 17, 18, 21, 22]\n",
    "# indeces_hadronic_tau = [0, 3, 7, 8, 9, 10, 11, 13, 14, 15, 21, 22]\n",
    "# indeces_jet = [0, 4, 5, 6, 8, 9, 12, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
    "# indeces_MTE = [0, 1, 3, 8, 9, 11, 19, 20, 21, 22]\n",
    "\n",
    "tX_lepton_test = tX_test[:, indeces_lepton]\n",
    "tX_tilda_lepton_test = np.insert(tX_lepton_test, 0, np.ones(tX_test.shape[0]), axis=1)\n",
    "labels_lepton = predict_labels(w_opt_lepton, tX_tilda_lepton_test)\n",
    "print(1 / np.array(labels_lepton).shape[0] * np.count_nonzero(np.array(labels_lepton) == 1))\n",
    "\n",
    "tX_hadronic_tau_test = tX_test[:, indeces_hadronic_tau]\n",
    "tX_tilda_hadronic_tau_test = np.insert(tX_hadronic_tau_test, 0, np.ones(tX_test.shape[0]), axis=1)\n",
    "labels_hadronic_tau = predict_labels(w_opt_hadronic_tau, tX_tilda_hadronic_tau_test)\n",
    "print(1 / np.array(labels_hadronic_tau).shape[0] * np.count_nonzero(np.array(labels_hadronic_tau) == 1))\n",
    "\n",
    "tX_jet_test = tX_test[:, indeces_jet]\n",
    "tX_tilda_jet_test = np.insert(tX_jet_test, 0, np.ones(tX_test.shape[0]), axis=1)\n",
    "labels_jet = predict_labels(w_opt_jet, tX_tilda_jet_test)\n",
    "print(1 / np.array(labels_jet).shape[0] * np.count_nonzero(np.array(labels_jet) == 1))\n",
    "\n",
    "tX_MTE_test = tX_test[:, indeces_MTE]\n",
    "tX_tilda_MTE_test = np.insert(tX_MTE_test, 0, np.ones(tX_test.shape[0]), axis=1)\n",
    "labels_MTE = predict_labels(w_opt_MTE, tX_tilda_MTE_test)\n",
    "print(1 / np.array(labels_MTE).shape[0] * np.count_nonzero(np.array(labels_MTE) == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRIAL, UNDERESTIMATION OF THE TRUE PROBABILITY TO GET A BOSON\n",
    "# count = np.array(labels_MTE) + np.array(labels_lepton) + np.array(labels_hadronic_tau) + np.array(labels_jet)\n",
    "# TrueSignal = np.array([int(bool((counted == 4))) for counted in count])\n",
    "# print( 1 / np.array(count).shape[0] * np.count_nonzero(np.array(TrueSignal) == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
